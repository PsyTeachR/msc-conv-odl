\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={MSc Conversion in Psychological Studies},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{MSc Conversion in Psychological Studies}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2020-03-17}

\usepackage{booktabs}

\newenvironment{danger}
    {
    \hline\\
    }
    { 
    \\\\\hline
    }
    
\newenvironment{warning}
    {
    \hline\\
    }
    { 
    \\\\\hline
    }
    
\newenvironment{info}
    {
    \hline\\
    }
    { 
    \\\\\hline
    }
    
\newenvironment{try}
    {
    \hline\\
    }
    { 
    \\\\\hline
    }

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{overview}{%
\chapter*{Overview}\label{overview}}
\addcontentsline{toc}{chapter}{Overview}

In RM2 you will learn core data skills that allow you to manipulate and analyse quantitative data, a key component of an accredited psychology programme. In addition to this book, there are video walkthroughs available on Moodle and you can use Teams to ask any R related questions that you have.

The ability to work with quantitative data is a key skill for psychologists and by using R as our tool we can also promote reproducible research practices. Although it may seem like writing a programming script is more time-consuming than other point-and-click software you may have used, this is not the case! Once you have a script you can easily re-run your analysis without having to go through each step again manually which is a) easier and b) less likely to result in errors if you do something slightly different or forget one of the steps.

Crucially, with an analysis script other researchers can also see how you got from the raw data to the statistics you report in your final paper. Sharing analysis scripts online on sites such as the \href{https://osf.io/}{Open Science Framework} is now seen as an important open science practice. Even if you don't continue with quantitative research in the future, the skills you develop on this course will allow you to evaluate quantitative research and to understand what goes on behind the scenes with data before the conclusions are presented.

\hypertarget{how-to-use-this-book}{%
\section{How to use this book}\label{how-to-use-this-book}}

For each session there will be a chapter of the book to work through as an activity. \textbf{It is crucial that you work through this book consistently throughout the term}. You will learn R much more easily if you work on it each week and give yourself time to build and practice your skills. If you are comfortable with R and/or have programming experience, please feel free to work through this book at your own pace and complete more advanced chapters.

\hypertarget{intended-learning-outcomes}{%
\section{Intended Learning Outcomes}\label{intended-learning-outcomes}}

By the end of this course students will be able to:

\begin{itemize}
\tightlist
\item
  Clean and wrangle data into appropriate forms for analysis
\item
  Visualise data using a range of plots
\item
  Conduct and interpret a core set of statistical tests (t-test, correlation, ANOVA, regression)
\end{itemize}

\hypertarget{ref3}{%
\chapter{Programming Basics}\label{ref3}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In this chapter we will go over some basic programming concepts and terminology, common pitfalls, helpful hints, and where to get help. Those of you who have no programming experience should find this chapter particularly helpful, however, even if you've used R before there may be some helpful hints and tips so please make sure you read through this chapter before moving on.

We don't expect you to memorise the information that is contained in this chapter and some sections of it will make more sense when you start writing your own code - just make sure you know what help is available!

\hypertarget{r-and-r-studio}{%
\section{R and R Studio}\label{r-and-r-studio}}

For this course, you need two different bits of software, \href{https://www.r-project.org/}{R} and \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio}. R is a programming language that you will write code in and R Studio is an Integrated Development Environment (IDE) which makes working with R easier. Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it wouldn't look as good and it would be much harder without things like spell-checking and formatting. In a similar way, you can use R without R Studio but we wouldn't recommend it. The key thing to remember is that although you will do all of your work using R Studio for this course, you are actually using two pieces of software which means that from time-to-time, both of them may have separate updates.

If you're at the University of Glasgow, all computers have R and R Studio installed, however, we can only guarantee that the correct packages and updates are installed in the Boyd Orr Level 5 and 6 psychology labs. Both pieces of softwarde are freely available so you we'd reccomend that you install them on your own machine.

If you need help installing R and R Studio, you may find \href{https://psyteachr.github.io/hack-your-data/r_instructions.html}{this page} useful.

\hypertarget{getting-to-know-r-studio}{%
\subsection{Getting to know R Studio}\label{getting-to-know-r-studio}}

R Studio has a console that you can try out code in (appearing as the bottom left window in Figure \ref{fig:img-rstudio}), there is a script editor (top left), a window showing functions and objects you have created in the ``Environment'' tab (top right window in the figure), and a window that shows plots, files packages, and help documentation (bottom right).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/rstudio} 

}

\caption{RStudio interface}\label{fig:img-rstudio}
\end{figure}

You will learn more about how to use the features included in R Studio throughout this course, however, we highly recommend watching \href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-part-1/}{RStudio Essentials 1} from the R Studio team. The video lasts \textasciitilde{}30 minutes and gives a tour of the main parts of R Studio.

\hypertarget{functions-and-arguments}{%
\section{Functions and arguments}\label{functions-and-arguments}}

\textbf{Functions} in R execute specific tasks and normally take a number of \textbf{arguments} (if you're into linguistics you might want to think as these as verbs that require a subject and an object). You can look up all the arguments that a function takes by using the help documentation by using the format \texttt{?function}. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value.

As an example, let's look at the help documentation for the function \texttt{rnorm()} which randomly generates a set of numbers with a normal distribution.

Open up R Studio and in the \textbf{console}, type the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?rnorm}
\end{Highlighting}
\end{Shaded}

The help documentation for \texttt{rnorm()} should appear in the bottom right help panel. In the usage section, we see that \texttt{rnorm()} takes the following form:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the arguments section, there are explanations for each of the arguments. \texttt{n} is the number of observations we want to create, \texttt{mean} is the mean of the data points we will create and \texttt{sd} is the standard deviation of the set. In the details section it notes that if no values are entered for \texttt{mean} and \texttt{sd} it will use a default of 0 and 1 for these values. Because there is no default value for \texttt{n} it must be specified otherwise the code won't run.

Let's try an example and just change the required argument \texttt{n} to ask R to produce 5 random numbers. Copy and paste the following code into the console.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12042016}\NormalTok{)}
\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2896163 -0.6428964  0.5829221 -0.3286728 -0.5110101
\end{verbatim}

These numbers have a mean of 0 and an SD of 1. Now we can change the additional arguments to produce a different set of numbers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{10}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13.320853  9.377956 10.235461  9.811793 13.019102
\end{verbatim}

This time R has still produced 5 random numbers, but now this set of numbers has a mean of 10 and an sd of 2 as specified. Always remember to use the help documentation to help you understand what arguments a function requires.

\begin{info}
If you're looking up examples of code online, you may often see code
that starts with the function \texttt{set.seed()}. This function
controls the random number generator - if you're using any functions
that generate numbers randomly (such as \texttt{rnorm()}), running
\texttt{set.seed()} will ensure that you get the same result (in some
cases this may not be what you want to do). We call \texttt{set.seed()}
in this example because it means that you will get the same random
numbers as this book.
\end{info}

\hypertarget{argument-names}{%
\subsection{Argument names}\label{argument-names}}

In the above examples, we have written out the argument names in our code (e.g., \texttt{n}, \texttt{mean}, \texttt{sd}), however, this is not strictly necessary. The following two lines of code would produce the same result:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{6}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{3}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Importantly, if you do not write out the argument names, R will use the default order of arguments, that is for \texttt{rnorm} it will assume that the first number you enter is \texttt{n}. the second number is \texttt{mean} and the third number is \texttt{sd}.

If you write out the argument names then you can write the arguments in whatever order you like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{sd =} \DecValTok{1}\NormalTok{, }\DataTypeTok{n =} \DecValTok{6}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see examples of code online that do not use argument names so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check).

In this course, we will always write out the argument names the first time we use each function, however, in subsequent uses they may be omitted.

\hypertarget{tab-auto-complete}{%
\subsection{Tab auto-complete}\label{tab-auto-complete}}

One very useful feature of R Studio is the tab auto-complete for functions (see Figure \ref{fig:img-autocomplete}. If you write the name of the function and then press the tab key, R Studio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/autocomplete} 

}

\caption{Tab auto-complete}\label{fig:img-autocomplete}
\end{figure}

\hypertarget{packages}{%
\section{Base R and packages}\label{packages}}

When you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as \textbf{Base R} and there is a useful cheat sheet that shows many Base R functions \href{https://www.rstudio.com/wp-content/uploads/2016/05/base-r.pdf}{here}.

However, the power of R is that it is extendable and open source - put simply, if a function doesn't exist or doesn't work very well, anyone can create a new \textbf{package} that contains data and code to allow you to perform new tasks. You may find it useful to think of Base R as the default apps that come on your phone and packages as additional apps that you need to download separately.

\hypertarget{installing-and-loading-packages}{%
\subsection{Installing and loading packages}\label{installing-and-loading-packages}}

In order to use a package, you must first install it. The following code installs the package \texttt{tidyverse}, a package we will use very frequently in this course.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it.

To load packages we use the function \texttt{library()}. Typically you would start any analysis script by loading all of the packages you need, but we will come back to that in the lab.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Now that we've loaded the \texttt{tidyverse} package we can use any of the functions it contains but remember, you need to run the \texttt{library()} function every time you start R.

\begin{info}
All of the University of Glasgow computers in the Boyd Orr will already
have all of the packages you need for this course so you only need to
install packages if you are using your own machine. Please do not
install any new packages on the university machines.
\end{info}

\hypertarget{package-updates}{%
\subsection{Package updates}\label{package-updates}}

In addition to updates to R and R Studio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run \texttt{install.packages()} it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn't a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway.

\begin{danger}
To avoid accidentally overwriting a package with a later version, you
should \textbf{never} include \texttt{install.packages()} in your
analysis scripts in case you, or someone else runs the code by mistake.
\end{danger}

\hypertarget{conflicts}{%
\subsection{Package conflicts}\label{conflicts}}

There are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages \texttt{dplyr} and \texttt{MASS} both have a function named \texttt{select()}. If you load both of these packages, R will produce a warning telling you that there is a conflict.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     select
\end{verbatim}

In this case, R is telling you that the function \texttt{select()} in the \texttt{dplyr} is being hidden (or `masked') by another function with the same name. If you were to try and use \texttt{select()}, R would use the function from the package that was loaded most recently - in this case it would use the function from \texttt{MASS}.

If you want to specify which package you want to use for a particular function you can use code in the format \texttt{package::function}, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(...)}
\NormalTok{MASS}\OperatorTok{::}\KeywordTok{select}\NormalTok{(...)}
\end{Highlighting}
\end{Shaded}

\hypertarget{objects}{%
\section{Objects}\label{objects}}

A large part of your coding will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses.You assign content to an object using \texttt{\textless{}-}.

Copy and paste the following code into the console and run it. You should see that \texttt{name}, \texttt{age}, \texttt{today}, \texttt{new\_year}, and \texttt{data} appear in the environment pane.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{name <-}\StringTok{ "emily"}
\NormalTok{age <-}\StringTok{ }\DecValTok{15} \OperatorTok{+}\StringTok{ }\DecValTok{19} 
\NormalTok{today <-}\KeywordTok{Sys.Date}\NormalTok{()}
\NormalTok{new_year <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-01-01"}\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{15}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/objects-enviro} 

}

\caption{Objects in the environment}\label{fig:img-objects-enviro}
\end{figure}

Note that in these examples, \texttt{name},\texttt{age}, and \texttt{new\_year} would always contain the values \texttt{emily}, \texttt{33}, and the date of New Year's Day 2020, however, \texttt{today} will draw the date from the operating system and \texttt{data} will be a randomly generated set of data so the values of these objects will not be static.

Importantly, objects can be involved in calculations and can interact with each other. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age }\OperatorTok{+}\StringTok{ }\DecValTok{10}
\NormalTok{new_year }\OperatorTok{-}\StringTok{ }\NormalTok{today}
\KeywordTok{mean}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 44
## Time difference of -76 days
## [1] 17.66644
\end{verbatim}

Finally, you can store the result of these operations in a new object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decade <-}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{try}
You may find it helpful to read \texttt{\textless{}-} as
\texttt{contains}, e.g., \texttt{name} contains the text \texttt{emily}.
\end{try}

You will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can numbers, text, or the result of operations, and that they can be used in further operations to create new variables.

\begin{info}
You may also see objects referred to as `variables'. There is a
difference between the two in programming terms, however, they are used
synonymously very frequently.
\end{info}

\hypertarget{looking-after-the-environment}{%
\section{Looking after the environment}\label{looking-after-the-environment}}

If you've been writing a lot of code you may find that the environment pane (or workspace) has become cluttered with many objects. This can make it difficult to figure out which object you need and therefore you run the risk of using the wrong data frame. If you're working on a new dataset, or if you've tried lots of different code before getting the final version, it is good practice to remember to clear the environment to avoid using the wrong object. You can do this in several way.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To remove individual objects, you can type \texttt{rm(object\_name)} in the console. Try this now to remove one of the objects you created in the previous section.
\item
  To clear all objects from the environment run \texttt{rm(list\ =\ ls())} in the console.
\item
  To clear all objects from the environment you can also click the broom icon in the environment pane.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/broom} 

}

\caption{Clearing the workspace}\label{fig:img-broom}
\end{figure}

\hypertarget{r-sessions}{%
\section{R sessions}\label{r-sessions}}

When you open up R and start writing code, loading packages, and creating objects, you're doing so in a new \textbf{session}. In addition to clearing the workspace, it can sometimes be useful to start a new session. This will happen automatically each time you start R, however, if you find your code isn't working and you can't figure out why, it might be worth starting a new session. This will clear the environment and detach all loaded packages - think of it like restarting your phone.

To do this, click `Session - Restart R'. Remember that you will then need to load the packages you need and your data again.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/new_session} 

}

\caption{The truth about programming}\label{fig:img-session}
\end{figure}

\hypertarget{help-and-additional-resources}{%
\section{Help and additional resources}\label{help-and-additional-resources}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/kitteh} 

}

\caption{The truth about programming}\label{fig:img-kitteh}
\end{figure}

Getting good at programming really means getting good trying stuff out, searching for help online, and finding examples of code to copy. If you are having difficulty with any of the exercises contained in this book then you can ask for help on Teams, however, learning to problem-solve effectively is a key skill that you need to develop throughout this course.

\begin{itemize}
\tightlist
\item
  Use the help documentation. If you're struggling to understand how a function works, remember the \texttt{?function} command.
\item
  If you get an error message, copy and paste it in to Google - it's very likely someone else has had the same problem.
\item
  In addition to these course materials there are a number of excellent resources for learning R:

  \begin{itemize}
  \tightlist
  \item
    \href{http://www.cookbook-r.com/}{R Cookbook}
  \item
    \href{https://stackoverflow.com/}{StackOverflow}
  \item
    \href{https://r4ds.had.co.nz/}{R for Data Science}
  \item
    Search or use the \href{https://twitter.com/search?f=tweets\&q=\%23rstats\&src=typd}{\#rstats} hashtag on Twitter
  \end{itemize}
\end{itemize}

If you would like to push yourself further with R, we recommend working through \href{https://r4ds.had.co.nz/}{R for Data Science} in full and/or the materials for the \href{https://gupsych.github.io/data_skills/}{MSc Data Skills course} run by Prof.~Lisa DeBruine and Dr.~Dale Barr at the University of Glasgow.

\hypertarget{debugging-tips}{%
\section{Debugging tips}\label{debugging-tips}}

A large part of coding is trying to figure why your code doesn't work and this is true whether you are a novice or an expert. As you progress through this course you should keep a record of mistakes you make and how you fixed them. In each chapter we will provide a number of common mistakes to look out for but you will undoubtedly make (and fix!) new mistakes yourself.

\begin{itemize}
\tightlist
\item
  Have you loaded the correct packages for the functions you are trying to use? One very common mistake is to write the code to load the package, e.g., \texttt{library(tidyverse)} but then forget to run it.
\item
  Have you made a typo? Remember \texttt{data} is not the same as \texttt{DATA} and \texttt{t.test} is not the same as \texttt{t\_test}.
\item
  Is there a package conflict? Have you tried specifying the package and function with \texttt{package::function}?
\item
  Is it definitely an error? Not all red text in R means an error - sometimes it is just giving you a message with information.
\end{itemize}

\hypertarget{test-yourself}{%
\section{Test yourself}\label{test-yourself}}

\textbf{Question 1.} Why should you never include the code \texttt{install.packages()} in your analysis scripts? You should use library() instead Packages are already part of Base R You (or someone else) may accidentally install a package update that stops your code working You already have the latest version of the package

Explain This Answer

Remember, when you run \texttt{install.packages()} it will always install the latest version of the package and it will overwrite any older versions of the package you may have installed.

\textbf{Question 2.}What will the following code produce?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

 A dataset with 10 numbers that has a mean of 6 and an SD of 50 A dataset with 6 numbers that has a mean of 50 and an SD of 10 A dataset with 50 numbers that has a mean of 10 and an SD of 6 A dataset with 50 numbers that has a mean of 10 and an SD of 6

Explain This Answer

The default form for \texttt{rnorm()} is \texttt{rnorm(n,\ mean,\ sd)}. If you need help remembering what each argument of a function does, look up the help documentation by running \texttt{?rnorm}

\textbf{Question 3.} If you have two packages that have functions with the same name and you want to specify exactly which package to use, what code would you use?

 package::function function::package library(package) install.packages(package)

Explain This Answer

You should use the form \texttt{package::function}, for example \texttt{dplyr::select}. Remember that when you first load your packages R will warn you if any functions have the same name - remember to look out for this!

\hypertarget{ref2}{%
\chapter{Intro to R}\label{ref2}}

There are nine activities in total for this chapter, but don't worry, they are broken down into very small steps!

\hypertarget{activity-1-create-the-working-directory}{%
\section{Activity 1: Create the working directory}\label{activity-1-create-the-working-directory}}

If you want to load data into R, or save the output of what you've created (which you almost always will want to do), you first need to tell R where the \textbf{working directory} is. All this means is that we tell R where the files we need (such as raw data) are located and where we want to save any files you have created. Think of it just like when you have different subjects, and you have separate folders for each topic e.g.~biology, history and so on. When working with R, it's useful to have all the data sets and files you need in one folder.

We recommend making a new folder called ``Research Methods R'' with sub-folders for each chapter and saving any data, scripts, and portfolio files for each chapter into these folders. We suggest that you save your work onto a cloud storage server like OneDrive so that you never lose your work.

\begin{itemize}
\tightlist
\item
  Choose a location for your book work and then create the necessary folders for each chapter.
\end{itemize}

\begin{warning}
Whatever you do, don't call the folder your keep your R work in ``R''.
If you do this, sometimes R has an identity crisis and won't save or
load your files properly.
\end{warning}

\hypertarget{activity-2-set-the-working-directory}{%
\section{Activity 2: Set the working directory}\label{activity-2-set-the-working-directory}}

Once you have created your folders, open R Studio. To set the working directory click \texttt{Session} -\textgreater{} \texttt{Set\ Working\ Directory} -\textgreater{} \texttt{Choose\ Directory} and then select the relevant folder for this chapter as your working directory.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/working-dir} 

}

\caption{Setting the working directory}\label{fig:img-working-dir}
\end{figure}

\hypertarget{r-markdown-for-r-book-work-and-portfolio-assignments}{%
\section{R Markdown for R book work and portfolio assignments}\label{r-markdown-for-r-book-work-and-portfolio-assignments}}

For the R book work and portfolio assignments you will use a worksheet format called R Markdown (abbreviated as Rmd) which is a great way to create dynamic documents with embedded chunks of code. These documents are self-contained and fully reproducible (if you have the necessary data, you should be able to run someone else's analyses with the click of a button) which makes it very easy to share. This is an important part of your open science training as one of the reasons we are using R Studio is that it enables us to share open and reproducible information. Using these worksheets enables you to keep a record of all the code you write during this course, and when it comes time for the portfolio assignments, we can give you a task you can and then fill in the required code.

For more information about R Markdown feel free to have a look at their main webpage sometime \url{http://rmarkdown.rstudio.com}. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then \textbf{knit} it using the package \texttt{knitr} to create your document as either a webpage (HTML), a PDF, or Word document (.docx).

\hypertarget{activity-3-open-and-save-a-new-r-markdown-document}{%
\section{Activity 3: Open and save a new R Markdown document}\label{activity-3-open-and-save-a-new-r-markdown-document}}

To open a new R Markdown document click the `new item' icon and then click `R Markdown'. You will be prompted to give it a title, call it ``Intro to R''. Also, change the author name to your GUID as this will be good practice for the portfolio assignments. Keep the output format as HTML.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/new-markdown} 

}

\caption{Opening a new R Markdown document}\label{fig:img-new-markdown}
\end{figure}

Once you've opened a new document be sure to save it by clicking \texttt{File} -\textgreater{} \texttt{Save\ as}. Name this file ``Intro to R'' as well. If you've set the working directory correctly, you should now see this file appear in your file viewer pane in the bottom right hand corner like in the example below (your file names and folders will be different).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/file-dir} 

}

\caption{New file in working directory}\label{fig:img-file-dir}
\end{figure}

\hypertarget{activity-4-create-a-new-code-chunk}{%
\section{Activity 4: Create a new code chunk}\label{activity-4-create-a-new-code-chunk}}

When you first open a new R Markdown document you will see a bunch of welcome text that looks like this:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/markdown-default} 

}

\caption{New R Markdown text}\label{fig:img-markdown-default}
\end{figure}

Do the following steps:\\
* Delete \textbf{everything} below line 7\\
* On line 8 type ``About me''\\
* Click \texttt{Insert} -\textgreater{} \texttt{R}

Your Markdown document should now look something like this:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/new-chunk} 

}

\caption{New R chunk}\label{fig:img-new-chunk}
\end{figure}

What you have created is a \textbf{code chunk}. In R Markdown, anything written in the white space is regarded as normal text, and anything written in a grey code chunk is assumed to be code. This makes it easy to combine both text and code in one document.

\begin{warning}
When you create a new code chunk you should notice that the grey box
starts and ends with three back ticks ```. One common mistake is to
accidentally delete these back ticks. Remember, code chunks are grey and
text entry is white - if the colour of certain parts of your Markdown
doesn't look right, check that you haven't deleted the back ticks.
\end{warning}

\hypertarget{activity-5-write-some-code}{%
\section{Activity 5: Write some code}\label{activity-5-write-some-code}}

Now we're going to use the code examples you read about in Programming Basics to add some simple code to our R Markdown document. In your code chunk write the below code but replace the values of name/age/birthday with your own details). Note that text values and dates need to be contained in quotation marks but numerical values do not. Missing and/or unnecessary quotation marks are a common cause of code not working - remember this!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{name <-}\StringTok{ "Emily"} 
\NormalTok{age <-}\StringTok{ }\DecValTok{34}
\NormalTok{today <-}\StringTok{ }\KeywordTok{Sys.Date}\NormalTok{()}
\NormalTok{next_birthday <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-07-11"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{running-code}{%
\section{Running code}\label{running-code}}

When you're working in an R Markdown document, there are several ways to run your lines of code.

First, you can highlight the code you want to run and then click \texttt{Run} -\textgreater{} \texttt{Run\ Selected\ Line(s)}, however this is very slow.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/run1} 

}

\caption{Slow method of running code}\label{fig:img-run1}
\end{figure}

Alternatively, you can press the green ``play'' button at the top-right of the code chunk and this will run \textbf{all} lines of code in that chunk.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/run2} 

}

\caption{Slightly better method of running code}\label{fig:img-run2}
\end{figure}

Even better though is to learn some of the keyboard shortcuts for R Studio. To run a single line of code, make sure that the cursor is in the line of code you want to run (it can be anywhere) and press \texttt{ctrl\ +\ enter}. If you want to run all of the code in the code chunk, press \texttt{ctrl\ +\ shift\ +\ enter}. Learn these shortcuts, they will make your life easier!

\hypertarget{activity-6-run-your-code}{%
\section{Activity 6: Run your code}\label{activity-6-run-your-code}}

Run your code using one of the methods above. You should see the variables \texttt{name}, \texttt{age}, \texttt{today}, and \texttt{next\_birthday} appear in the environment pane in the top right corner.

\hypertarget{activity-7-inline-code}{%
\section{Activity 7: Inline code}\label{activity-7-inline-code}}

An incredibly useful feature of R Markdown is that R can insert values into your writing using \textbf{inline code}. If you've ever had to copy and paste a value or text from one file in to another, you'll know how easy it can be to make mistakes. Inline code avoids this. It's easier to show you what inline code does rather than to explain it so let's have a go.

First, copy and paste this text exactly (do not change \emph{anything}) to the \textbf{white space} underneath your code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{My name is }\StringTok{`}\DataTypeTok{r name}\StringTok{`}\NormalTok{ and I am }\StringTok{`}\DataTypeTok{r age}\StringTok{`}\NormalTok{ years old. It is }\StringTok{`}\DataTypeTok{r next_birthday - today}\StringTok{`}\NormalTok{ days until my birthday.}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-8-knitting-your-file}{%
\section{Activity 8: Knitting your file}\label{activity-8-knitting-your-file}}

Nearly finished! As our final step we are going to ``knit'' our file. This simply means that we're going to compile our code into a document that is more presentable. To do this click \texttt{Knit} -\textgreater{} \texttt{Knit\ to\ HMTL}. R Markdown will create a new HTML document and it will automatically save this file in your working directory.

As if by magic, that slightly odd bit of text you copied and pasted now appears as a normal sentence with the values pulled in from the objects you created.

\textbf{My name is Emily and I am 34 years old. It is 116 days until my birthday.}

We're not going to use this function very often in the rest of the course but hopefully you can see just how useful this would be when writing up a report with lots of numbers! R Markdown is an incredibly powerful and flexible format - this book was written using it! If you want to push yourself with R, additional functions and features of R Markdown would be a good place to start.

Before we finish, there are a few final things to note about knitting that will be useful for the portfolio and quantitative project:

\begin{itemize}
\tightlist
\item
  R Markdown will only knit if your code works - this is a good way of checking for the portfolio assignments whether you've written legal code!\\
\item
  You can choose to knit to a Word document rather than HTML. This can be useful for e.g., sharing with others, however, it may lose some functionality and it probably won't look as good so we'd recommend always knitting to HTML.\\
\item
  You can choose to knit to PDF, however, this requires an LaTex installation and is quite complicated. If you don't already know what LaTex is and how to use it, do not knit to PDF. If you do know how to use LaTex, you don't need us to give you instructions!
\item
  R will automatically open the knitted HTML file in the viewer, however, you can also navigate to the folder it is stored in and open the HTML file in your web browser (e.g., Chrome or Firefox).
\end{itemize}

\hypertarget{activity-9-make-r-your-own}{%
\section{Activity 9: Make R your own}\label{activity-9-make-r-your-own}}

Finally, you can customise how R Studio looks to make it feel more like your own personal version. Click \texttt{Tools} - \texttt{Global\ Options} - \texttt{Apperance}. You can change the default font, font size, and general apperance of R Studio, including using dark mode. Play around with the settings and see which one you prefe - you're going to spend a lot of time with R, it might as well look nice!

\hypertarget{finished}{%
\section{Finished}\label{finished}}

And you're done! On your very first time using R you've not only written functioning code but you've written a reproducible output! You could send someone else your R Markdown document and they would be able to produce exactly the same HTML document as you, just by pressing knit.

The key thing we want you to take away from this chapter is that R isn't scary. It might be very new to a lot of you, but we're going to take you through it step-by-step. You'll be amazed at how quickly you can start producing professional-looking data visualisations and analysis.

If you have any questions about anything contained in this chapter or in Programming Basics, please use the Research Methods forum on Teams.

\hypertarget{loading-data}{%
\chapter{Loading data}\label{loading-data}}

Part of becoming a psychologist is asking questions and gathering data to enable you to answer these questions effectively. It is very important that you understand all aspects of the research process such as experimental design, ethics, data management and visualisation.

In this chapter, you will continue to develop reproducible scripts. This means scripts that completely and transparently perform an analysis from start to finish in a way that yields the same result for different people using the same software on different computers. And transparency is a key value of science, as embodied in the ``trust but verify'' motto. When you do things reproducibly, others can understand and check your work.

This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. The topic of open science is a big debate in the scientific community at the moment. Some classic psychological experiments have been found not to be replicable and part of the explanation for this has been a historical lack of transparency about data and analysis methods. If you'd like more information on this, you may find the following articles interesting:

\href{https://www.theguardian.com/science/2015/aug/27/study-delivers-bleak-verdict-on-validity-of-psychology-experiment-results}{Study delivers bleak verdict on validity of psychology experiment results}

\href{http://www.apa.org/science/about/psa/2015/09/low-replicability.aspx}{Low replicability in psychological science}

As part of your skill development, it is important that you work with data so that you can become confident and competent in your management and analysis of data. In this course, we will work with real data that has been shared by other researchers.

\hypertarget{getting-data-ready-to-work-with}{%
\subsection{Getting data ready to work with}\label{getting-data-ready-to-work-with}}

In this chapter you will learn how to load the packages required to work with our data. You'll then load the data into R Studio before getting it organised into a sensible format that relates to our research question. If you can't remember what packages are, go back and revise \ref{packages}.

\hypertarget{activity-1-set-up}{%
\subsection{Activity 1: Set-up}\label{activity-1-set-up}}

Before we begin working with the data we need to do some set-up. If you need help with any of these steps, you should refer to Intro to R and Programming Basics:

\begin{itemize}
\tightlist
\item
  Download \texttt{ahi-cesd.csv} and \texttt{participant-info.csv} into your chapter folder.\\
\item
  Open R and ensure the environment is clear.
\item
  Set the working directory to your chapter folder.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Loading Data''.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk.\\
\item
  You can use the white space to take any notes that might help you for each activity.
\end{itemize}

\hypertarget{activity-2-load-in-the-package}{%
\subsection{Activity 2: Load in the package}\label{activity-2-load-in-the-package}}

Today we need to use the \texttt{tidyverse} package. You will use this package in every single chapter of this course as the functions it contains are those we use for data wrangling, descriptive statistics, and visualisation.

\begin{itemize}
\tightlist
\item
  To load the \texttt{tidyverse} type the following code into your code chunk and then run it.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{open-data}{%
\subsection{Open data}\label{open-data}}

For this chapter we are going to be using real data from the following paper:

\href{https://openpsychologydata.metajnl.com/articles/10.5334/jopd.35/}{Woodworth, R.J., O'Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from, `Web-based Positive Psychology Interventions: A Reexamination of Effectiveness'. Journal of Open Psychology Data, 6(1).}

We recommend that you read through this paper and open up the .csv files in order to understand the data better but briefly, the files contains data from two scales, the Authentic Happiness Inventory (AHI) and the Center for Epidemiological Studies Depression (CES-D) scale, as well as demographic information about participants.

\hypertarget{activity-3-read-in-data}{%
\subsection{Activity 3: Read in data}\label{activity-3-read-in-data}}

Now we can read in the data. To do this we will use the function \texttt{read\_csv()} that allows us to read in .csv files. There are also functions that allow you to read in .xlsx files and other formats, however in this course we will only use .csv files.

\begin{itemize}
\tightlist
\item
  First, we will create an object called \texttt{dat} that contains the data in the \texttt{ahi-cesd.csv} file. Then, we will create an object called \texttt{info} that contains the data in the \texttt{participant-info.csv}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{ (}\StringTok{"ahi-cesd.csv"}\NormalTok{)}
\NormalTok{pinfo <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"participant-info.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{danger}
There is also a function called \texttt{read.csv()}. Be very careful NOT
to use this function instead of \texttt{read\_csv()} as they have
different ways of naming columns. For the portfolio tasks, unless your
results match our exactly you will not get the marks which means you
need to be careful to use the right functions.
\end{danger}

\hypertarget{activity-4-check-yo-data}{%
\subsection{Activity 4: Check yo' data}\label{activity-4-check-yo-data}}

You should now see that the objects \texttt{dat} and \texttt{pinfo} have appeared in the environment pane. Whenever you read data into R you should always do an initial check to see that your data looks like you expected. There are several ways you can do this, try them all out to see how the results differ.

\begin{itemize}
\tightlist
\item
  In the environment pane, click on \texttt{dat} and \texttt{pinfo}. This will open the data to give you a spreadsheet-like view (although you can't edit it like in Excel)\\
\item
  In the environment pane, click the small blue play button to the left of \texttt{dat} and \texttt{pinfo}. This will show you the structure of the object information including the names of all the variables in that object and what type they are (also see \texttt{str(pinfo)})
\item
  Use \texttt{summary(pinfo)}
\item
  Use \texttt{head(pinfo)}
\item
  Just type the name of the object you want to view, e.g., \texttt{dat}.
\end{itemize}

\hypertarget{join}{%
\subsection{Activity 5: Join the files together}\label{join}}

We have two files, \texttt{dat} and \texttt{info} but what we really want is a single file that has both the data and the demographic information about the participants. R makes this very easy by using the function \texttt{inner\_join()}.

Remember to use the help function \texttt{?inner\_join} if you want more information about how to use a function and to use tab auto-complete to help you write your code.

The below code will create a new object \texttt{all\_dat} that has the data from both \texttt{dat} and \texttt{pinfo} and it will use the columns \texttt{id} and \texttt{intervention} to match the participants' data.

\begin{itemize}
\tightlist
\item
  Type and run this code and then view the new dataset using one of the methods from Activity 4.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_dat <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dat, }\CommentTok{# the first table you want to join}
                      \DataTypeTok{y =}\NormalTok{ pinfo, }\CommentTok{# the second table you want to join}
                      \DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{, }\StringTok{"intervention"}\NormalTok{)) }\CommentTok{# columns the two tables have in common}
\end{Highlighting}
\end{Shaded}

\hypertarget{ref1}{%
\subsection{Activity 6: Pull out variables of interest}\label{ref1}}

Our final step is to pull our variables of interest. Very frequently, datasets will have more variables and data than you actually want to use and it can make life easier to create a new object with just the data you need.

In this case, the file contains the responses to each individual question on both the AHI scale and the CESD scale as well as the total score (i.e., the sum of all the individual responses). For our analysis, all we care about is the total scores, as well as the demographic information about participants.

To do this we use the \texttt{select()} function to create a new object named \texttt{summarydata}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summarydata <-}\StringTok{ }\KeywordTok{select}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ all_dat, }\CommentTok{# name of the object to take data from}
\NormalTok{                      ahiTotal, cesdTotal, sex, age, educ, income, occasion,elapsed.days) }\CommentTok{# all the columns you want to keep}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Type and run the above code and then run \texttt{head(summarydata)}. If everything has gone to plan it should look something like this:
\end{itemize}

\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
ahiTotal & cesdTotal & sex & age & educ & income & occasion & elapsed.days\\
\hline
32 & 50 & 1 & 46 & 4 & 3 & 5 & 182.03\\
\hline
34 & 49 & 1 & 37 & 3 & 2 & 2 & 14.19\\
\hline
34 & 47 & 1 & 37 & 3 & 2 & 3 & 33.03\\
\hline
35 & 41 & 1 & 19 & 2 & 1 & 0 & 0.00\\
\hline
36 & 36 & 1 & 40 & 5 & 2 & 5 & 202.10\\
\hline
37 & 35 & 1 & 49 & 4 & 1 & 0 & 0.00\\
\hline
\end{tabular}

Finally, try knitting the file to HTML. And that's it, well done! Remember to save your Markdown in your chapter folder and make a note of any mistakes you made and how you fixed them. You have started on your journey to become a confident and competent member of the open scientific community!

\hypertarget{finished-1}{%
\subsubsection{Finished!}\label{finished-1}}

There is no portfolio assessment this week, instead, use the time to get comfortable with what we've covered already and revise the activities and support materials presented so far if needed. If you're feeling comfortable with R, you can work your way through this book at your own pace or push yourself by using the additional resources highlighted in \ref{ref3}.

\hypertarget{debugging-tips-1}{%
\section{Debugging tips}\label{debugging-tips-1}}

\begin{itemize}
\tightlist
\item
  When you downloaded the files from Moodle did you save the file names \textbf{exactly} as they were originally? If you download the file more than once you will find your computer may automatically add a number to the end of the file name. \texttt{data.csv} is not the same as \texttt{data(1).csv}. Pay close attention to names!
\item
  Have you used the \textbf{exact} same object names as we did in each activity? Remember, \texttt{name} is different to \texttt{Name}. In order to make sure you can follow along with this book, pay special attention to ensuring you use the same object names as we do.\\
\item
  Have you used quotation marks where needed?\\
\item
  Have you accidentally deleted any back ticks (```) from the beginning or end of code chunks?
\end{itemize}

\hypertarget{test-yourself-1}{%
\section{Test yourself}\label{test-yourself-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When loading in a .csv file, which function should you use?
\end{enumerate}

 read\_csv() read.csv()

Explain this answer

Remember, in this course we use \texttt{read\_csv()} and it is important for the portfolio assignment that you use this function otherwise you may find that the variable names are slightly different and you won't get the marks

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The function \texttt{inner\_join()} takes the arguments \texttt{x}, \texttt{y}, \texttt{by}. What does \texttt{by} do?
\end{enumerate}

 Specifies the first table to join Specifies the second table to join Specifies the column to join by that both tables have in common

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  What does the function \texttt{select()} do?
   Keeps only the observations you specify Keeps only the variables you specify Keeps only the objects you specify
\end{enumerate}

\hypertarget{data-wrangling-1}{%
\chapter{Data wrangling 1}\label{data-wrangling-1}}

Data comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g.~a subject), and each column stands for a different variable (e.g.~a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place.

It may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80\% of time spent on data analysis involves such data preparation tasks (Dasu \& Johnson, 2003)!

Many people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand.

In short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible.

In this lesson you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues.

\hypertarget{tidyverse}{%
\section{Tidyverse}\label{tidyverse}}

Tidyverse (\url{https://www.tidyverse.org/}) is a collection of R packages created by world-famous data scientist Hadley Wickham.

Tidyverse contains six core packages: \texttt{dplyr}, \texttt{tidyr}, \texttt{readr}, \texttt{purrr}, \texttt{ggplot2}, and \texttt{tibble}. Last week when you typed \texttt{library(tidyverse)} into R, you will have seen that it loads in all of these packages in one go. Within these six core packages, you should be able to find everything you need to wrangle and visualise your data.

In this chapter, we are going to focus on the \texttt{dplyr} package, which contains six important functions:

\begin{itemize}
\tightlist
\item
  \texttt{select()} Include or exclude certain variables (columns)
\item
  \texttt{filter()} Include or exclude certain observations (rows)
\item
  \texttt{mutate()} Create new variables (columns)
\item
  \texttt{arrange()} Change the order of observations (rows)
\item
  \texttt{group\_by()} Organize the observations into groups
\item
  \texttt{summarise()} Derive aggregate variables for groups of observations
\end{itemize}

These six functions are known as 'single table verbs' because they only operate on one table at a time. Although the operations of these functions may seem very simplistic, it's amazing what you can accomplish when you string them together: Hadley Wickham has claimed that 90\% of data analysis can be reduced to the operations described by these six functions.

\hypertarget{the-babynames-database}{%
\section{\texorpdfstring{The \texttt{babynames} database}{The babynames database}}\label{the-babynames-database}}

To demonstrate the power of the six \texttt{dplyr} verbs, we will use them to work with the babynames data from the \texttt{babynames} package. The babynames dataset has historical information about births of babies in the U.S.

\hypertarget{activity-1-set-up-1}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-1}}

Do the following. If you need help, consult Intro to R or Programming Basics.

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your chapter folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Data wrangling 1''.\\
\item
  If you are working on your own computer, install the package \texttt{babynames}. Remember, \textbf{never install packages if you are working on a university computer}.
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the packages \texttt{tidyverse} and \texttt{babynames} using \texttt{library()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(babynames)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-2-look-at-the-data}{%
\subsection{Activity 2: Look at the data}\label{activity-2-look-at-the-data}}

The package \texttt{babynames} contains an object of the same name that contains all the data about babynames.

\begin{itemize}
\tightlist
\item
  View a preview of this dataset by typing \texttt{babynames} in to the console. You should see the following output:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 5
##     year sex   name          n   prop
##    <dbl> <chr> <chr>     <int>  <dbl>
##  1  1880 F     Mary       7065 0.0724
##  2  1880 F     Anna       2604 0.0267
##  3  1880 F     Emma       2003 0.0205
##  4  1880 F     Elizabeth  1939 0.0199
##  5  1880 F     Minnie     1746 0.0179
##  6  1880 F     Margaret   1578 0.0162
##  7  1880 F     Ida        1472 0.0151
##  8  1880 F     Alice      1414 0.0145
##  9  1880 F     Bertha     1320 0.0135
## 10  1880 F     Sarah      1288 0.0132
## # ... with 1,924,655 more rows
\end{verbatim}

The first line tells us that the object we are looking at is in a \texttt{tibble} with information on five variables with over 1.9 million rows. Yes, this dataset contains 1.8 \textbf{million} observations. Interested in analyzing these data by hand? No thanks! A tibble is basically a table of data presenting a two dimensional array of your data.

Each row in the table represents data about births for a given name and sex in a given year. The variables are:

\begin{longtable}[]{@{}lll@{}}
\toprule
variable & type & description\tabularnewline
\midrule
\endhead
year & double (numeric) & year of birth\tabularnewline
sex & character & recorded sex of baby (F = female, M = male)\tabularnewline
name & character & forename given to baby\tabularnewline
n & integer & number of babies given that name\tabularnewline
prop & double (numeric) & proportion of all babies of that sex\tabularnewline
\bottomrule
\end{longtable}

The first row of the table tells us that in the year 1880, there were 7065 baby girls born in the U.S. who were given the name Mary, and this accounted for about 7\% of all baby girls.

\hypertarget{activity-3-your-first-plot}{%
\subsection{Activity 3: Your first plot}\label{activity-3-your-first-plot}}

\begin{itemize}
\tightlist
\item
  Type the code below into the Activity 3 code chunk and run it.
\end{itemize}

The code might not make much sense to you right now, but don't worry about not understanding it yet! The point is show you how much you can accomplish with very little code. The code creates a graph showing the popularity of four girl baby names - Alexandra, Beverly, Emily, and Kathleen - from 1880 to 2014. You should see Figure \ref{fig:babynames-plot} appear, which shows the proportion of each name across different years - you can plug in different names if you like and see how the plot changes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\NormalTok{babynames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(name }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Emily"}\NormalTok{,}\StringTok{"Kathleen"}\NormalTok{,}\StringTok{"Alexandra"}\NormalTok{,}\StringTok{"Beverly"}\NormalTok{), sex}\OperatorTok{==}\StringTok{"F"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year,}\DataTypeTok{y =}\NormalTok{ prop, }\DataTypeTok{colour=}\NormalTok{name))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{04-wrangling-1_files/figure-latex/babynames-plot-1} 

}

\caption{Proportion of four baby names from 1880 to 2014}\label{fig:babynames-plot}
\end{figure}

\hypertarget{activity-4-selecting-variables-of-interest}{%
\subsection{Activity 4: Selecting variables of interest}\label{activity-4-selecting-variables-of-interest}}

There are two numeric measurements of name popularity, \texttt{prop} (the proportion of all babies with each name) is probably more useful than \texttt{n} (total number of babies with that name), because it takes into account that different numbers of babies are born in different years.

Just like in Chapter \ref{ref1}, if we wanted to create a dataset that only includes certain variables, we can use the \texttt{select()} function from the \texttt{dplyr} package. Run the below code to only select the columns \texttt{year}, \texttt{sex}, \texttt{name} and \texttt{prop}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, }\CommentTok{# the object you want to select variables from}
\NormalTok{       year, sex, name, prop) }\CommentTok{# the variables you want to select}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 4
##     year sex   name        prop
##    <dbl> <chr> <chr>      <dbl>
##  1  1880 F     Mary      0.0724
##  2  1880 F     Anna      0.0267
##  3  1880 F     Emma      0.0205
##  4  1880 F     Elizabeth 0.0199
##  5  1880 F     Minnie    0.0179
##  6  1880 F     Margaret  0.0162
##  7  1880 F     Ida       0.0151
##  8  1880 F     Alice     0.0145
##  9  1880 F     Bertha    0.0135
## 10  1880 F     Sarah     0.0132
## # ... with 1,924,655 more rows
\end{verbatim}

Alternatively, you can also tell R which variables you don't want, in this case, rather than telling R to select \texttt{year}, \texttt{sex}, \texttt{name} and \texttt{prop}, we can simply tell it to drop the column \texttt{n} using the minus sign \texttt{-} before the variable name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, }\OperatorTok{-}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 4
##     year sex   name        prop
##    <dbl> <chr> <chr>      <dbl>
##  1  1880 F     Mary      0.0724
##  2  1880 F     Anna      0.0267
##  3  1880 F     Emma      0.0205
##  4  1880 F     Elizabeth 0.0199
##  5  1880 F     Minnie    0.0179
##  6  1880 F     Margaret  0.0162
##  7  1880 F     Ida       0.0151
##  8  1880 F     Alice     0.0145
##  9  1880 F     Bertha    0.0135
## 10  1880 F     Sarah     0.0132
## # ... with 1,924,655 more rows
\end{verbatim}

Note that \texttt{select()} does not change the original tibble, but makes a new tibble with the specified columns. If you don't save this new tibble to an object, it won't be saved. If you want to keep this new dataset, create a new object. When you run this code, you will see your new tibble appear in the environment pane.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_dat <-}\StringTok{ }\KeywordTok{select}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, }\OperatorTok{-}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-5-arranging-the-data}{%
\subsection{Activity 5: Arranging the data}\label{activity-5-arranging-the-data}}

The function \texttt{arrange()} will sort the rows in the table according to the columns you supply. Try running the following code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, }\CommentTok{# the data you want to sort}
\NormalTok{        name) }\CommentTok{# the variable you want to sort by}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 5
##     year sex   name      n       prop
##    <dbl> <chr> <chr> <int>      <dbl>
##  1  2007 M     Aaban     5 0.00000226
##  2  2009 M     Aaban     6 0.00000283
##  3  2010 M     Aaban     9 0.00000439
##  4  2011 M     Aaban    11 0.00000542
##  5  2012 M     Aaban    11 0.00000543
##  6  2013 M     Aaban    14 0.00000694
##  7  2014 M     Aaban    16 0.00000783
##  8  2015 M     Aaban    15 0.00000736
##  9  2016 M     Aaban     9 0.00000446
## 10  2017 M     Aaban    11 0.0000056 
## # ... with 1,924,655 more rows
\end{verbatim}

The data are now sorted in ascending alphabetical order by name. The default is to sort in ascending order. If you want it descending, wrap the name of the variable in the \texttt{desc()} function. For instance, to sort by year in descending order, run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(babynames,}\KeywordTok{desc}\NormalTok{(year)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 5
##     year sex   name          n    prop
##    <dbl> <chr> <chr>     <int>   <dbl>
##  1  2017 F     Emma      19738 0.0105 
##  2  2017 F     Olivia    18632 0.00994
##  3  2017 F     Ava       15902 0.00848
##  4  2017 F     Isabella  15100 0.00805
##  5  2017 F     Sophia    14831 0.00791
##  6  2017 F     Mia       13437 0.00717
##  7  2017 F     Charlotte 12893 0.00688
##  8  2017 F     Amelia    11800 0.00629
##  9  2017 F     Evelyn    10675 0.00569
## 10  2017 F     Abigail   10551 0.00563
## # ... with 1,924,655 more rows
\end{verbatim}

You can also sort by more than one column. What do you think the following code will do?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(babynames, }\KeywordTok{desc}\NormalTok{(year), }\KeywordTok{desc}\NormalTok{(sex), }\KeywordTok{desc}\NormalTok{(prop)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-6-using-filter-to-select-observations}{%
\subsection{Activity 6: Using filter to select observations}\label{activity-6-using-filter-to-select-observations}}

We have previously used \texttt{select()} to select certain variables or columns, however, frequently you will also want to select only certain observations or rows, for example, only babies born after 1999, or only babies named ``Mary''. You do this using the verb \texttt{filter()}. The \texttt{filter()} function is a bit more involved than the other verbs, and requires more detailed explanation, but this is because it is also extremely powerful.

Here is an example of filter, can you guess what it will do?

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{filter}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, year }\OperatorTok{>}\StringTok{ }\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first part of the code tells the function to use the object \texttt{babynames}. The second argument, \texttt{year\ \textgreater{}\ 2000}, is what is known as a \textbf{Boolean expression}: an expression whose evaluation results in a value of TRUE or FALSE. What \texttt{filter()} does is include any observations (rows) for which the expression evaluates to TRUE, and exclude any for which it evaluates to FALSE. So in effect, behind the scenes, \texttt{filter()} goes through the entire set of 1.8 million observations, row by row, checking the value of year for each row, keeping it if the value is greater than 2000, and rejecting it if it is less than 2000. To see how a boolean expression works, consider the code below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{years <-}\StringTok{ }\DecValTok{1996}\OperatorTok{:}\DecValTok{2005}
\NormalTok{years}
\NormalTok{years }\OperatorTok{>}\StringTok{ }\DecValTok{2000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005
##  [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
\end{verbatim}

You can see that the expression \texttt{years\ \textgreater{}\ 2000} returns a \textbf{logical vector} (a vector of TRUE and FALSE values), where each element represents whether the expression is true or false for that element. For the first five elements (1996 to 2000) it is false, and for the last five elements (2001 to 2005) it is true.

Here are the most commonly used Boolean expressions.

\begin{longtable}[]{@{}lll@{}}
\toprule
Operator & Name & is TRUE if and only if\tabularnewline
\midrule
\endhead
A \textless{} B & less than & A is less than B\tabularnewline
A \textless{}= B & less than or equal & A is less than or equal to B\tabularnewline
A \textgreater{} B & greater than & A is greater than B\tabularnewline
A \textgreater{}= B & greater than or equal & A is greater than or equal to B\tabularnewline
A == B & equivalence & A exactly equals B\tabularnewline
A != B & not equal & A does not exactly equal B\tabularnewline
A \%in\% B & in & A is an element of vector B\tabularnewline
\bottomrule
\end{longtable}

If you want only those observations for a specific name (e.g., Mary), you use the equivalence operator \texttt{==}. Note that you use double equal signs, not a single equal sign.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(babynames, name }\OperatorTok{==}\StringTok{ "Mary"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 268 x 5
##     year sex   name      n     prop
##    <dbl> <chr> <chr> <int>    <dbl>
##  1  1880 F     Mary   7065 0.0724  
##  2  1880 M     Mary     27 0.000228
##  3  1881 F     Mary   6919 0.0700  
##  4  1881 M     Mary     29 0.000268
##  5  1882 F     Mary   8148 0.0704  
##  6  1882 M     Mary     30 0.000246
##  7  1883 F     Mary   8012 0.0667  
##  8  1883 M     Mary     32 0.000284
##  9  1884 F     Mary   9217 0.0670  
## 10  1884 M     Mary     36 0.000293
## # ... with 258 more rows
\end{verbatim}

If you wanted all the names except Mary, you use the `not equals' operator:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(babynames, name}\OperatorTok{!=}\StringTok{"Mary"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,397 x 5
##     year sex   name          n   prop
##    <dbl> <chr> <chr>     <int>  <dbl>
##  1  1880 F     Anna       2604 0.0267
##  2  1880 F     Emma       2003 0.0205
##  3  1880 F     Elizabeth  1939 0.0199
##  4  1880 F     Minnie     1746 0.0179
##  5  1880 F     Margaret   1578 0.0162
##  6  1880 F     Ida        1472 0.0151
##  7  1880 F     Alice      1414 0.0145
##  8  1880 F     Bertha     1320 0.0135
##  9  1880 F     Sarah      1288 0.0132
## 10  1880 F     Annie      1258 0.0129
## # ... with 1,924,387 more rows
\end{verbatim}

And if you wanted names from a defined set - e.g., names of British queens - you can use \texttt{\%in\%}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(babynames, name }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{,}\StringTok{"Elizabeth"}\NormalTok{,}\StringTok{"Victoria"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 772 x 5
##     year sex   name          n      prop
##    <dbl> <chr> <chr>     <int>     <dbl>
##  1  1880 F     Mary       7065 0.0724   
##  2  1880 F     Elizabeth  1939 0.0199   
##  3  1880 F     Victoria     93 0.000953 
##  4  1880 M     Mary         27 0.000228 
##  5  1880 M     Elizabeth     9 0.0000760
##  6  1881 F     Mary       6919 0.0700   
##  7  1881 F     Elizabeth  1852 0.0187   
##  8  1881 F     Victoria    117 0.00118  
##  9  1881 M     Mary         29 0.000268 
## 10  1882 F     Mary       8148 0.0704   
## # ... with 762 more rows
\end{verbatim}

This gives you data for the names in the vector on the right hand side of \texttt{\%in\%}. You can always invert an expression to get its opposite. So, for instance, if you instead wanted to get rid of all Marys, Elizabeths, and Victorias you would use the following:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(babynames, }\OperatorTok{!}\NormalTok{(name }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{,}\StringTok{"Elizabeth"}\NormalTok{,}\StringTok{"Victoria"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,923,893 x 5
##     year sex   name         n   prop
##    <dbl> <chr> <chr>    <int>  <dbl>
##  1  1880 F     Anna      2604 0.0267
##  2  1880 F     Emma      2003 0.0205
##  3  1880 F     Minnie    1746 0.0179
##  4  1880 F     Margaret  1578 0.0162
##  5  1880 F     Ida       1472 0.0151
##  6  1880 F     Alice     1414 0.0145
##  7  1880 F     Bertha    1320 0.0135
##  8  1880 F     Sarah     1288 0.0132
##  9  1880 F     Annie     1258 0.0129
## 10  1880 F     Clara     1226 0.0126
## # ... with 1,923,883 more rows
\end{verbatim}

You can include as many expressions as you like as additional arguments to \texttt{filter()} and it will only pull out the rows for which all of the expressions for that row evaluate to TRUE. For instance, \texttt{filter(babynames,\ year\ \textgreater{}\ 2000,\ prop\ \textgreater{}\ .01)} will pull out only those observations beyond the year 2000 that represent greater than 1\% of the names for a given sex; any observation where either expression is false will be excluded. This ability to string together criteria makes \texttt{filter()} a very powerful member of the Wickham Six.

\textbf{Remember that this section exists. It will contain a lot of the answers to problems you face when wrangling data!}

\hypertarget{mutate}{%
\subsection{Activity 7: Creating new variables}\label{mutate}}

Sometimes we need to create a new variable that doesn't exist in our dataset. For instance, we might want to figure out what decade a particular year belongs to. To create new variables, we use the function \texttt{mutate()}. Note that if you want to save this new column, you need to save it to an object. Here, you are mutating a new column and attaching it to the \texttt{new\_dat} object you created in Activity 4.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_dat <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ babynames, }\CommentTok{# the tibble you want to add a colum to}
                  \DataTypeTok{decade =} \KeywordTok{floor}\NormalTok{(year}\OperatorTok{/}\DecValTok{10}\NormalTok{) }\OperatorTok{*}\DecValTok{10}\NormalTok{) }\CommentTok{# new column name = what you want it to contain}
\NormalTok{new_dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,924,665 x 6
##     year sex   name          n   prop decade
##    <dbl> <chr> <chr>     <int>  <dbl>  <dbl>
##  1  1880 F     Mary       7065 0.0724   1880
##  2  1880 F     Anna       2604 0.0267   1880
##  3  1880 F     Emma       2003 0.0205   1880
##  4  1880 F     Elizabeth  1939 0.0199   1880
##  5  1880 F     Minnie     1746 0.0179   1880
##  6  1880 F     Margaret   1578 0.0162   1880
##  7  1880 F     Ida        1472 0.0151   1880
##  8  1880 F     Alice      1414 0.0145   1880
##  9  1880 F     Bertha     1320 0.0135   1880
## 10  1880 F     Sarah      1288 0.0132   1880
## # ... with 1,924,655 more rows
\end{verbatim}

In this case, you are creating a new column decade which has the decade each year appears in. This is calculated using the command \texttt{decade\ =\ floor(year/10)*10}.

\hypertarget{activity-8-grouping-and-summarising}{%
\subsection{Activity 8: Grouping and summarising}\label{activity-8-grouping-and-summarising}}

Most quantitative analyses will require you to summarise your data somehow, for example, by calculating the mean, median or a sum total of your data. You can perform all of these operations using the function \texttt{summarise()}.

First, let's use the object \texttt{dat} that just has the data for the four girls names, Alexandra, Beverly, Emily, and Kathleen. To start off, we're simply going to calculate the total number of babies across all years that were given one of these four names.

It's useful to get in the habit of translating your code into full sentences to make it easier to figure out what's happening. You can read the below code as ``run the function summarise using the data in the object \texttt{dat} to create a new variable named \texttt{total} that is the result of adding up all the numbers in the column \texttt{n}''.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ dat, }\CommentTok{# the data you want to use}
          \DataTypeTok{total =} \KeywordTok{sum}\NormalTok{(n)) }\CommentTok{# result name = operation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##     total
##     <int>
## 1 2161374
\end{verbatim}

\texttt{summarise()} becomes even more powerful when combined with the final \texttt{dplyr} function, \texttt{group\_by()}. Quite often, you will want to produce your summary statistics broken down by groups, for examples, the scores of participants in different conditions, or the reading time for native and non-native speakers.

There are two ways you can use \texttt{group\_by()}. First, you can create a new, grouped object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group_dat <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ dat, }\CommentTok{# the data you want to group}
\NormalTok{                      name) }\CommentTok{# the variable you want to group by}
\end{Highlighting}
\end{Shaded}

If you look at this object in the viewer, it won't look any different to the original \texttt{dat}, however, the underlying structure has changed. Let's run the above summarise code again, but now using the grouped data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ group_dat, }
          \DataTypeTok{total =} \KeywordTok{sum}\NormalTok{(n)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##   name       total
##   <chr>      <int>
## 1 Alexandra 231364
## 2 Beverly   376914
## 3 Emily     841491
## 4 Kathleen  711605
\end{verbatim}

\texttt{summarise()} has performed exactly the same operation as before - adding up the total number in the column \texttt{n} - but this time it has done is separately for each group, which in this case was the variable \texttt{name}.

You can request multiple summary calculations to be performed in the same function. For example, the following code calculates the mean and median number of babies given each name every year.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(group_dat,}
          \DataTypeTok{mean_year =} \KeywordTok{mean}\NormalTok{(n),}
          \DataTypeTok{median_year =} \KeywordTok{median}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
##   name      mean_year median_year
##   <chr>         <dbl>       <dbl>
## 1 Alexandra     1977.        192 
## 2 Beverly       3089.        710.
## 3 Emily         6098.       1392.
## 4 Kathleen      5157.       3098
\end{verbatim}

You can also add multiple grouping variables. For example, the following code groups \texttt{new\_dat} by \texttt{sex} and \texttt{decade} and then calculates the summary statistics to give us the mean and median number of male and female babies in each decade.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group_new_dat <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(new_dat, sex, decade)}
\KeywordTok{summarise}\NormalTok{(group_new_dat,}
          \DataTypeTok{mean_year =} \KeywordTok{mean}\NormalTok{(n),}
          \DataTypeTok{median_year =} \KeywordTok{median}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 28 x 4
## # Groups:   sex [2]
##    sex   decade mean_year median_year
##    <chr>  <dbl>     <dbl>       <dbl>
##  1 F       1880      111.          13
##  2 F       1890      128.          13
##  3 F       1900      131.          12
##  4 F       1910      187.          12
##  5 F       1920      211.          12
##  6 F       1930      214.          12
##  7 F       1940      262.          12
##  8 F       1950      288.          13
##  9 F       1960      235.          12
## 10 F       1970      147.          11
## # ... with 18 more rows
\end{verbatim}

\hypertarget{activity-9-pipes}{%
\subsection{Activity 9: Pipes}\label{activity-9-pipes}}

The final activity for this pre-lab essentially repeats what we've already covered but in a slightly different way. In the previous activities, you created new objects with new variables or groupings and then you called \texttt{summarise()} on those new objects in separate lines of code. As a result, you had multiple objects in your environment pane and you need to make sure that you keep track of the different names.

Instead, you can use \textbf{pipes}. Pipes are written as \texttt{\%\textgreater{}\%}and they should be read as ``and then''. Pipes allow you to string together `sentences' of code into `paragraphs' so that you don't need to create intermediary objects. Again, it is easier to show than tell.

The below code does \textbf{exactly} the same as all the code we wrote above but it only creates one object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pipe_summary <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(babynames, }\DataTypeTok{decade =} \KeywordTok{floor}\NormalTok{(year}\OperatorTok{/}\DecValTok{10}\NormalTok{) }\OperatorTok{*}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(name }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Emily"}\NormalTok{,}\StringTok{"Kathleen"}\NormalTok{,}\StringTok{"Alexandra"}\NormalTok{,}\StringTok{"Beverly"}\NormalTok{), sex}\OperatorTok{==}\StringTok{"F"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(name, decade) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_decade =} \KeywordTok{mean}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

The reason that this function is called a pipe is because it `pipes' the data through to the next function. When you wrote the code previously, the first argument of each function was the dataset you wanted to work on. When you use pipes it will automatically take the data from the previous line of code so you don't need to specify it again.

\begin{try}
When learning to code it can be a useful practice to read your code `out
loud' in full sentences to help you understand what it is doing. You can
read the code above as ``create a new variable called decade AND THEN
only keep the names Emily, Kathleen, Alexandra and Beverly that belong
to female babies AND THEN group the dataset by name and decade AND THEN
calculate the mean number of babies with each name per decade.'' Try
doing this each time you write a new bit of code.
\end{try}

Some people find pipes a bit tricky to understand from a conceptual point of view, however, it's well worth learning to use them as when your code starts getting longer they are much more efficient and mean you have to write less code which is always a good thing!

\hypertarget{data-wrangling-2}{%
\chapter{Data wrangling 2}\label{data-wrangling-2}}

One of the key skills in an researcher's toolbox is the ability to work with data. When you run an experiment you get lots of data in various files. For instance, it is not uncommon for an experimental software to create a new file for every participant you run and for each participant's file to contain numerous columns and rows of data, only some of which are important. Being able to wrangle that data, manipulate it into different layouts, extract the parts you need, and summarise it, is one of the most important skills we will help you learn.

Over this course you will develop your skills in working with data. This chapter focuses on organizing data using the \texttt{tidyverse} package that you have read about in Week 1. Over the course, you will learn the main functions for data wrangling and how to use them, and we will use a number of different datasets to give you a wide range of exposure to what Psychology is about, and to reiterate that the same skills apply across different datasets. \textbf{The skills don't change, just the data!}

There are some questions to answer as you go along to test your skills: use the example code as a guide and the solutions are at the bottom. Remember to be pro-active in your learning, work together as a community, and if you get stuck use the \textbf{\href{https://www.rstudio.com/resources/cheatsheets/}{cheatsheets}}. The key cheatsheet for this activity is the Data Transformation with dplyr.

\hypertarget{learning-to-wrangle-is-there-a-chastity-belt-on-perception}{%
\section{Learning to wrangle: Is there a chastity belt on perception}\label{learning-to-wrangle-is-there-a-chastity-belt-on-perception}}

Nearly all data in research methods is stored in two-dimensional tables, either called data-frames, tables or tibbles. There are other ways of storing data that you will discover in time but mainly we will be using tibbles (if you would like more info, type \texttt{vignette("tibble")} in the console). A tibble is really just a table of data with columns and rows of information. But within that table you can get different types of data, i.e.~numeric, integer, and character.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.16\columnwidth}\raggedright
Type of Data\strut
\end{minipage} & \begin{minipage}[b]{0.78\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.16\columnwidth}\raggedright
Numeric\strut
\end{minipage} & \begin{minipage}[t]{0.78\columnwidth}\raggedright
Numbers including decimals\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright
Integer\strut
\end{minipage} & \begin{minipage}[t]{0.78\columnwidth}\raggedright
Numbers without decimals\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright
Character\strut
\end{minipage} & \begin{minipage}[t]{0.78\columnwidth}\raggedright
Tends to contain letters or be words\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright
Factor\strut
\end{minipage} & \begin{minipage}[t]{0.78\columnwidth}\raggedright
Nominal (categorical). Can be words or numbers (e.g., male/1, female/2)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Today we are going to be using data from this paper: \href{http://journals.sagepub.com/doi/abs/10.1177/0956797617730892}{Is there a Chastity Belt on Perception}. You can read the full paper if you like, but we will summarise the paper for you. The paper asks, \textbf{does your ability to perform an action influence your perception?} For instance, does your ability to hit a tennis ball influence how fast you perceive the ball to be moving? Or to phrase another way, do expert tennis players perceive the ball moving slower than novice tennis players? This experiment does not use tennis players however, they used the Pong task: ``a computerised game in which participants aim to block moving balls with various sizes of paddles''. A bit like a very classic retro arcade game. Participants tend to estimate the balls as moving faster when they have to block it with a smaller paddle as opposed to when they have a bigger paddle. You can read the paper to get more details if you wish but hopefully that gives enough of an idea to help you understand the wrangling we will do on the data. We have cleaned up the data a little to start with. Let's begin!

\hypertarget{activity-1-set-up-2}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-2}}

\begin{itemize}
\tightlist
\item
  Download PongBlueRedBack 1-16 Codebook.csv into your chapter folder.\\
\item
  Set the working directory to your chapter folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Data wrangling 2''.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk.
\item
  Copy and paste the below code into this code chunk and then run the code.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\NormalTok{pong_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"PongBlueRedBack 1-16 Codebook.csv"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pong_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Participant     JudgedSpeed      PaddleLength   BallSpeed  
##  Min.   : 1.00   Min.   :0.0000   Min.   : 50   Min.   :2.0  
##  1st Qu.: 4.75   1st Qu.:0.0000   1st Qu.: 50   1st Qu.:3.0  
##  Median : 8.50   Median :1.0000   Median :150   Median :4.5  
##  Mean   : 8.50   Mean   :0.5471   Mean   :150   Mean   :4.5  
##  3rd Qu.:12.25   3rd Qu.:1.0000   3rd Qu.:250   3rd Qu.:6.0  
##  Max.   :16.00   Max.   :1.0000   Max.   :250   Max.   :7.0  
##   TrialNumber     BackgroundColor      HitOrMiss       BlockNumber   
##  Min.   :  1.00   Length:4608        Min.   :0.0000   Min.   : 1.00  
##  1st Qu.: 72.75   Class :character   1st Qu.:0.0000   1st Qu.: 3.75  
##  Median :144.50   Mode  :character   Median :1.0000   Median : 6.50  
##  Mean   :144.50                      Mean   :0.6866   Mean   : 6.50  
##  3rd Qu.:216.25                      3rd Qu.:1.0000   3rd Qu.: 9.25  
##  Max.   :288.00                      Max.   :1.0000   Max.   :12.00
\end{verbatim}

\hypertarget{activity-2-look-at-your-data}{%
\section{Activity 2: Look at your data}\label{activity-2-look-at-your-data}}

Let's have a look at the \texttt{pong\_data} and see how it is organized. Type \texttt{pong\_data} in your console window.

In the dataset you will see that each row (observation) represents one trial per participant and that there were 288 trials for each of the 16 participants. The columns (variables) we have in the dataset are as follows:

\begin{longtable}[]{@{}cll@{}}
\toprule
Variable & Type & Description\tabularnewline
\midrule
\endhead
Participant & integer & participant number\tabularnewline
JudgedSpeed & integer & speed judgement (1=fast, 0=slow)\tabularnewline
PaddleLength & integer & paddle length (pixels)\tabularnewline
BallSpeed & integer & ball speed (2 pixels/4ms)\tabularnewline
TrialNumber & integer & trial number\tabularnewline
BackgroundColor & character & background display colour\tabularnewline
HitOrMiss & integer & hit ball=1, missed ball=0\tabularnewline
BlockNumber & integer & block number (out of 12 blocks)\tabularnewline
\bottomrule
\end{longtable}

We will use this data to master our skills of the Wickham Six verbs, taking each verb in turn. You should refer to the explanations and example code in Week 1 to help you complete these. There are \textbf{6 verbs to work through} and after that we will briefly recap on two other functions before finishing with a quick look at pipes. Try each activity and ask your peers or your tutor if you need help.

\hypertarget{activity-3-select}{%
\section{\texorpdfstring{Activity 3: \textbf{\texttt{select()}}}{Activity 3: select()}}\label{activity-3-select}}

Either by inclusion (telling R all the variables you want to keep) or exclusion (telling R which variables you want to drop), select only the \texttt{Participant}, \texttt{PaddleLength}, \texttt{TrialNumber}, \texttt{BackgroundColor} and \texttt{HitOrMiss} columns from \texttt{pong\_data} and store it in a new object named \texttt{select\_dat}.

\hypertarget{activity-4-reorder-the-variables}{%
\section{Activity 4: Reorder the variables}\label{activity-4-reorder-the-variables}}

\texttt{select()} can also be used to reorder the columns in a table as the new table will display the variables in the order that you wrote them. Use \texttt{select()} to keep only the columns \texttt{Participant}, \texttt{JudgedSpeed}, \texttt{BallSpeed}, \texttt{TrialNumber}, and \texttt{HitOrMiss} but have them display in alphabetical order, left to right. Save this table in a new object named \texttt{reorder\_dat}.

\hypertarget{activity-5-arrange-f}{%
\section{\texorpdfstring{Activity 5: \textbf{\texttt{arrange()}} F}{Activity 5: arrange() F}}\label{activity-5-arrange-f}}

Arrange the data by two variables: \texttt{HitOrMiss} (putting hits - 1 - first), and \texttt{JudgedSpeed} (fast judgement - 1 - first). Do not store this output in a new object.

\hypertarget{activity-6-filter}{%
\section{\texorpdfstring{Activity 6: \texttt{filter()}}{Activity 6: filter()}}\label{activity-6-filter}}

Use \texttt{filter()} to extract all Participants that had a fast speed judgement, for speeds 2, 4, 5, and 7, but missed the ball. Store this remaining data in a new object called \texttt{pong\_fast\_miss}

Helpful Hint

There are three parts to this filter so it is best to think about them individually and then combine them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Filter all fast speed judgements (\texttt{JudgedSpeed})
\item
  Filter for the speeds 2, 4, 5 and 7 (\texttt{BallSpeed})
\item
  Filter for all Misses (\texttt{HitOrMiss})
\end{enumerate}

You could do this in three filters where each one uses the output of the preceding one, or remember that filter functions can take more than one argument. Also, because the \texttt{JudgedSpeed} and \texttt{HitOrMiss} are Integer you will need \texttt{==} instead of just \texttt{=}.

\begin{warning}
The filter function is very useful but if used wrongly can give you very
misleading findings. This is why it is very important to always check
your data after you perform an action. Let's say you are working in
comparative psychology and have run a study looking at how cats, dogs
and horses perceive emotion. Let's say the data is all stored in the
tibble \texttt{animal\_data} and there is a column called
\texttt{animals} that tells you what type of animal your participant
was. Imagine you wanted all the data from just cats:

\texttt{filter(animal\_data,\ animals\ ==\ "cat")}

Exactly! But what if you wanted cats and dogs?

\texttt{filter(animal\_data,\ animals\ ==\ "cat",\ animals\ ==\ "dog")}

Right? Wrong! This actually says ``give me everything that is a cat and
a dog''. But nothing is a cat and a dog, that would be weird - like a
dat or a cog. In fact you want everything that is either a cat
\textbf{or} a dog, which is

filter(animal\_data, animals == ``cat'' \textbar{} animals == ``dog'')`

The vertical line is the symbol for Or. So always pay attention to what
you want and most importantly to what your code produces.
\end{warning}

\hypertarget{recode}{%
\section{\texorpdfstring{Activity 7: \texttt{mutate()}}{Activity 7: mutate()}}\label{recode}}

In Chapter \ref{mutate}, you learned how the \texttt{mutate()} function lets us create a new variable in our dataset. However, it also has another useful function in that it can be combined with \texttt{recode()} to create new columns with recoded values. For example, let's add a new column to \texttt{pong\_data} in which the background colour is converted into numeric form where \texttt{red} will become 1, and \texttt{blue} will become 2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_data <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(pong_data, }
                    \DataTypeTok{BackgroundColorNumeric =} \KeywordTok{recode}\NormalTok{(BackgroundColor, }
                                                    \StringTok{"red"}\NormalTok{ =}\StringTok{ }\DecValTok{1}\NormalTok{, }
                                                    \StringTok{"blue"}\NormalTok{ =}\StringTok{ }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The code here is is a bit complicated but:

\begin{itemize}
\tightlist
\item
  \texttt{BackgroundColorNumeric} is the name of your new column,
\item
  \texttt{BackgroundColor} is the name of the old column and the one to take information from
\item
  and 1 and 2 are the new codings of red and blue respectively
\item
  character strings like ``red'' and ``blue'' are in quotation marks, numbers are not
\end{itemize}

The \texttt{mutate()} function is also handy for making some calculations on or across columns in your data. For example, say you realise you made a mistake in your experiment where your participant numbers should be 1 higher for every participant, i.e.~Participant 1 should actually be numbered as Participant 2, etc. You would do something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_data <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(pong_data, }\DataTypeTok{Participant =}\NormalTok{ Participant }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note here that you are giving the new column the same name as the old column \texttt{Participant}. What happens here is that you are \textbf{overwriting the old data with the new data}! So watch out, mutate can create a new column or overwrite an existing column, depending on what you tell it to do!

Imagine you realise there is a mistake in your dataset and that all your trial numbers are wrong. The first trial (trial number 1) was a practice so should be excluded and your experiment actually started on trial 2.

\begin{itemize}
\tightlist
\item
  Filter out all trials with the number 1 (\texttt{TrialNumber} column) from \texttt{pong\_data},
\item
  Then use the \texttt{mutate()} function to recount all the remaining trial numbers, starting them at one again instead of two. Overwrite \texttt{TrialNumber} in \texttt{pong\_data} with this new data.
\end{itemize}

You can either do this in two separate steps and create a new object, or you can uses pipes \texttt{\%\textgreater{}\%} and do it it one line of code.

Helpful Hint

Step 1. filter(\texttt{TrialNumber} does not equal 1) - remember to store this output in a variable?

Step 2. mutate(\texttt{TrialNumber} = TrialNumber minus 1)

\hypertarget{activity-8-group_by}{%
\section{\texorpdfstring{Activity 8: \texttt{group\_by()}}{Activity 8: group\_by()}}\label{activity-8-group_by}}

\begin{itemize}
\tightlist
\item
  Group the data by \texttt{BlockNumber} and by \texttt{BackgroundColor}, in that order and save it in a new object named \texttt{pong\_data\_group}.
\item
  View this new object by typing \texttt{pong\_data\_group}into the console.
\end{itemize}

Enter the number of groups (i.e.~a number) you get as a result:

Helpful Hint

It is the same procedure as this but with different column names:

\texttt{group\_by(pong\_data,\ HitOrMiss,\ BackgroundColor)}

The number of groups should be between the product of the number of background colours (red and blue) and the number of blocks (12).

\texttt{group\_by()} is incredibly useful as once the data is organised into groups you can then apply other functions (\texttt{filter}, \texttt{arrange}, \texttt{mutate}\ldots{}etc.) to the groups within your data that you are interested in, instead of to the entire dataset. For instance, a common second step after \texttt{group\_by} might be to \texttt{summarise} the data\ldots{}

\hypertarget{activity-9-summarising-data}{%
\section{Activity 9: Summarising data}\label{activity-9-summarising-data}}

The \texttt{summarise()} function lets you calculate descriptive statistics for your data. For example, say you want to know the total number of hits there were for different paddle lengths as well as the mean number of hits, or number of hits there were when the background colour was red or blue.

We will do this using pipes, to get you used to using them. Remember to try and read the code out loud and to pronounce \texttt{\%\textgreater{}\%} as `and then'. Copy and paste the below code into a new code chunk and run the code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_data_hits<-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(pong_data, BackgroundColor, PaddleLength) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# first group the data}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{total_hits =} \KeywordTok{sum}\NormalTok{(HitOrMiss, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{meanhits =} \KeywordTok{mean}\NormalTok{(HitOrMiss, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\CommentTok{# and then create a new variable called total_hits}
\end{Highlighting}
\end{Shaded}

\texttt{summarise()} has a range of internal functions that make life really easy, e.g. \texttt{mean}, \texttt{sum}, \texttt{max}, \texttt{min}, etc. See the \href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{dplyr cheatsheet} for more examples.

\begin{info}
\texttt{na.rm\ =\ TRUE} is an argument that we can add when calculating
descriptive statistics to tell R what to do if there are missing values.
In this dataset, there are no missing values but if there were and we
asked R to calculate the mean, it would return \texttt{NA} as the result
because it doesn't know how to average nothing. Remember this argument
exists, you will use it often and it save you a lot of time!
\end{info}

\begin{itemize}
\tightlist
\item
  View \texttt{pong\_data\_hits} and enter the number of hits made with the small paddle (50) and the red colour background in this box: 
\end{itemize}

\textbf{Note:}

\begin{itemize}
\item
  The name of the column within \texttt{pong\_data\_hits} is \texttt{total\_hits}; this is what you called it in the above code. You could have called it anything you wanted but always try to use something sensible.
\item
  Make sure to call your variables something you (and anyone looking at your code) will understand and recognize later (i.e.~not variable1, variable2, variable3. etc.), and avoid spaces (use\_underscores\_never\_spaces).
\end{itemize}

\begin{try}
After grouping data together using the \texttt{group\_by()} function and
then performing a task on it, e.g. \texttt{filter()}, it can be very
good practice to ungroup the data before performing another function.
Forgetting to ungroup the dataset won't always affect further
processing, but can really mess up other things. Again just a good
reminder to always check the data you are getting out of a function a)
makes sense and b) is what you expect.
\end{try}

\hypertarget{two-other-useful-functions}{%
\section{Two other useful functions}\label{two-other-useful-functions}}

The Wickham Six verbs let you to do a lot of things with data, however there are thousands of other functions at your disposal. If you want to do something with your data that you are not sure how to do using these functions, do a Google search for an alternative function - chances are someone else has had the same problem and has a help guide. For example, two other functions to note are the \texttt{bind\_rows()} function and the \texttt{count()} functions.

The \texttt{bind\_rows()} function is useful if you want to combine two tibbles together into one larger tibble that have the same column structure. For example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# a tibble of ball speeds 1 and 2}
\NormalTok{slow_ball<-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data, BallSpeed }\OperatorTok{<}\StringTok{ }\DecValTok{3}\NormalTok{) }

\CommentTok{# a tibble of ball speeds 6 and 7}
\NormalTok{fast_ball <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data, BallSpeed }\OperatorTok{>=}\StringTok{ }\DecValTok{6}\NormalTok{) }

\CommentTok{# a combined tibble of extreme ball speeds}
\NormalTok{extreme_balls <-}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(slow_ball, fast_ball) }
\end{Highlighting}
\end{Shaded}

Finally, the \texttt{count()} function is a shortcut that can sometimes be used to count up the number of rows you have for groups in your data, without having to use the \texttt{group\_by()} and \texttt{summarise()} functions. For example, in Task 6 we combined \texttt{group\_by()} and \texttt{summarise()} to calculate how many hits there were based on background colour and paddle length. Alternatively we could have done:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(pong_data, BackgroundColor, PaddleLength, HitOrMiss)}
\end{Highlighting}
\end{Shaded}

The results are the same, just that in the \texttt{count()} version we get all the information, including misses, because we are just counting rows. In the \texttt{summarise()} method we only got hits because that was the effect of what we summed. So two different methods give similar answers - coding can be individualised and get the same result!

\hypertarget{pipes}{%
\section{\texorpdfstring{Pipes (\textbf{\texttt{\%\textgreater{}\%}})}{Pipes (\%\textgreater{}\%)}}\label{pipes}}

Finally, a quick recap on pipes. Here is an example of code that doesn't use pipes to find how many hits there were with the large paddle length and the red background.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# First we group the data accordingly, storing it in `pong_data_group`}
\NormalTok{pong_data_group <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(pong_data, BackgroundColor, PaddleLength)}

\CommentTok{# And then we summarise it, storing the answer in `total_hits`}
\NormalTok{pong_data_hits <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(pong_data_group, }\DataTypeTok{total_hits =} \KeywordTok{sum}\NormalTok{(HitOrMiss))}

\CommentTok{# And filter just the red, small paddle hits}
\NormalTok{pong_data_hits_red_small <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data_hits, BackgroundColor }\OperatorTok{==}\StringTok{ "red"}\NormalTok{, PaddleLength }\OperatorTok{==}\StringTok{ }\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can make our code even more efficient, using less code, by stringing our sequence of functions together using pipes. This would look like:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Same pipeline using pipes}
\NormalTok{pong_data_hits_red_small <-}\StringTok{ }\NormalTok{pong_data }\OperatorTok{%>%}\StringTok{ }\CommentTok{# take pong_data and then}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(BackgroundColor, PaddleLength) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# group by BackgroundColor and PaddleLength and then}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{total_hits =} \KeywordTok{sum}\NormalTok{(HitOrMiss)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# calculate the total number of hits and then}
\StringTok{  }\KeywordTok{filter}\NormalTok{(BackgroundColor }\OperatorTok{==}\StringTok{ "red"}\NormalTok{, PaddleLength }\OperatorTok{==}\StringTok{ }\DecValTok{250}\NormalTok{) }\CommentTok{# only keep the data for the red large paddle}
\end{Highlighting}
\end{Shaded}

One last point on pipes is that they can be written in a single line of code but it's much easier to see what the pipe is doing if each function takes its own line. Every time you add a function to the pipeline, remember to add a \texttt{\%\textgreater{}\%} first and \textbf{note that when using separate lines for each function, the \texttt{\%\textgreater{}\%} must appear at the end of the line and not the start of the next line}. Compare the two examples below. The first won't work but the second will because the second puts the pipes at the end of the line where they need to be!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Piped version that wont work }
\NormalTok{data_arrange <-}\StringTok{ }\NormalTok{pong_data }
                \OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(PaddleLength }\OperatorTok{==}\StringTok{ "50"}\NormalTok{)}
                \OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(BallSpeed) }

\CommentTok{# Piped version that will work }
\NormalTok{data_arrange <-}\StringTok{ }\NormalTok{pong_data }\OperatorTok{%>%}
\StringTok{                }\KeywordTok{filter}\NormalTok{(PaddleLength }\OperatorTok{==}\StringTok{ "50"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{                }\KeywordTok{arrange}\NormalTok{(BallSpeed) }
\end{Highlighting}
\end{Shaded}

\begin{try}
Where piping becomes most useful is when we \textbf{string a series of
functions together}, rather than using them as separate steps and having
to save the data each time under a new variable name and getting
ourselves all confused. In the non-piped version we have to create a new
variable each time, for example, \texttt{data}, \texttt{data\_filtered},
\texttt{data\_arranged}, \texttt{data\_grouped},
\texttt{data\_summarised} just to get to the final one we actually want,
which was \texttt{data\_summarised}. This creates a lot of variables and
tibbles in our environment and can make everything unclear and
eventually slow down our computer. The piped version however uses one
variable name, saving space in the environment, and is clear and easy to
read. With pipes we skip unnecessary steps and avoid cluttering our
environment.
\end{try}

\hypertarget{finished-2}{%
\subsection{Finished!}\label{finished-2}}

We have now learned a number of functions and verbs that you will need as you progress through this book. You will use them in the next chapter so be sure to go over these and try them out to make yourself more comfortable with them. If you have any questions please post them on Teams. \textbf{Happy Wrangling!}

\hypertarget{activity-solutions}{%
\section{Activity solutions}\label{activity-solutions}}

Below you will find the solutions to the above questions. Only look at them after giving the questions a good try and speaking to the tutor about any issues.

\hypertarget{activity-3}{%
\subsection{Activity 3}\label{activity-3}}

Solution Task 3

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# To include variables:}
\NormalTok{select_dat <-}\StringTok{ }\KeywordTok{select}\NormalTok{(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss)}

\CommentTok{# To exclude variables:}
\NormalTok{select_dat <-}\KeywordTok{select}\NormalTok{(pong_data, }\OperatorTok{-}\NormalTok{JudgedSpeed, }\OperatorTok{-}\NormalTok{BallSpeed, }\OperatorTok{-}\NormalTok{BlockNumber)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-4}{%
\subsection{Activity 4}\label{activity-4}}

Solution Activity 4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reorder_dat <-}\StringTok{ }\KeywordTok{select}\NormalTok{(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-5}{%
\subsection{Activity 5}\label{activity-5}}

Solution Task 2

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(pong_data, }\KeywordTok{desc}\NormalTok{(HitOrMiss), }\KeywordTok{desc}\NormalTok{(JudgedSpeed))}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-6}{%
\subsection{Activity 6}\label{activity-6}}

Solution Activity 6

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_fast_miss}\OperatorTok{<}\StringTok{ }\OperatorTok{-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data, }
\NormalTok{                         JudgedSpeed }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }
\NormalTok{                         BallSpeed }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"2"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"5"}\NormalTok{, }\StringTok{"7"}\NormalTok{), }
\NormalTok{                         HitOrMiss }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-7}{%
\subsection{Activity 7}\label{activity-7}}

Solution Activity 7 4

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is the solution if you used two separate steps}

\NormalTok{pong_data_filt <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data, TrialNumber }\OperatorTok{>=}\StringTok{ }\DecValTok{2}\NormalTok{) }
\CommentTok{# you can call this variable anything, as long as it makes sense to yourself and others}

\NormalTok{pong_data <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(pong_data_filt, }\DataTypeTok{TrialNumber =}\NormalTok{ TrialNumber }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}

\CommentTok{# this is the solution if you used pipes}

\NormalTok{pong_data<-}\StringTok{ }\KeywordTok{filter}\NormalTok{(pong_data, TrialNumber }\OperatorTok{>=}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{TrialNumber =}\NormalTok{ TrialNumber }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-8}{%
\subsection{Activity 8}\label{activity-8}}

Solution Task 5

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_data_group <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(pong_data, BlockNumber, BackgroundColor)}
\NormalTok{pong_data_group}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-9}{%
\subsection{Activity 9}\label{activity-9}}

Solution Activity 9

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pong_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"PongBlueRedBack 1-16 Codebook.csv"}\NormalTok{)}
\NormalTok{pong_data_group <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(pong_data, BackgroundColor, PaddleLength)}
\NormalTok{pong_data_hits <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(pong_data_group, }\DataTypeTok{total_hits =} \KeywordTok{sum}\NormalTok{(HitOrMiss))}
\CommentTok{# the answer should give 517}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{debugging-tips-2}{%
\section{Debugging tips}\label{debugging-tips-2}}

\begin{itemize}
\tightlist
\item
  Make sure you have spelt the data file name \textbf{exactly} as it is shown. Spaces and everything. Do not change the name of the csv file, fix your code instead. If you have a different name for your file than someone else then your code is not reproducible.
\item
  Remember when uploading data we use \texttt{read\_csv()} which has an underscore, whereas the data file itself will have a dot in its name, \texttt{filename.csv}.
\item
  Finally, check that the datafile is actually in the folder you have set as your working directory.
\end{itemize}

\hypertarget{test-yourself-2}{%
\section{Test yourself}\label{test-yourself-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What type of data would these most likely be:
\end{enumerate}

\begin{itemize}
\item
  Male = Character Numeric Integer
\item
  7.15 = Character Numeric Integer
\item
  137 = Character Numeric Integer
\end{itemize}

Explain these answers

There is a lot of different types of data and as well as different types of levels of measurements and it can get very confusing. It's important to try to remember which is which because you can only do certain types of analyses on certain types of data and certain types of measurements. For instance, you can't take the average of Characters just like you can't take the average of Categorical data. Likewise, you can do any maths on Numeric data, just like you can on Interval and Ratio data. Integer data is funny in that sometimes it is Ordinal and sometimes it is Interval, sometimes you should take the median, sometimes you should take the mean. The main point is to always know what type of data you are using and to think about what you can and cannot do with them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Which of the Wickham Six would you use to sort columns from smallest to largest: select filter mutate arrange group\_by summarise
\item
  Which of the Wickham Six would you use to calculate the mean of a column: select filter mutate arrange group\_by summarise
\item
  Which of the Wickham Six would you use to remove certain observations - e.g.~remove all males: select filter mutate arrange group\_by summarise
\item
  What does this line of code say? \texttt{data\ \%\textgreater{}\%\ filter()\ \%\textgreater{}\%\ group\_by()\ \%\textgreater{}\%\ summarise()}: take the data and then group it and then filter it and then summarise it take the data and then filter it and then group it and then summarise it take the data and then summarise it and then filter it and then group it take the data and then group it and then summarise it and then filter it
\end{enumerate}

\hypertarget{data-wrangling-3}{%
\chapter{Data Wrangling 3}\label{data-wrangling-3}}

\hypertarget{data-wrangling-recap}{%
\section{Data wrangling recap}\label{data-wrangling-recap}}

In the last chapter, we looked at using one-table Wickham verbs to \texttt{filter}, \texttt{arrange}, \texttt{group\_by}, \texttt{select}, \texttt{mutate} and \texttt{summarise}. Now we will focus on working with data across two or more tables. The two main verbs we will practice adding to the Wickham six in this chapter are \texttt{pivot\_longer()} and \texttt{inner\_join()}and these will help you process your data for the your quantitative project in Research Methods 2.

\begin{itemize}
\tightlist
\item
  \texttt{pivot\_longer()} allows us to \textbf{transform} a table from wide format to long format (more on this below).
\item
  \texttt{inner\_join()} allows us to \textbf{combine} two tables together based on common columns.
\end{itemize}

\begin{try}
A function is a tool that takes an input, performs some action, and
gives an output. They are nothing more than that. If you think about it
your toaster is a function: it takes bread as an input; it performs the
action of heating it up (nicely sometimes; on both sides would be a
luxury); and it gives an output, the toast. A good thing about the
Wickham six functions is that they are nicely named as verbs to describe
what they do - \texttt{mutate()} mutates (adds on a column);
\texttt{arrange()} arranges columns, \texttt{summarise()} summarises,
etc.

In terms of remembering all the functions, the truth is you don't have
to know them all. However, through practice and repetition, you will
quickly learn to remember which ones are which and what package they
come from. Sort of like where to find your spoons in your kitchen - you
don't look in the fridge, and then the washing machine, and then the
drawer. Nope, you learnt, by repetition, to look in the drawer first
time. It's the same with functions. Keep in mind that research methods
is like a language in that the more you use it and work with it the more
it makes sense.
\end{try}

\hypertarget{tidy-data}{%
\subsection{Tidy data}\label{tidy-data}}

We will use a type of data organisation known as \textbf{tidy data} or sometimes \textbf{long-form data}. Any data in this format is easily processed through the \texttt{tidyverse} package. However, the data you work with will not always be formatted this way. If that happens then your first step is to put it into tidy data format. There are three fundamental rules defining Tidy Data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable must have its own column.
\item
  Each observation must have its own row.
\item
  Each value must have its own cell (i.e.~no grouping two variables together, e.g.~time/date in one cell).

  \begin{enumerate}
  \def\labelenumii{\roman{enumii})}
  \tightlist
  \item
    A cell is where any specific row and column meet; a single data point in a tibble is a cell.
  \end{enumerate}
\end{enumerate}

\begin{try}
If you've worked with any kind of data before, particularly if you've
used Excel, it's very likely that you will have used \textbf{wide
format} data. In wide format, each participant's data is all in one row
with multiple columns for different data points. This means that the
data set tends to be very wide and you will have as many rows as you
have participants.

This layout can be easy to read, however, it makes programming quite
difficult. Whilst Tidy Data can be conceptually more difficult to
understand at first, it means you can manipulate your data in whatever
way you want very easily.
\end{try}

\hypertarget{analysing-the-autism-spectrum-quotient-aq}{%
\subsection{Analysing the Autism Spectrum Quotient (AQ)}\label{analysing-the-autism-spectrum-quotient-aq}}

To continue building your data wrangling skills in this chapter you will tidy data from the Autism Spectrum Quotient (AQ) questionnaire. The AQ10 is a non-diagnostic short form of the AQ with only 10 questions per participant. It is a discrete scale and the higher a participant scores on the AQ10 the more autistic-like traits they are said to display. Anyone scoring 7 or above is recommended for further diagnosis. You can see an example of the AQ10 through this link: AQ10 Example.

There are 66 participants and your goal in this pre-class activity is to find an AQ score for each of them through your data-wrangling skills.

There are four data files to work with that you should download into your chapter:

\begin{itemize}
\tightlist
\item
  responses.csv containing the AQ survey responses to each of the 10 questions for the 66 participants
\item
  qformats.csv containing information on how a question should be coded - i.e.~forward or reverse coding
\item
  scoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded
\item
  pinfo.csv containing participant information such as Age, Sex and importantly \texttt{ID} number.
\end{itemize}

\begin{try}
\texttt{csv} stands for `comma separated variable', and is a very basic
way of transferring data. It really just stores numbers and text and
nothing else. The great thing about being this basic is that it can be
read by many different machines and does not need expensive licenses to
open it.
\end{try}

\hypertarget{activity-1-set-up-3}{%
\subsection{Activity 1: Set-up}\label{activity-1-set-up-3}}

Do the following. If you need help, consult Chapter \ref{ref2} and Chapter \ref{ref3}.

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your chapter folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Data wrangling 3''.\\
\item
  Download the four .csv files above and save them in your chapter folder. Make sure that you do not change the file names at all.
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the package \texttt{tidyverse} using the \texttt{library()} function.
\end{itemize}

\hypertarget{activity-2-load-in-the-data}{%
\subsection{Activity 2: Load in the data}\label{activity-2-load-in-the-data}}

Now you need to load in the \texttt{.csv} data files using the \texttt{read\_csv()} function and save them as variables in the environment. For example, to load in the \texttt{responses} file we would type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"responses.csv"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Add the following lines of code to your Markdown and complete them to load in all four \texttt{.csv} data files. Use the above code as an example and name each variable the same as its original file name (minus the .csv part), again as above, e.g. \texttt{responses.csv} gets saved as \texttt{responses}. Remember to run the lines so that the data loaded in and is stored in your environment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses <-}\StringTok{  }\KeywordTok{read_csv}\NormalTok{()    }\CommentTok{# survey responses}
\NormalTok{qformats <-}\StringTok{                 }\CommentTok{# question formats}
\NormalTok{scoring <-}\StringTok{                  }\CommentTok{# scoring info}
\NormalTok{pinfo <-}\StringTok{                    }\CommentTok{# participant information}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-3-look-at-your-data}{%
\subsection{Activity 3: Look at your data}\label{activity-3-look-at-your-data}}

Now that we have the data loaded in it is always best to have a look at it to get an idea of its layout. We showed you ways of doing this before, but you can also use the \texttt{glimpse()} or \texttt{View()} functions in your Console window and put the name of the data between the brackets to see how it is arranged. Don't add these to your script though they are just one-offs for testing.

\begin{itemize}
\tightlist
\item
  Have a look at the data in \texttt{responses} to see if you think it is Tidy or not and answer the following question: The data in \texttt{responses} is in Tidy Wide format
\end{itemize}

Explain this answer

The \texttt{responses} tibble is far from being tidy; each row represents multiple observations from the same participant, i.e.~each row shows responses to multiple questions and there are the same number of rows as there are participants (66) - \texttt{wide\ format}. Remember we want the data in tidy format as described above.

\hypertarget{gather}{%
\subsection{Activity 4: Tidying data}\label{gather}}

We now have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the \texttt{responses} tibble to Tidy Data using the \texttt{pivot\_longer()} function.

\begin{itemize}
\tightlist
\item
  Type the below code line into the Activity 4 code chunk and run it.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rlong <-}\StringTok{ }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ responses, }\CommentTok{# the dataset we want to work on}
                \DataTypeTok{names_to =} \StringTok{"Question"}\NormalTok{, }\CommentTok{# the name of the column that will store what is currently the names of each column (question numbers)}
                \DataTypeTok{values_to =} \StringTok{"Response"}\NormalTok{, }\CommentTok{# the name of the new column that will store the values (data points)}
\NormalTok{                Q1}\OperatorTok{:}\NormalTok{Q10) }\CommentTok{# the columns we want to put into long-form}
\end{Highlighting}
\end{Shaded}

\begin{warning}
\texttt{pivot\_longer()} is a relatively new function in the tidyverse
package. If you get an error message that says
\texttt{could\ not\ find\ function\ pivot\_longer()} you will need to
update the tidyverse package by reinstalling it. You may also need to
update R itself and other packages such a \texttt{rlang}.
\end{warning}

In case you are wondering if we wanted to go back the way,we would use the \texttt{pivot\_wider()} function: e.g. \texttt{rwide\ \textless{}-\ pivot\_wider(rlong,\ names\_from\ =\ Questions,\ values\_from\ =\ Response)}. But we do not want to do that here so let's not add this to the code.

\begin{warning}
In the code above we have used the notation \texttt{Q1:Q10}. This means
'select all the columns from Q1 to Q10. We could have written out the
name of each column individually, for example
\texttt{Q1,\ Q2,\ Q3,\ Q4,\ Q5,\ Q6,\ Q7,\ Q8,\ Q9,\ Q10} but obviously
it is much easier to use the shorthand notation. You must be careful
though to know what you are selecting. R isn't clever enough to realise
that what you want is all the Question columns - it would take any and
all columns that exist between Q1 and Q10. This means that if your
dataset is out of order you may end up selecting columns you didn't mean
to. Always look at your data and make sure you know the layout.
\end{warning}

\begin{itemize}
\tightlist
\item
  Look at the new dataset \texttt{rlong}. Compare it to the original dataset \texttt{responses} and try to understand how they relate to each other.
\end{itemize}

\hypertarget{join}{%
\subsection{Activity 5: Combining data}\label{join}}

Now the \texttt{responses} data is in tidy format, you are closer to being able to calculate an AQ score for each person. However, you still need some extra information:

\begin{itemize}
\tightlist
\item
  Is the question reverse or forward scored (i.e., is strongly agree a positive or negative response)? This information is found in \texttt{qformats}
\item
  How many points are given to give a specific response? This information is found in \texttt{scoring}.
\end{itemize}

This is a typical analysis situation where different information is in different tables and you need to join them altogether. Both these pieces of information are contained in \texttt{qformats} and \texttt{scoring} respectively, but we want to join them to \texttt{responses} to create one informative tidy table with all the information we need. We can do this through the function \texttt{inner\_join()}; a function to combine information in two tibbles using a column common to both tibbles.

\begin{itemize}
\tightlist
\item
  Replace the \texttt{NULL} values in the below code with the necessary variable names to join \texttt{rlong1} and \texttt{qformats} by \texttt{Question}. If you need extra help, revisit Chapter \ref{join} - you used the same function then! You can also check the solutions for the answer (but make sure you try yourself first).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rlong2 <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{by =} \StringTok{"NULL"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Now view \texttt{rlong2}. You have matched each question with its scoring format, \texttt{forward} or \texttt{reverse}.
\end{itemize}

\begin{try}
A lot of questionnaires have some questions that are Forward scored and
some questions that are Reverse scored. What does this mean? Imagine a
situation where your options in replying to a question are: 1 -
extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely
disagree. In a forward-scoring question you would get 1 point for
extremely agree, 2 for agree, 3 for neutral, etc. In a reverse scoring
question you would get 5 for extremely agree, 4 for agree, 3 for
neutral, etc.

The reasoning behind this shift is that sometimes agreeing or
disagreeing might be more favourable depending on how the question is
worded. Secondly, sometimes these questions are used just to catch
people out - imagine if you had two similar questions where one has the
reverse meaning of the other. In this scenario, people should respond
opposites. If they respond the same then they might not be paying
attention.
\end{try}

\hypertarget{activity-6-combining-more-data}{%
\subsection{Activity 6: Combining more data}\label{activity-6-combining-more-data}}

Now you need to combine the information in our new table, \texttt{rlong2}, with the \texttt{scoring} table so you know how many points to attribute each question based on the answer the participant gave, and whether the question was forward or reverse coded. Again, you can use the \texttt{inner\_join()} function, but this time the common columns found in \texttt{rlong2} and \texttt{scoring} are \texttt{QFormat} and \texttt{Response}. To combine by two columns you just write them in sequence as shown below. **Note: when there is more than one common column between two tibbles you are joining, it is best to combine by all the columns to avoid repeat columns names in the new tibble.

\begin{itemize}
\tightlist
\item
  Type the below line into the Activity 6 code chunk, run it, and then view the new object.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# combine rows in rlong2 and scoring based on QFormat and Response}
\NormalTok{rscores <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(rlong2, scoring, }\KeywordTok{c}\NormalTok{(}\StringTok{"QFormat"}\NormalTok{, }\StringTok{"Response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-7-calculating-the-aq-scores.}{%
\subsection{Activity 7: Calculating the AQ scores.}\label{activity-7-calculating-the-aq-scores.}}

You have now created \texttt{rscores} which has information on how each participant responded to each question and how each question should be coded and scored, all within the one tibble. All you need now is to sum the scores for each participant to get their AQ score.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Based on your knowledge from the last chapter, type the below line into your code and replace the NULLs to obtain individual \texttt{aq\_scores} for each participant.\\
\item
  Save your Markdown and knit it to make sure all your code works.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aq_scores <-}\StringTok{ }\NormalTok{rscores }\OperatorTok{%>%}\StringTok{ }
\StringTok{             }\KeywordTok{group_by}\NormalTok{(}\OtherTok{NULL}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# how will you group individual participants?}
\StringTok{             }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{AQ =} \KeywordTok{sum}\NormalTok{(}\OtherTok{NULL}\NormalTok{)) }\CommentTok{# which column will you sum to obtain AQ scores?}
\end{Highlighting}
\end{Shaded}

Helpful Hint

Each participant could be grouped by their Id.

If we summed up the value for each Score we might get a full AQ Score for each particpipant.

\hypertarget{activity-8-one-last-thing-on-pipes}{%
\subsection{Activity 8: One last thing on pipes}\label{activity-8-one-last-thing-on-pipes}}

You now have a complete code to load in your data, convert it to Tidy, combine the tables and calculate an AQ score for each participant. But, if you look at it, some of your code could be more efficient by using pipes.

Go back through your code and try to rewrite it using pipes \texttt{\%\textgreater{}\%} so that it is as efficient as possible.

Helpful Hint

You have now recapped one-table and two-table verbs. These are great to know as for example, in the above, it actually only took a handful of reproducible steps to get from messy data to tidy data; could you imagine doing this by hand in Excel through cutting and pasting? Not to mention the mistakes you could make!

If you have any questions, please post them on Teams.

\hypertarget{activity-solutions-1}{%
\subsection{Activity solutions}\label{activity-solutions-1}}

Below you will find the solutions to the above questions. Only look at them after giving the questions a good try and trying to find help on Google or Teams about any issues.

\hypertarget{activity-2}{%
\subsubsection{Activity 2}\label{activity-2}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"responses.csv"}\NormalTok{)                  }
\NormalTok{qformats <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"qformats.csv"}\NormalTok{)                 }
\NormalTok{scoring <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"scoring.csv"}\NormalTok{)                  }
\NormalTok{pinfo <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"pinfo.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Click the tab to see the solution}

\hypertarget{activity-5-1}{%
\subsubsection{Activity 5}\label{activity-5-1}}

Solution Task 5

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rlong2 <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ rlong, }\DataTypeTok{y =}\NormalTok{ qformats, }\DataTypeTok{by =} \StringTok{"Question"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Click the tab to see the solution}

\hypertarget{activity-7-1}{%
\subsubsection{Activity 7}\label{activity-7-1}}

Solution Task 7

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aq_scores <-}\StringTok{ }\NormalTok{rscores }\OperatorTok{%>%}\StringTok{ }
\StringTok{             }\KeywordTok{group_by}\NormalTok{(Id) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# group by the ID number in column Id}
\StringTok{             }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{AQ =} \KeywordTok{sum}\NormalTok{(Score)) }\CommentTok{# sum column Score to obtain AQ scores.}
\end{Highlighting}
\end{Shaded}

\textbf{Click the tab to see the solution}

\hypertarget{activity-8-1}{%
\subsubsection{Activity 8}\label{activity-8-1}}

Activity 8

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aq_scores2 <-}\StringTok{ }\NormalTok{responses }\OperatorTok{%>%}\StringTok{ }\CommentTok{# take the data in `responses` and then}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{names_to =} \StringTok{"Question"}\NormalTok{, }\DataTypeTok{values_to =} \StringTok{"Response"}\NormalTok{, Q1}\OperatorTok{:}\NormalTok{Q10) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# gather up columns Q1 to Q10, put the column names in Question and the scores in Response and then}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(qformats, }\StringTok{"Question"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# join with `qformats` and match the data by the column `Question` and then}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(scoring, }\KeywordTok{c}\NormalTok{(}\StringTok{"QFormat"}\NormalTok{, }\StringTok{"Response"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# join with `scoring` and match the data by the columns `Qformat` and `Response` and then}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Id) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# group by participant ID and then}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{AQ =} \KeywordTok{sum}\NormalTok{(Score)) }\CommentTok{# calculate the total AQ score}
\end{Highlighting}
\end{Shaded}

\textbf{Click the tab to see the solution}

\hypertarget{test-yourself-3}{%
\subsection{Test yourself}\label{test-yourself-3}}

\begin{itemize}
\tightlist
\item
  You want to put the first three columns of a file called \texttt{responses} (Q1, Q2, Q3) into long-form, put the question numbers in a column called \texttt{Jam}, the responses in a column called \texttt{Strawberry}, and store everything in a tibble called \texttt{sandwich}. Fill in the box with what you would write: 
\end{itemize}

Explain this answer

\begin{itemize}
\item
  Complete the sentence, the higher the AQ score\ldots{} the less autistic-like traits displayed has no relation to autistic-like traits the more autistic-like traits displayed
\item
  Type in the AQ score (just the number) of Participant ID No.~87: 
\item
  Type how many participants had an AQ score of 3 (again just the number): 
\item
  The cut-off for the AQ10 is usually said to be around 6 meaning that anyone with a score of more than 6 should be referred for diagnostic assessment. Type in how many participants we should refer from our sample: 
\end{itemize}

Explain these answers

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  From the link above you can see that an appropriate citation for the AQ10 would be (Allison, Auyeung, and Baron-Cohen, (2012))
\item
  As mentioned, the higher the score on the AQ10 the more autistic-like traits a participant is said to show.
\item
  You could do this by code with \texttt{filter(aq\_scores,\ Id\ ==\ 87)}, which would give you a tibble of 1x2 showing the ID number and score. If you just wanted the score you could use \texttt{pull()} which we haven't shown you that yet: \texttt{filter(aq\_scores,\ Id\ ==\ 87)\ \%\textgreater{}\%\ pull(AQ)}. The answer is an AQ score of 2.
\item
  Same as above but changing the argument of the filter. \texttt{filter(aq\_scores,\ AQ\ ==\ 3)\ \%\textgreater{}\%\ count()}. The answer is 13. Remember you can do this by counting but the code makes it reproducible and accurate every time. You might make mistakes.
\item
  \texttt{filter(aq\_scores,\ AQ\ \textgreater{}\ 6)\ \%\textgreater{}\%\ count()} or \texttt{filter(aq\_scores,\ AQ\ \textgreater{}=\ 7)\ \%\textgreater{}\%\ count()}. The answer is 6.
\end{enumerate}

\hypertarget{comparing-categories}{%
\chapter{Comparing categories}\label{comparing-categories}}

We have spent the chapters so far focusing on the basics of R and data wrangling. You may think that the tasks we ask you to do in R will get harder as this course progresses but that isn't true. The hardest part of learning R is at the very beginning, trying to learn the new terminology, figuring out how to load in data and wrangle it into the format you need. It may feel like you are still struggling so it's worth reflecting on just how far you've come in a short time.

You can now:

\begin{itemize}
\tightlist
\item
  Understand what functions, arguments, objects, variables, and tibbles are\\
\item
  Read data into R\\
\item
  Tidy data into an appropriate format\\
\item
  Calculate a range of descriptive statistics
\end{itemize}

That's amazing! Now we're going to move on to performing some simple descriptive comparisons and create a plot to visualise the data.

\hypertarget{reminders-through-association}{%
\subsection{Reminders through association}\label{reminders-through-association}}

For this chapter, we're going to use data from \href{https://journals.sagepub.com/stoken/default+domain/6XgDSfr6ZHSDs47tx5bu/full}{Rogers, T. \& Milkman, K. L. (2016). Reminders through association. Psychological Science, 27, 973-986}. You can read the full paper online but the short version is that the authors looked at how people remember to follow through with the intention of doing something. Although there are lots of potential reasons (e.g., some people may lack the self-control resources), Rogers and Milkman (2016) propose that some people fail to follow through simply because they forget about their good intentions. If this is the case, the authors argue, then having visual reminders to follow through on their intentions may help people remember to keep them. For example, a person may choose to put a sticker for their gym on their car window, so that every time they get in the car they remember to go to the gym.

In Study 1, participants took part in an unrelated experiment but at the start of the task they were asked to return a small stack of paper clips to the reception of the building at the end of the study and if they did so the researchers would donate \$1 to a charity. They were then asked if they intended to do this. Those in the reminder-through-association (RTA) condition read ``Thank you! To remind you to pick up a paper clip, an elephant statuette will be sitting on the counter as you collect your payment.'' This message was followed by a picture of the elephant statuette. Those in the control condition simply read ``Thank you!''.

What we want to do is to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition. In the paper, the authors use an inferential chi-square analysis to do this, however, we're just going to look at the descriptive statistics (although the optional chi-square code is provided if you want to go further).

\hypertarget{activity-1-set-up-4}{%
\subsection{Activity 1: Set-up}\label{activity-1-set-up-4}}

Do the following. If you need help, consult Chapter \ref{ref2} and Chapter \ref{ref3}.

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your chapter folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Comapring categories''.\\
\item
  Download RTA\_study1.csv and save it in your chapter folder. Make sure that you do not change the file name at all.
\item
  If you are working on your own computer, install the package \texttt{lsr}. Remember \textbf{do not install packages on university computers, they are already installed}.
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the packages \texttt{tidyverse} and \texttt{lsr} using the \texttt{library()} function and loads the data into an object named \texttt{intent\_data} using \texttt{read\_csv()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\NormalTok{intent_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-2-look-at-the-data-1}{%
\subsection{Activity 2: Look at the data}\label{activity-2-look-at-the-data-1}}

Using your preferred method, look at the data. It is a fairly simple data file that contains four variables for 87 participants:

\begin{itemize}
\tightlist
\item
  \texttt{condition}: this variable indicates which condition participants were in, 1 = reminder-through-association condition, 2 = control condition
\item
  \texttt{intend}: this variable indicates whether participants said they were intending to return the paper-clips, 1 = yes, 0 = no
\item
  \texttt{actualdonate}: this variable indicates whether participants actually ended up returning the paper-clips and therefore donating to charity, 1 = yes, 0 = no
\item
  \texttt{id}: this variable indicates the participant ID number
\end{itemize}

\hypertarget{activity-3-wrangle-and-recode-the-data}{%
\subsection{Activity 3: Wrangle and recode the data}\label{activity-3-wrangle-and-recode-the-data}}

We need to do a little bit of wrangling to get our data into the format we need. First, we need to remove all the participants who said that they did not intend to return the paper-clips (\texttt{intend\ =\ 0}) as we are only interested in whether people follow through on an intention. Second, to make the output easier to read, we're going to recode \texttt{condition} to have text labels rather than numerical values.

\begin{itemize}
\tightlist
\item
  Use \texttt{filter()} to remove all participants who said that they did not intend to return the paper-clips
\item
  Use \texttt{mutate()} and \texttt{recode()} to recode the values in \texttt{condition} to make 1 = rta and 2 = control and the values in \texttt{actualdonate} to 1 = donated and 0 = no\_donation. If you need help with this, consult Chapter \ref{recode}.
\item
  You can do this in two separate steps, or you can use pipes. Regardless of how you do it, save the final output to an object named \texttt{intent\_recode}.
\end{itemize}

The solutions are at the end of this chapter but make sure you try it yourself and ask your peers and tutor for help first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intent_recode <-}
\end{Highlighting}
\end{Shaded}

Helpful hint

You will need to put both sides of each recode argument (i.e., 1 and rta) in quotation marks, even though 1 and 2 are numbers, they actually represent categories rather than numerical data.

\texttt{intent\_recode} should have data from 77 participants and should look something like this:

\begin{tabular}{c|c|c|c}
\hline
condition & intend & actualdonate & id\\
\hline
rta & 1 & donated & 1\\
\hline
rta & 1 & donated & 2\\
\hline
rta & 1 & donated & 3\\
\hline
rta & 1 & donated & 4\\
\hline
rta & 1 & donated & 5\\
\hline
rta & 1 & donated & 6\\
\hline
\end{tabular}

\begin{warn}
There are several different packages that have a function called
\texttt{recode()} and \texttt{select()}. At the moment, we haven't
loaded them so there won't be any problems however, in the future you
should remember these as potentially problematic functions. To avoid any
issues remember Chapter \ref{conflicts}: you can specify exactly which
package you want the function to come from using
\texttt{dplyr::recode()} and \texttt{dplyr::select()}. Remember this -
it may save you a lot of time in the future!
\end{warn}

\hypertarget{activity-4-descriptive-statistics}{%
\subsection{Activity 4: Descriptive statistics}\label{activity-4-descriptive-statistics}}

Next you need to calculate descriptive statistics. For frequency data these are simply counts so we can use the function \texttt{count()} rather than having to use \texttt{summarise}. We want to know how many participants are in each group (rta - donated, rta - didn't donate, control - donated, control - didn't donate) so we will need to use \texttt{group\_by} to display the results for all combinations of \texttt{condition} and \texttt{actualdonate}.

\begin{itemize}
\tightlist
\item
  Replace the NULLs in the below code to calculate the number of participants in each category and save it to an object named \texttt{intent\_counts}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intent_counts <-}\StringTok{ }\NormalTok{intent_recode }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\OtherTok{NULL}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

How many participants in the control condition didn't donate? \\
How many participants in the control condition donated? \\
How many participants in the rta condition didn't donate? \\
How many participants in the rta condition donated?

You may also want to calculate the percentage of people who donated in each condition, if so you can adapt the code like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intent_percent <-}\StringTok{ }\NormalTok{intent_recode }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(condition, actualdonate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\CommentTok{# ungroups the code}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(condition) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# then groups it again but just by condition}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{percent_condition =}\NormalTok{ n}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(n) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c|c}
\hline
condition & actualdonate & n & percent\_condition\\
\hline
control & donated & 16 & 42.11\\
\hline
control & no\_donation & 22 & 57.89\\
\hline
rta & donated & 29 & 74.36\\
\hline
rta & no\_donation & 10 & 25.64\\
\hline
\end{tabular}

\hypertarget{ggplot2}{%
\subsection{\texorpdfstring{\texttt{ggplot2()}}{ggplot2()}}\label{ggplot2}}

Now you have calculated how many participants are in each cell (or combination of the categories), however, it is also useful to create a visualisation of the data - the old saying is true, a picture is worth a thousand words. To make our data visualisations we're going to use the package \texttt{ggplot2()} which was loaded as part of the \texttt{tidyverse}.

\texttt{ggplot()} builds plots by combining layers (see Figure \ref{fig:img-layers})). If you're used to making plots in Excel this might seem a bit odd at first, however, it means that you can customise each layer and R is capable of making very complex and beautiful figures (\href{https://www.data-to-viz.com/}{this website} gives you a good sense of what's possible).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/layers} 

}

\caption{ggplot2 layers from Field et al. (2012)}\label{fig:img-layers}
\end{figure}

\hypertarget{bar}{%
\subsection{Activity 5: Bar plot}\label{bar}}

We want to create a simple bar plot of our count data.

\begin{itemize}
\tightlist
\item
  Type the below code into a new R chunk and run it.\\
\item
  The first line (or layer) sets up the base of the graph: the data to use and the aesthetics (what will go on the x and y axis, how the plot will be grouped).\\
\item
  \texttt{aes()} can take both an \texttt{x} and \texttt{y} argument, however, with a bar plot you are just asking R to count the number of data points in each group so you don't need to specify this.\\
\item
  \texttt{fill} will separate the data into each level of the grouping variable and give it a different colour. In this case, there is a different coloured bar for each level of \texttt{actualdonate}.\\
\item
  The next layer adds a \textbf{geom} or a shape, in this case we use \texttt{geom\_bar()} as we want to draw a bar plot.\\
\item
  \texttt{position\ =\ "dodge"} places the bars next to each other, rather than on top of each other. Try removing this argument and just running the code with \texttt{geom\_bar()} to see what happens.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ intent_recode, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{fill =}\NormalTok{ actualdonate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{07-chi-square_files/figure-latex/layer1-1} 

}

\caption{Bar plot of RTA Study 1 data}\label{fig:layer1}
\end{figure}

\begin{warning}
In R terms, \texttt{ggplot2} is a fairly old package. As a result, the
use of pipes wasn't included when it was originally written. As you can
see in the code above, the layers of the code are separated by
\texttt{+} rather than \texttt{\%\textgreater{}\%}. In this case,
\texttt{+} is doing essentially the same job as a pipe - be careful not
to confuse them.
\end{warning}

As you can see, the plot makes it much easier to visualise the data - participants in the RTA condition appear to have been more likely to remember to donate than those in the control condition.

\hypertarget{activity-6-make-the-plot-pretty}{%
\subsection{Activity 6: Make the plot pretty}\label{activity-6-make-the-plot-pretty}}

As mentioned, \texttt{ggplot2} allows you to customise all aspects of your plots, so let's tidy ours up a little bit. We're going to do the following:

\begin{itemize}
\tightlist
\item
  Edit the labels on the x-axis, y-axis and fill\\
\item
  Change the colours of the bars to colour-blind friendly options \texttt{scale\_fill\_viridis\_d()}\\
\item
  Change the theme of the plot to change how it looks visually
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ intent_recode, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{fill =}\NormalTok{ actualdonate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Condition"}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"RTA"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Behaviour"}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Donated"}\NormalTok{, }\StringTok{"Did not donate"}\NormalTok{), }\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=1\linewidth]{07-chi-square_files/figure-latex/plot_edits-1}

\}

\caption{Prettier bar plot of RTA Study}

(\#fig:plot\_edits)
\textbackslash{}end\{figure\}

There are a few things to note about the code we just added on:

\begin{itemize}
\tightlist
\item
  The first two lines are the same code as we used in Activity 4, what we've done now is add on extra layers.\\
\item
  \texttt{scale\_fill\_viridis\_d()} has multiple options for colours, try replacing ``E'' with a letter from A - D and see which one you prefer.
  *The \texttt{d} in \texttt{scale\_fill\_viridis\_d()} stands for discrete, because we have a categorical variable. There is also a \texttt{scale\_fill\_viridis\_c()} that you would use if your fill variable was continuous.
\item
  There are multiple themes that you can apply. If you type \texttt{theme\_} the auto-complete will show you the options - try a few out and see which one you prefer.\\
\item
  If you want more information on any of these functions, remember you can look at the help documentation by typing \texttt{?function}.
\end{itemize}

\hypertarget{activity-7-chi-square}{%
\subsection{Activity 7: Chi-square}\label{activity-7-chi-square}}

So, let's finally run that chi-square analysis to see whether our intuition from the plot holds up and there is a significant association between the grouping variables. As promised, the code is quite simple - type the below code into a new R chunk and run it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{chisq.test}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ intent_recode}\OperatorTok{$}\NormalTok{condition, }\CommentTok{# the first grouping variable}
                      \DataTypeTok{y =}\NormalTok{ intent_recode}\OperatorTok{$}\NormalTok{actualdonate, }\CommentTok{# the second grouping variable}
                      \DataTypeTok{correct =} \OtherTok{FALSE}\NormalTok{) }\CommentTok{# whether we want to apply the continuity correction}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  intent_recode$condition and intent_recode$actualdonate
## X-squared = 8.244, df = 1, p-value = 0.004089
\end{verbatim}

This code looks a little different to code you've used up until this point as it comes from Base R. The x and y variables use the notation \texttt{object\$variable} so our x variable could be read as "use the variable \texttt{condition} from the object \texttt{intent\_recode}. The reason that we chose not to apply the continuity correction is because this is what the analysis in the original paper did.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the chi-square statistic? 
\item
  Is the p-value significant? Yes No\\
\item
  What are the degrees of freedom for the test? 
\end{enumerate}

Explain these answers

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The chi-square statistic is noted in the output as X-squared. Refer to the lecture for more information on how this number is calculated.\\
\item
  The traditional cut-off for significance is p \textless{} .05. This means that if your p-value is smaller than .05 there is a statistically significant association, that is, you would be unlikely to observe this pattern of data by chance if the null hypothesis was true. If p is larger than .05 it means that there is a higher probability that any difference you see would be likely to occur even if the null hypothesis was true. Pay attention to the decimal places, they make a huge difference!\\
\item
  Degrees of freedom are noted as \texttt{df} in the output. Refer to the lecture for more information on what they are and how they are calculated.
\end{enumerate}

Go and find the results section in the original paper, do your numbers match the ones they report?

\hypertarget{activity-8-additional-analysis-information}{%
\subsection{Activity 8: Additional analysis information}\label{activity-8-additional-analysis-information}}

You may have noticed that when you ran the chi-square an object appeared in the environment that saved the results of the analysis. This object is a \textbf{list} which is a bit different to the type of objects we've worked with so far. Lists don't just contain one data table or a vector of numbers or characters, they can contain multiple different types of information and multiple different tables. We can see that our object \texttt{results} is a list of 9, which means it has 9 components. Click on \texttt{results} in the environment pane to view the contents of the list (you could also type \texttt{str(results)}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/lists} 

}

\caption{Contents of a list}\label{fig:img-lists}
\end{figure}

Each of these components can be viewed separately using the same \texttt{object\$variable} notation we used above. For example, if we wanted to view the observed frequencies (refer to the lecture), we would run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\OperatorTok{$}\NormalTok{observed}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|c|c}
\hline
  & donated & no\_donation\\
\hline
control & 16 & 22\\
\hline
rta & 29 & 10\\
\hline
\end{tabular}

\hypertarget{assumption-checks}{%
\subsection{Assumption checks}\label{assumption-checks}}

The assumptions for chi-square are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The data in the cells should be frequencies, or counts of cases rather than percentages or some other transformation of the data.
\item
  The levels (or categories) of the variables are mutually exclusive. That is, a particular participant fits into one and only one group of each of the variables.
\item
  Each subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.
\item
  The study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher's data consists of paired samples, such as in studies in which a parent is paired with his or her child.
\item
  There are 2 variables, and both are measured as categories, usually at the nominal level. While Chi-square has no rule about limiting the number of cells (by limiting the number of categories for each variable), a very large number of cells (over 20) can make it difficult to meet assumption \#6 below, and to interpret the meaning of the results.
\item
  The expected cell frequencies should be greater than 5.
\end{enumerate}

\hypertarget{activity-9-check-the-expected-frequencies}{%
\subsection{Activity 9: Check the expected frequencies}\label{activity-9-check-the-expected-frequencies}}

We know that assumptions 1-5 have been met because we know the design of the study and the type of data we have fits these criteria. The final assumption we need to test is that all expected frequencies are greater than 5.

\begin{itemize}
\tightlist
\item
  Using the same \texttt{object\$variable} code as in Activity 7, view the expected frequencies
\end{itemize}

Does the data meet assumption 6? Yes - all expected frequencies are \textgreater{} 5 No - one or more expected frequencies are \textless{} 5

\hypertarget{cramer}{%
\subsection{Activity 10: Effect size}\label{cramer}}

Although it wasn't in the original paper, as our last step we will calculate an effect size so that we have a standardised measure of how large the association between our grouping variable is, the effect size measure for chi-square is Cramer's V that you covered in the lecture.

To calculate Cramer's V we're going to use the function \texttt{cramersv()} from the \texttt{lsr} package. This function is very easy to use - you copy and paste the code you gave to \texttt{chisq.test()} in Activity 7.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eff_size <-}\StringTok{ }\KeywordTok{cramersV}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ intent_recode}\OperatorTok{$}\NormalTok{condition, }\DataTypeTok{y =}\NormalTok{ intent_recode}\OperatorTok{$}\NormalTok{actualdonate, }\DataTypeTok{correct =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{eff_size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.327207
\end{verbatim}

\hypertarget{activity-11-write-up}{%
\subsection{Activity 11: Write-up}\label{activity-11-write-up}}

Now that you've run all of the analyses you can use inline coding to help you write up your results. This isn't something you're going to be tested on in this course but it's a really cool feature of Markdown so for each statistical test we'll show you the code that does it so that you can use it in the future if you wanted to. We're going to replicate the exact write-up of the results from the original paper (with the addition of the effect size).

In the \textbf{white-space} in your Markdown document, copy and paste the following (do not change anything). Note that this will only work if you have called your variables \textbf{exactly} the same as the examples in this book:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Those }\ControlFlowTok{in}\NormalTok{ the reminder}\OperatorTok{-}\NormalTok{through}\OperatorTok{-}\NormalTok{association condition performed the intended behavior at a significantly higher }\KeywordTok{rate}\NormalTok{ (}\StringTok{`}\DataTypeTok{r round(pluck(intent_percent$percent_condition, 3),0)}\StringTok{`}\NormalTok{%, }\StringTok{`}\DataTypeTok{r pluck(intent_percent$n, 3)}\StringTok{`}\NormalTok{ out of }\StringTok{`}\DataTypeTok{r pluck(intent_percent$n, 3) + pluck(intent_percent$n, 4)}\StringTok{`}\NormalTok{) than did those }\ControlFlowTok{in}\NormalTok{ the control }\KeywordTok{condition}\NormalTok{ (}\StringTok{`}\DataTypeTok{r round(pluck(intent_percent$percent_condition, 1),0)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r pluck(intent_percent$n, 1)}\StringTok{`}\NormalTok{ out of }\StringTok{`}\DataTypeTok{r pluck(intent_percent$n, 1) + pluck(intent_percent$n, 2)}\StringTok{`}\NormalTok{)}\ErrorTok{)}\NormalTok{, χ2(}\StringTok{`}\DataTypeTok{r results$parameter}\StringTok{`}\NormalTok{, }\DataTypeTok{N =} \StringTok{`}\DataTypeTok{r length(intent_recode$id)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r round(results$statistic,2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r round(results$p.value, 3)}\StringTok{`}\NormalTok{, V =}\StringTok{ `}\DataTypeTok{r round(eff_size, 2)}\StringTok{`}\NormalTok{.}
\end{Highlighting}
\end{Shaded}

This will knit as:

\begin{quote}
Those in the reminder-through-association condition performed the intended behaviour at a significantly higher rate (74\%, 29 out of 39) than did those in the control condition (42, 16 out of 38)), χ2(1, N = 77) = 8.24, p = 0.004,V = 0.33.
\end{quote}

If you're feeling comfortable with R at this point, push yourself to reverse-engineer what each bit of this inline code is doing so that you could use it yourself (remember the \texttt{?help} function).

\hypertarget{finished-3}{%
\subsubsection{Finished!}\label{finished-3}}

And you're done! The second and final R Portfolio for RM1 will be available on Moodle and again will only assess you on code we have covered in this book. If you need to, take the time over the break to revise what we've covered so far - if you can get comfortable with the content of the last few chapters, RM2 won't pose a problem. If you're OK with, or even enjoying R so far then please feel free to work through this book at your own pace ahead of the scheduled content

\hypertarget{activity-solutions-2}{%
\subsection{Activity solutions}\label{activity-solutions-2}}

\hypertarget{activity-1}{%
\subsubsection{Activity 1}\label{activity-1}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{intent_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"RTA_study1.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-3-1}{%
\subsubsection{Activity 3}\label{activity-3-1}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# solution using pipes}

\NormalTok{intent_recode <-}\StringTok{ }\NormalTok{intent_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(intend }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{condition =} \KeywordTok{recode}\NormalTok{(condition, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "rta"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "control"}\NormalTok{),}
         \DataTypeTok{actualdonate =} \KeywordTok{recode}\NormalTok{(actualdonate, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "donated"}\NormalTok{, }\StringTok{"0"}\NormalTok{ =}\StringTok{ "no_donation"}\NormalTok{))}

\CommentTok{# solution using separate steps}

\NormalTok{intent_filter <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(intent_data, intend }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{intent_recode <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(intent_filter, }\DataTypeTok{condition =} \KeywordTok{recode}\NormalTok{(condition, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "rta"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "control"}\NormalTok{),}
                        \DataTypeTok{actualdonate =} \KeywordTok{recode}\NormalTok{(actualdonate, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "donated"}\NormalTok{, }\StringTok{"0"}\NormalTok{ =}\StringTok{ "no_donation"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-9-1}{%
\subsubsection{Activity 9}\label{activity-9-1}}

Activity 9

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\OperatorTok{$}\NormalTok{expected}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{test-yourself-4}{%
\subsection{Test yourself}\label{test-yourself-4}}

** This question is currently borked

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You have a dataset where gender has been coded numerically. You want to recode this to use text labels. Which code will work? mutate(gender = recode(gender, ``male'' = ``1'', ``female'' = ``2'', ``nonbinary'' = ``3'')) mutate(gender = recode(gender, male = 1, ``female = 2, nonbinary = 3)) mutate(gender = recode(gender,''1" = ``male'', ``2'' = ``female'', ``3'' = ``nonbinary''))
\end{enumerate}

Explain this answer

The first option has the new and old codes in the wrong position, the second option is missing quotation marks, the third option is correct.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  From the below code, what would the plot have on the x-axis? exp\_data gender score
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ exp_data, }\KeywordTok{aes}\NormalTok{(gender, score))}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  From the below code, how would the bars in the plot be positioned? On top of each other Next to each other
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{fill =}\NormalTok{ actualdonate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Explain this answer

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Which of the following is not an argument of \texttt{chisq.test()} (you may need to look at the help documentation to answer this question)? x y p continuity
\end{enumerate}

\hypertarget{correlations}{%
\chapter{Correlations}\label{correlations}}

As \href{https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view}{Miller and Haden (2013)} state at the start of Chapter 11, correlations are \textbf{used to detect and quantify relationships among numerical variables}. In short, you measure two variables and the correlation analysis tells you whether or not they are related in some manner - positively or negatively; one increases as the other increases; one decreases as the other increases; etc..

To actually carry out a correlation is very simple and we will show you that today in a little while: you just need the \texttt{cor.test()} function. The harder part of correlations is really wrangling the data and interpreting what the results mean. You are going to run a few correlations today to give you good practice at running and interpreting the relationships between two variables.

\textbf{Note:} When dealing with correlations you should always refer to relationships and not predictions. In a correlation, X does not predict Y, that is regression which we will look at later this semester. In a correlation, all we can say is whether X and Y are related. So try to get the correct terminology and please feel free to pull us up if we say the wrong thing in class. It is an easy slip of the tongue to make!

\hypertarget{activity-1-set-up-5}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-5}}

In this chapter we will use the examples in Miller and Haden (2013), Chapter 11, looking at the relationship between four variables: reading ability, intelligence (IQ), the number of minutes per week spent reading at home (Home); and the number of minutes per week spent watching TV at home (TV). You can see in this situation that it would be unethical to manipulate these variables so measuring them as they exist in the environment is most appropriate; hence the use of correlations.

Do the following. If you need help, consult Chapter \ref{ref3} and Chapter @(ref2).

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 5 folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 5''.\\
\item
  Download MillerHadenData.csv and save it in your Week 5 folder. Make sure that you do not change the file name at all.\\
\item
  If you are working on your own computer, install the packages \texttt{Hmisc}, \texttt{car}, and \texttt{broom}. Remember \textbf{do not install packages on university computers, they are already installed}.
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the packages \texttt{broom}, \texttt{car}, \texttt{lsr}, \texttt{Hmisc}, and \texttt{tidyverse} (in that order) using the \texttt{library()} function and loads the data into an object named \texttt{mh} using \texttt{read\_csv()}
\end{itemize}

\hypertarget{activity-2-look-at-your-data-1}{%
\section{Activity 2: Look at your data}\label{activity-2-look-at-your-data-1}}

\begin{itemize}
\tightlist
\item
  Look at your data, you can do this by clicking on the object in the environment, or using \texttt{summary(mh)} or \texttt{head(mh)}.
\end{itemize}

As in Miller and Haden, we have 5 columns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The participant (\texttt{Participant}),
\item
  Reading Ability score (\texttt{Abil}),
\item
  Intelligence score (\texttt{IQ}),
\item
  Number of minutes spent reading at home per week (\texttt{Home}),
\item
  And number of minutes spent watching TV per week (\texttt{TV}).
\end{enumerate}

For the chapter we will focus on Reading Ability and IQ but for further practice you can look at other relationships in your free time.\\
A probable hypothesis for today could be that as Reading Ability increases so does Intelligence (think of the issue with causality and direction). Phrasing the hypothesis more formally, we hypothesise that the reading ability of school children, as measured through a standardized test, and intelligence, again measured through a standardized test, are positively correlated.

\hypertarget{activity-3-assumptions}{%
\section{Activity 3: Assumptions}\label{activity-3-assumptions}}

First, however, we must check some assumptions of the correlation tests. The main assumptions we need to check are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the data interval, ratio, or ordinal?
\item
  Is there a data point for each participant on both variables?
\item
  Is the data normally distributed in both variables?
\item
  Does the relationship between variables appear linear?
\item
  Does the spread have homoscedasticity?
\end{enumerate}

We will look at these in turn.

\hypertarget{assumption-1-level-of-measurement}{%
\subsection{Assumption 1: Level of Measurement}\label{assumption-1-level-of-measurement}}

If we want to run a Pearson correlation then we need interval or ratio data; Spearman correlations can run with ordinal, interval or ratio data. What type of data do we have?

\begin{itemize}
\tightlist
\item
  The type of data in this analysis is most probably ratio interval ordinal nominal as the data is continuous discrete and there is unlikely to be a true zero
\end{itemize}

Hints on data type

\begin{itemize}
\tightlist
\item
  Are the variables continuous?
\item
  Is the difference between 1 and 2 on the scale equal to the difference between 2 and 3?
\end{itemize}

\hypertarget{assumption-2-pairs-of-data}{%
\subsection{Assumption 2: Pairs of Data}\label{assumption-2-pairs-of-data}}

All correlations must have a data point for each participant in the two variables being correlated. This should make sense as to why - you can't correlate against an empty cell! So now go check that you have a data point in both columns for each participant.

It looks like that everyone has data in all the columns but let's test our skills a little whilst we are here. Answer the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How is missing data represented in a tibble? an empty cell NA a large number don't know
\item
  Which code would leave you with just the participants who were missing Reading Ability data in mh:
   filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil)
\item
  Which code would leave you with just the participants who were not missing Reading Ability data in mh: filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil)
\end{enumerate}

Hints on removing missing data points

\begin{itemize}
\tightlist
\item
  \texttt{filter(dat,\ is.na(variable))} versus \texttt{filter(dat,\ !is.na(variable))}
\end{itemize}

\hypertarget{assumption-3-5-normality-linearity-homoscedasticity}{%
\subsection{Assumption 3-5: Normality, linearity, homoscedasticity}\label{assumption-3-5-normality-linearity-homoscedasticity}}

The remaining assumptions are all best checked through visualisations. You can use histograms and QQ-plots to check that the data (\texttt{Abil} and \texttt{IQ}) are both normally distributed, and you can use a scatterplot of IQ as a function of Abil to check whether the relationship is linear, with homoscedasticity, and without outliers. There are various options and tests for assessing these assumptions but today we will just use visual checks.

\begin{itemize}
\tightlist
\item
  Run the below code to create a histogram for \texttt{Abil}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ mh, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Abil)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{08-week-5_files/figure-latex/abil-hist-1} 

}

\caption{Histogram of Abil}\label{fig:abil-hist}
\end{figure}

This code should look very similar to the code you used to create a bar plot in Chapter \ref{bar}. We have specified that we want to display \texttt{Abil} on the x-axis and that the shape we want to produce is a histogram, hence \texttt{geom\_histogram()}. Just like \texttt{geom\_bar()}, you do not need to specify the y-axis because if it's a histogram, it's always a count.

\begin{itemize}
\tightlist
\item
  Write and run the code to produce another histogram for the variable \texttt{IQ}.
\end{itemize}

The QQ-plots require us to use the package \texttt{car} rather than \texttt{ggplot2}. You can make QQ-plots in \texttt{ggplot2} but they aren't as useful, however, the code is still very simple.

\begin{itemize}
\tightlist
\item
  Run the below code to create a QQ-plot for \texttt{Abil}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqPlot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mh}\OperatorTok{$}\NormalTok{Abil)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{08-week-5_files/figure-latex/qq-abil-1} 

}

\caption{QQ-plot for Abil}\label{fig:qq-abil}
\end{figure}

\begin{verbatim}
## [1] 15  4
\end{verbatim}

The QQ-plot includes a \textbf{confidence envelope} (the blue dotted lines). The simple version is that if your data points fall within these dotted lines then you can assume normality. The \texttt{ggplot2} version of QQ-plots make it more difficult to add on this confidence envelope, which is why we're using a different package. \texttt{qqPlot()} will also print the IDs of the data points that are potentially problematic. In this case, the 4th and 15th data point in \texttt{Abil} are flagged to review.

\begin{itemize}
\tightlist
\item
  Write and run the code to create a QQ-plot for \texttt{IQ}.
\end{itemize}

In order to assess linearity and homoscedasticity, we can create a scatterplot using \texttt{ggplot2}. This code is slightly different to what you have already done with \texttt{ggplot()} in that with a scatterplot you need to specify both the x and y axis in \texttt{aes()}.

The first geom, \texttt{geom\_point()} adds in the data points, the second geom, \texttt{geom\_smooth} adds in the line of best fit. The shaded area around the line is a \textbf{confidence interval}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ mh, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Abil, }\DataTypeTok{y =}\NormalTok{ IQ)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm) }\CommentTok{# if you don't want the shaded CI, add se = FALSE to this}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{08-week-5_files/figure-latex/unnamed-chunk-5-1} 

}

\caption{Scatterplot of scores}\label{fig:unnamed-chunk-5}
\end{figure}

\begin{itemize}
\tightlist
\item
  Remember that \texttt{ggplot2} works on layers and that you customise each layer. Edit the above code to add in layer of \texttt{scale\_x\_continuous()} that changes the label \texttt{Abil} to \texttt{Reading\ Ability}.
\end{itemize}

Based on the above visualisations:

\begin{itemize}
\tightlist
\item
  Is the assumption of normality met for both variables? Yes No
\item
  Is the assumption of linearity met for both variables? Yes No
\item
  Is the assumption of homoscedasticity met for both variables? Yes No
\end{itemize}

Explain these answers

When assessing assumptions through the use of visualisations your decision will always be a judgement call. In this dataset, we only have data from 25 participants therefore it is very unlikely we would ever observe perfect normality and linearity in this dataset. It is likely that a researcher would assume that this data is approximately normal, that there is no evidence of a non-linear relationship, and that the spread of data points around the line is relatively even.

Many students become fixated with needing a `perfect' dataset that follows an exactly normal distribution. This is unlikely to ever happen with real data - learn to trust your instincts!

Look at the scatterplot and think back to the lecture, how would you describe this correlation in terms of direction and strength?

\hypertarget{activity-4-descriptive-statistics-1}{%
\section{Activity 4: Descriptive statistics}\label{activity-4-descriptive-statistics-1}}

Many researchers (and indeed members of the School of Psychology!) disagree as to whether you need to report descriptive statistics such as the mean and SD for a correlation. The argument against reporting them is that the scatterplot is actually the descriptive of the correlation that you would use to describe the potential relationship in regards to your hypothesis.

The counter argument is that providing descriptive statistics can still be informative about the spread of data for each variable, for example, in the current example it would make it easier to understand whether the participants as a whole compare to the population IQ score.

There's no fixed answer to this question but the person writing this book takes the second view that you should always report descriptive statistics so that's what we're going to do.

\begin{itemize}
\tightlist
\item
  Calculate the mean score and standard deviation for \texttt{Abil} and \texttt{IQ} using \texttt{summarise()}
\item
  Name the output of the calculations \texttt{Abil\_mean}, \texttt{Abil\_SD}, \texttt{IQ\_mean}, and \texttt{IQ\_SD}. Make sure to use these exact spellings otherwise later activities won't work.
\item
  Store the output of this in an object called \texttt{descriptives} and then view the object. It should look something like this:
\end{itemize}

\begin{tabular}{c|c|c|c}
\hline
Abil\_mean & Abil\_SD & IQ\_mean & IQ\_SD\\
\hline
55.12 & 6.08 & 100.04 & 9.04\\
\hline
\end{tabular}

\hypertarget{activity-5-correlation}{%
\section{Activity 5: Correlation}\label{activity-5-correlation}}

Finally we will run the correlation using the \texttt{cor.test()} function. Remember that for help on any function you can type \texttt{?cor.test} in the console window. The \texttt{cor.test()} function requires:

\begin{itemize}
\tightlist
\item
  the column name of Variable 1
\item
  the column name of Variable 2
\item
  the type of correlation you want to run: e.g. \texttt{pearson}, \texttt{spearman}
\item
  the type of NHST tail you want to run: e.g. \texttt{one.sided}, \texttt{two.sided}
\end{itemize}

For example, if your data is stored in \texttt{dat} and you want to do a two-sided pearson correlation of the variables (columns) \texttt{X} and \texttt{Y}, then you would do:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X, dat}\OperatorTok{$}\NormalTok{Y, }\DataTypeTok{method =} \StringTok{"pearson"}\NormalTok{, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Based on your answers to the assumption tests, spend a couple of minutes deciding with your group which correlation method to use (e.g.~pearson or spearman) and the type of NHST tail to set (e.g.~two.sided or one.sided).
\item
  Run the correlation between IQ and Ability and save it in an object called \texttt{results}.
\item
  View the output by typing \texttt{results} in the console
\end{itemize}

As you can see from the environment, the output from the correlation has saved as a list. This can make it a little more difficult to work with so we're going to use a function from the \texttt{broom} package that we loaded to make the table a bit tidier. The following code is going to overwrite the object results with the tidy version.

\begin{itemize}
\tightlist
\item
  Run the below code and then view the object by clicking on \texttt{results} in the environment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\CommentTok{# take the object results and then}
\StringTok{  }\KeywordTok{tidy}\NormalTok{()              }\CommentTok{# tidy it up}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-6-interpreting-the-correlation}{%
\section{Activity 6: Interpreting the correlation}\label{activity-6-interpreting-the-correlation}}

You should now have a tibble called \texttt{results} that gives you the output of the correlation between Reading Ability and IQ for the school children measured in Miller and Haden (2013) Chapter 11. All that is left to do now, is interpret the output.

Look at \texttt{results}and then with your group, answer the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the value of Pearson's \emph{r} to 2 decimal places? 
\item
  The direction of the relationship between Ability and IQ is: positive negative no relationship
\item
  The strength of the relationship between Ability and IQ is: strong medium weak
\item
  Based on \(\alpha = .05\) the relationship between Ability and IQ is: significant not significant
\item
  The hypothesis was that the reading ability of school children, as measured through a standardized test, and intelligence, again through a standardized test, are positively correlated. Based on the results we can say that the hypothesis: is supported is not supported is proven is not proven
\end{enumerate}

Explain these answers

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The test statistic, in this case the r value, is usually labelled as the \texttt{estimate}.
\item
  If Y increases as X increases then the relationship is positive. If Y increases as X decreases then the relationship is negative. If there is no change in Y as X changes then there is no relationship
\item
  Depending on the field most correlation values greater than .5 would be strong; .3 to .5 as medium, and .1 to .3 as small.
\item
  The field standard says less than .05 is significant and our p-value is less than .05.
\item
  Hypotheses can only be supported or not supported, never proven. In this case, our results matched our hypothesis therefore it is supported.
\end{enumerate}

\hypertarget{activity-7-write-up}{%
\section{Activity 7: Write-up}\label{activity-7-write-up}}

Copy and paste the below \textbf{exactly} into \textbf{white space} in your R Markdown document and then knit the file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The mean IQ score was }\StringTok{`}\DataTypeTok{r round(pluck(descriptives$IQ_mean),2)}\StringTok{`}\NormalTok{ (}\StringTok{`}\DataTypeTok{r round(pluck(descriptives$IQ_SD),2)}\StringTok{`}\NormalTok{) and the mean reading ability score was }\StringTok{`}\DataTypeTok{r round(pluck(descriptives$Abil_mean),2)}\StringTok{`}\NormalTok{ (}\StringTok{`}\DataTypeTok{r round(pluck(descriptives$Abil_SD),2)}\StringTok{`}\NormalTok{). A Pearson\textbackslash{}}\StringTok{`}\DataTypeTok{s correlation found a significant, medium positive correlation between the two variables (r (}\StringTok{`}\NormalTok{r results}\OperatorTok{$}\NormalTok{parameter}\StringTok{`}\DataTypeTok{) = }\StringTok{`}\NormalTok{r }\KeywordTok{round}\NormalTok{(results}\OperatorTok{$}\NormalTok{estimate, }\DecValTok{2}\NormalTok{)}\StringTok{`}\DataTypeTok{, *p* = }\StringTok{`}\NormalTok{r }\KeywordTok{round}\NormalTok{(results}\OperatorTok{$}\NormalTok{p.value, }\DecValTok{3}\NormalTok{)}\StringTok{`}\DataTypeTok{).}
\end{Highlighting}
\end{Shaded}

It will magically transform into:

\begin{quote}
The mean IQ score was 100.04(9.04) and the mean reading ability score was 55.12(6.08). A Pearson`s correlation found a significant, medium positive correlation between the two variables (r (23) = 0.45, \emph{p} = 0.024)
\end{quote}

\hypertarget{activity-8-scatterplot-matrix}{%
\section{Activity 8: Scatterplot matrix}\label{activity-8-scatterplot-matrix}}

Above we ran one correlation and if we wanted to do a different correlation then we would have to edit the \texttt{cor.test()} line and run it again. However, when you have lots of variables in a dataset, to get a quick overview of patterns, one thing you might want to do is run all the correlations at the same time or create a matrix of scatterplots at the one time. You can do this with functions from the \texttt{Hmisc} and \texttt{lsr} packages. We will use the Miller and Haden data here again which you should still have in a tibble called \texttt{mh}.

\begin{itemize}
\tightlist
\item
  Run the following line. The \texttt{pairs()} function from the \texttt{Hmisc} library creates a matrix of scatterplots which you can then use to view all the relationships at the one time.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(mh)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{08-week-5_files/figure-latex/pairs-1} 

}

\caption{Scatterplot matrix}\label{fig:pairs}
\end{figure}

Notice something wrong? \texttt{pairs()} will create scatterplots for \textbf{all} variables in your data (as will \texttt{correlate()} below). This means that it has correlated the Participant ID number as well, which is totally meaningless.

\begin{itemize}
\tightlist
\item
  Overwrite \texttt{mh} and use \texttt{select()} to get rid of the \texttt{Participant} column then run \texttt{pairs(mh)} again.
\end{itemize}

\hypertarget{activity-9-multiple-correlations}{%
\section{Activity 9: Multiple correlations}\label{activity-9-multiple-correlations}}

To perform multiple correlations in one go, we will use the \texttt{correlate()} function from the \texttt{lsr} package. If you look at the help documentation for \texttt{correlate()} you will see that it takes the following form:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{correlate}\NormalTok{(x, }\DataTypeTok{y=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{test=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{corr.method=}\StringTok{"pearson"}\NormalTok{, }\DataTypeTok{p.adjust.method=}\StringTok{"holm"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

You can use \texttt{correlate()} similar to \texttt{cor.test()} and specify a specific variable for both \texttt{x} and \texttt{y} to perform a single correlation. However, you can also provide a data frame that has multiple variables as \texttt{x} and it will run all possible correlations between the variables.

\begin{itemize}
\tightlist
\item
  \texttt{test} controls whether or not p-values will be computed. The default setting for this is \texttt{FALSE}. You will almost always want to change this to \texttt{TRUE}.\\
\item
  \texttt{corr.method()} controls which correlation is computed, the default is \texttt{pearson} but if you needed to run the non-parametric version you could change this to \texttt{spearman}.\\
\item
  \texttt{p.adjust.method} is the reason we are using the \texttt{lsr} package. In the lectures we discussed the problem of multiple comparisons - the idea that if you run lots and lots of tests you're likely to produce a significant p-value just by chance. This argument applies a correction to the p-value that adjusts for the number of correlations you have performed. There are several different methods which you can look up in the help documentation, the default setting is a Bonferroni-Holm correction.\\
\item
  Because you're running multiple correlations and some may be positive and some may be negative, there is no option to specify a one or two-tailed test.
\end{itemize}

There's one last thing we need to do before we run the correlation. \texttt{lsr} is an older package and doesn't like working with tibbles, so we need to convert our tibble to a data frame, an older type of object.

\begin{itemize}
\tightlist
\item
  Run the below code. It will overwrite the tibble \texttt{mh} with a data frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mh <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(mh)}
\end{Highlighting}
\end{Shaded}

OK, finally let's run the correlations.

\begin{itemize}
\tightlist
\item
  Run the below code to calculate then view the correlation results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corr_results <-}\StringTok{ }\KeywordTok{correlate}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mh, }\CommentTok{# our data}
                          \DataTypeTok{test =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# compute p-values}
                          \DataTypeTok{corr.method =} \StringTok{"pearson"}\NormalTok{, }\CommentTok{# run a pearson test }
                          \DataTypeTok{p.adjust.method =} \StringTok{"holm"}\NormalTok{) }\CommentTok{# use the holm correction}
\NormalTok{corr_results}
\end{Highlighting}
\end{Shaded}

\texttt{corr\_results} is a list with 5 components and you can view each of these separately just like you did with chi-square, for example, trying running the below code to view just the correlation values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corr_results}\OperatorTok{$}\NormalTok{correlation}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the correlation between \texttt{Abil} and \texttt{Home} positive or negative? Positive Negative
\item
  This means that as \texttt{Abil} scores increase, \texttt{Home} scores will Increase Decrease
\item
  What is the strongest positive correlation? Abil * IQ Abil * Home Abil * TV
\item
  What is the strongest negative correlation? Abil * TV IQ * TV Home * TV
\item
  Is the correlation between \texttt{Abil} and \texttt{IQ} significant? Yes No
\item
  Is the correlation between \texttt{Abil} and \texttt{Home} significant? Yes No
\item
  How would you describe the strength of the correlation between \texttt{Home} and \texttt{TV}? Weak Medium Strong
\item
  Think back to the lecture. Why are we not calculating an effect size?
\end{enumerate}

Explain these answers

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Negative correlations are denoted by a negative r value.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    Positive correlations = as one score goes up so does the other, negative correlations = as one score goes up the other goes down.\\
    3 \& 4. Remember that correlations take values from -1 - 1 and that the nearer to one in either direction the stronger the correlation (i.e., an r value of 0 would demonstrate a lack of any relationship.\\
    5 \& 6. The traditional cut-off for significance is .05. Anything below .05 is considered significant. Be careful to pay attention to decimal places.\\
  \item
    Cohen's guidelines recommend weak = 1. - .3, medium = .3 - .5, strong \textgreater{} .5.\\
  \item
    Because r is an effect size.
  \end{enumerate}
\end{enumerate}

\hypertarget{finished-4}{%
\subsection{Finished!}\label{finished-4}}

Well done! You can now add correlations to the list of things you can do in R. If you have any questions, please post them on Slack or Moodle.

\hypertarget{activity-solutions-3}{%
\section{Activity solutions}\label{activity-solutions-3}}

\hypertarget{activity-1-1}{%
\subsection{Activity 1}\label{activity-1-1}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"lsr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"Hmisc"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\NormalTok{mh <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"MillerHadenData.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-3-2}{%
\subsection{Activity 3}\label{activity-3-2}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histogram}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ mh, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ IQ)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}

\CommentTok{# qqplot}
\KeywordTok{qqPlot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mh}\OperatorTok{$}\NormalTok{IQ)}

\CommentTok{# edited scatterplot}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ mh, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Abil, }\DataTypeTok{y =}\NormalTok{ IQ)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Reading Ability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-5-2}{%
\subsection{Activity 5}\label{activity-5-2}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{cor.test}\NormalTok{(mh}\OperatorTok{$}\NormalTok{Abil, mh}\OperatorTok{$}\NormalTok{IQ, }\DataTypeTok{method =} \StringTok{"pearson"}\NormalTok{, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-8-2}{%
\subsection{Activity 8}\label{activity-8-2}}

Activity 8

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mh <-}\StringTok{ }\NormalTok{mh }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Participant)}
\KeywordTok{pairs}\NormalTok{(mh)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{missing-data}{%
\chapter{Missing Data}\label{missing-data}}

In this chapter we're going to focus on how to deal with missing data and to reinforce the concept of tidy data. So far, we've given you complete datasets to work with, however, you will find that real data is often much messier than this, for example, participants may not answer some items in your questionnaire or may not turn up for repeated testing sessions.

\hypertarget{activity-1-set-up-6}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-6}}

Do the following. If you need help, consult Chapter \ref{ref3} and Chapter \ref{ref2}.

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 6 folder. Ensure the environment is clear. * Open a new R Markdown document and save it in your working directory. Call the file ``Week 6''.\\
\item
  Download messy.csvand save it in your Week 6 folder. Make sure that you do not change the file name at all.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the \texttt{tidyverse} using the \texttt{library()} function and loads the data into an object named \texttt{messy} using \texttt{read\_csv()}
\end{itemize}

\hypertarget{activity-2-look-at-the-data-2}{%
\section{Activity 2: Look at the data}\label{activity-2-look-at-the-data-2}}

\texttt{messy} is simulated data for an experiment looking at the effect of note-taking on test performance and whether this is affected by being a native speaker. Participants are first given a pre-test to judge their baseline knowledge, then they watch a lecture and take notes. Immediately after the lecture is finished they take another test. Finally, they are tested after a week delay. The dataset has six variables:

\begin{itemize}
\tightlist
\item
  \texttt{id} = the participant ID number\\
\item
  \texttt{speaker} = if the participant is a native or non-native English speaker\\
\item
  \texttt{gender} = if the participant is male, female, or non-binary\\
\item
  \texttt{pre} = pre-test score before any notes were taken\\
\item
  \texttt{post} = post-test score immediately after the lecture\\
\item
  \texttt{delay} = test score after one week delay
\end{itemize}

As you can see, there are a number of missing values in the columns \texttt{speaker} and \texttt{gender} (perhaps due to participants not wanting to divulge that information, perhaps due to experimenter error), and \texttt{delay} (due to participants not returning to be tested for the final session).

The best way to get a sense of how many missing data points you have is to use \texttt{summary()}. Because \texttt{speaker} and \texttt{gender} are text rather than numbers, in order to see how many values are missing we first need to convert them to factors.

\begin{itemize}
\tightlist
\item
  Run the below code
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy <-}\StringTok{ }\NormalTok{messy }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{speaker =} \KeywordTok{as.factor}\NormalTok{(speaker), }\CommentTok{# overwrite `speaker` with `speaker` as a factor}
         \DataTypeTok{gender =} \KeywordTok{as.factor}\NormalTok{(gender))}

\KeywordTok{summary}\NormalTok{(messy)}
\end{Highlighting}
\end{Shaded}

As you can see, there are 20 data points missing in each of \texttt{speaker}, \texttt{gender}, and \texttt{delay} (but importantly, this isn't from just 20 participants).There are several different approaches to dealing with missing data. We will cover the most common.

\hypertarget{activity-3-listwise-deletion}{%
\section{Activity 3: Listwise deletion}\label{activity-3-listwise-deletion}}

One method for dealing with missing data is \textbf{listwise deletion}. This approach removes any participant with a single missing value. So if there is missing data in any of the six columns in the dataset, that participant will be removed and you will only be left with complete datasets. We can achieve this using \texttt{drop\_na}

\begin{itemize}
\tightlist
\item
  Run the below code and then view the object.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy_listwise <-}\StringTok{ }\KeywordTok{drop_na}\NormalTok{(messy)}
\end{Highlighting}
\end{Shaded}

As you can see \texttt{messy\_listwise} now only contains data from participants with a complete set of data. This might seem like a good thing, and sometimes it is the most appropriate option, however, there are a couple of important points to consider.

First, \texttt{gender} isn't part of our experiment - it's not one of the IVs, it's just there as demographic information. We wouldn't include \texttt{gender} in any of our analyses but because of listwise deletion we have deleted experimental data if the participant was missing \texttt{gender}. This is related to the second problem which is that using full listwise deletion may result in the loss of a lot of data. Look at the environment pane - the original dataset had 200 participants, after using \texttt{drop\_na()} we only have 143 so we've lost over 25\% of our data by doing this. If this was real data we would also want to check if the missing values were coming from one particular group (i.e., non-random attrition).

One option is to amend the use of \texttt{drop\_na()} so that it doesn't include \texttt{gender} and we can do this using the same code as we would if we were using \texttt{select()}.

\begin{itemize}
\tightlist
\item
  Run the below code. How many observations does \texttt{messy\_listwise2} have? 
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy_listwise2 <-}\StringTok{ }\KeywordTok{drop_na}\NormalTok{(messy, }\OperatorTok{-}\NormalTok{gender)}
\end{Highlighting}
\end{Shaded}

\hypertarget{pairwise-deletion}{%
\section{Pairwise deletion}\label{pairwise-deletion}}

The alternative to listwise deletion is \textbf{pairwise deletion} when cases are removed depending upon the analysis. For example, if we were to calculate the correlations between \texttt{pre}, \texttt{post}, and \texttt{delay} without removing participants with missing data in the \texttt{delay} condition, R would use different numbers of participants in each correlation depending on missing data which you can see in the \texttt{Sample\ Sizes} section.

\begin{verbatim}
## 
## CORRELATIONS
## ============
## - correlation type:  pearson 
## - correlations shown only when both variables are numeric
## 
##         pre     post    delay   
## pre       .    0.428*** 0.518***
## post  0.428***     .    0.554***
## delay 0.518*** 0.554***     .   
## 
## ---
## Signif. codes: . = p < .1, * = p<.05, ** = p<.01, *** = p<.001
## 
## 
## p-VALUES
## ========
## - total number of tests run:  3 
## - correction for multiple testing:  holm 
## 
##         pre  post delay
## pre       . 0.000 0.000
## post  0.000     . 0.000
## delay 0.000 0.000     .
## 
## 
## SAMPLE SIZES
## ============
## 
##       pre post delay
## pre   200  200   180
## post  200  200   180
## delay 180  180   180
\end{verbatim}

\hypertarget{activity-3-na.rm-true}{%
\section{\texorpdfstring{Activity 3: \texttt{na.rm\ =\ TRUE}}{Activity 3: na.rm = TRUE}}\label{activity-3-na.rm-true}}

When running inferential tests like correlations and t-tests, R will usually know when to ignore missing values. However, if you're calculating descriptive statistics or if you want to calculate the average score of a number of different items, you need to explicitly tell R to ignore the missing values.

\begin{itemize}
\tightlist
\item
  Run the below code to calculate the mean score for each testing condition.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(messy, }
          \DataTypeTok{pre_mean =} \KeywordTok{mean}\NormalTok{(pre),}
          \DataTypeTok{post_mean =} \KeywordTok{mean}\NormalTok{(post),}
          \DataTypeTok{delay_mean =} \KeywordTok{mean}\NormalTok{(delay)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c}
\hline
pre\_mean & post\_mean & delay\_mean\\
\hline
10 & 17 & NA\\
\hline
\end{tabular}

The mean score for \texttt{delay} shows as \texttt{NA}. This is because R is trying to calculate an average of a dataset and including the missing value and this creates a logical problem (how do you take the average of nothing?). In order to calculate the mean we have to tell R to ignore the missing values by adding \texttt{na.rm\ =\ TRUE} to our code. You can read this as ``remove the NAs? Yes''.

\begin{itemize}
\tightlist
\item
  Run the below code. What is the mean score for the \texttt{delay} condition to 2 decimal places? 
\end{itemize}

There are other options, for example, some researchers will replace missing values with a particular score (e.g., the mean) using \texttt{replace\_na()} or you might have a cut-off such as participants need to answer at least 80\% of all questionnaire items to be included. If you're doing a quantitative dissertation, these are things to discuss with your supervisor. The key thing is to know your data.

\hypertarget{activity-4-tidy-data}{%
\section{Activity 4: Tidy data}\label{activity-4-tidy-data}}

For the rest of this chapter we're going to focus on tidy data. You already covered this in Chapter \ref{gather} when we introduced \texttt{gather()} but this is a very common task so it's worth repeating.

Remember the rules of tidy data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable must have its own column.
\item
  Each observation must have its own row.
\item
  Each value must have its own cell (i.e.~no grouping two variables together, e.g.~time/date in one cell).
\end{enumerate}

There are a number of different types of variable you might have in your dataset.

\begin{itemize}
\tightlist
\item
  Demographic variables like subject ID, age, and gender\\
\item
  Independent/predictor variables like experimental condition (but these could also be demographic groups like gender or native speaker status)\\
\item
  Dependent variables like score, reaction time, questionnaire response
\end{itemize}

How many variables does \texttt{messy} have? 4 5 6 7

Explain this answer

There are five variables. There are two demographic variables \texttt{id} and \texttt{gender}. There are two independent variables, \texttt{speaker} which is a between-subject variable, and test time which is a within-subject variable. Finally, there is one dependent variable, the participant's score.

\begin{itemize}
\tightlist
\item
  Use \texttt{gather()} to tidy the \texttt{messy} data and save it as a new object named \texttt{tidy}. Refer back to Chapter \ref{gather} if you need help with the code. Your tidy dataset should look like this:
\end{itemize}

\begin{tabular}{c|c|c|c|c}
\hline
id & speaker & gender & test\_time & score\\
\hline
S001 & native & female & pre & 13.91\\
\hline
S002 & native & male & pre & 4.35\\
\hline
S003 & native & female & pre & 15.58\\
\hline
S004 & native & nonbinary & pre & 8.36\\
\hline
S005 & native & nonbinary & pre & 7.68\\
\hline
S006 & native & male & pre & 17.58\\
\hline
\end{tabular}

\hypertarget{activity-5-questionnaire-data}{%
\section{Activity 5: Questionnaire data}\label{activity-5-questionnaire-data}}

One very common type of data you are likely to work with is questionnaire data.

\begin{itemize}
\tightlist
\item
  Download the questionnaire\_data.csv and scale\_info.csv and load them into new objects named \texttt{qdat} and \texttt{scales} using \texttt{read\_csv()} and then view the objects.
\end{itemize}

Again, this is simulated data that we're using for the purpose of this activity but this is a problem you're very likely to face.

\begin{itemize}
\tightlist
\item
  There are nine questions in total.\\
\item
  There are two scales, one scale asks about attitudes to homosexual people and has two sub-scales, attitudes to lesbians and attitudes to gay men. The second scale asks about gender role beliefs.\\
\item
  Questions 1 - 6 are from the homosexuality attitude scale (Q1 - Q3 lesbians, Q4 - Q6 gay men).
\item
  Questions 7 - 9 are from the gender role beliefs scale.
\end{itemize}

First, we're going to gather up all of the questions into one column using \texttt{gather()}.

\begin{itemize}
\tightlist
\item
  Run the below code to gather the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qdat_tidy <-}\StringTok{ }\NormalTok{qdat }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ item, }\DataTypeTok{value =}\NormalTok{ response, Q1}\OperatorTok{:}\NormalTok{Q9)}
\end{Highlighting}
\end{Shaded}

Next, we want to add in the information in \texttt{scales}.

\begin{itemize}
\tightlist
\item
  Join \texttt{qdat\_tidy} and \texttt{scales} by ``item'' using \texttt{inner\_join()}. If you need help with this, refer back to Chapter \ref{join}.
\end{itemize}

Your data should look like this:

\begin{tabular}{r|l|r|l}
\hline
id & item & response & scale\\
\hline
1 & Q1 & 1 & lesbian\\
\hline
2 & Q1 & 1 & lesbian\\
\hline
3 & Q1 & 4 & lesbian\\
\hline
4 & Q1 & 3 & lesbian\\
\hline
5 & Q1 & 3 & lesbian\\
\hline
6 & Q1 & 3 & lesbian\\
\hline
\end{tabular}

\hypertarget{activity-6-calculating-scale-scores}{%
\section{Activity 6: Calculating scale scores}\label{activity-6-calculating-scale-scores}}

Commonly with questionnaire data you will need to calculate the average response for a scale or a sub-scale for each participant. You can do this using \texttt{group\_by()} and \texttt{summarise()} that you should now be familiar with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qdat_scores <-}\StringTok{ }\NormalTok{qdat_tidy }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(id, scale) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_score =} \KeywordTok{mean}\NormalTok{(response)) }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c}
\hline
id & scale & mean\_score\\
\hline
1 & gay & 3.67\\
\hline
1 & gender\_roles & 5.00\\
\hline
1 & lesbian & 3.33\\
\hline
2 & gay & 2.67\\
\hline
2 & gender\_roles & 3.00\\
\hline
2 & lesbian & 1.67\\
\hline
\end{tabular}

What you do now will largely depend upon the exact design of your study. You might want to keep the data like this which would make it easy to perform operations on each of the three scales simultaneously. For example, you could easily create a plot that shows the scores.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(qdat_scores, }\KeywordTok{aes}\NormalTok{(scale, mean_score)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{09-week-6_files/figure-latex/img-scale-1} 

}

\caption{Boxplots of scale scores}\label{fig:img-scale}
\end{figure}

\hypertarget{activity-7-spread-back-to-wide-form}{%
\section{\texorpdfstring{Activity 7: \texttt{spread()} back to wide-form}{Activity 7: spread() back to wide-form}}\label{activity-7-spread-back-to-wide-form}}

You may also want to transform the dataset back to wide-form if, for example, you wanted to run correlations between the variables. You can do this using \texttt{spread()} a function we haven't used but essentially works like the reverse of \texttt{gather()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qdat_wide <-}\StringTok{ }\NormalTok{qdat_scores}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ scale, }\CommentTok{# the values in this column will become the name of the columns}
         \DataTypeTok{value =}\NormalTok{ mean_score) }\CommentTok{# the values in this column will fill the cells}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c|c}
\hline
id & gay & gender\_roles & lesbian\\
\hline
1 & 3.67 & 5.00 & 3.33\\
\hline
2 & 2.67 & 3.00 & 1.67\\
\hline
3 & 4.67 & 6.00 & 4.33\\
\hline
4 & 4.67 & 5.67 & 4.00\\
\hline
5 & 3.67 & 4.67 & 3.67\\
\hline
6 & 4.33 & 4.67 & 3.67\\
\hline
\end{tabular}

\hypertarget{finished-5}{%
\section{Finished!}\label{finished-5}}

And you're done! This isn't a comprehensive tutorial on every type of dataset you will come across and the concept of tidy data will take practice but hopefully this should give you a good starting point for when you have your own real, messy data.

\hypertarget{activity-solutions-4}{%
\section{Activity solutions}\label{activity-solutions-4}}

\hypertarget{activity-1-2}{%
\subsection{Activity 1}\label{activity-1-2}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\NormalTok{messy <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"messy.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-4-1}{%
\subsection{Activity 4}\label{activity-4-1}}

Activity 4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ messy, }\DataTypeTok{key =}\NormalTok{ test_time, }\DataTypeTok{value =}\NormalTok{ score, pre}\OperatorTok{:}\NormalTok{delay)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-5-3}{%
\subsection{Activity 5}\label{activity-5-3}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# loading in the data}
\NormalTok{qdat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"questionnaire_data.csv"}\NormalTok{)}
\NormalTok{scales <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"scale_info.csv"}\NormalTok{)}

\CommentTok{# join the datasets}

\NormalTok{qdat_tidy <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ qdat_tidy, }\DataTypeTok{y =}\NormalTok{ scales, }\DataTypeTok{by =} \StringTok{"item"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{t-tests}{%
\chapter{t-tests}\label{t-tests}}

Two-sample designs are very common as often we want to know whether there is a difference between groups on a particular variable. There are different types of two-sample designs depending on whether or not the two groups are independent (e.g.~different participants on different conditions) or not (e.g.~same participants on different conditions). In this lab we will perform one test of each type.

\begin{try}
One of the really confusing things about research design is that there
are many names for the same type of design.

\begin{itemize}
\tightlist
\item
  Independent and between-subjects design typically mean the same thing
  - different participants in different conditions
\item
  Within-subjects, dependent, paired samples, and repeated-measures tend
  to mean the same participants in all conditions
\item
  Matched pairs design means different people in different conditions
  but you have matched participants across the conditions so that they
  are effectively the same person (e.g.~age, IQ, Social Economic Status,
  etc)
\item
  Mixed design is when there is a combination of within-subjects and
  between-subjects designs in the one experiment. For example, say you
  are looking at attractiveness and dominance of male and female faces.
  Everyone might see both male and female faces (within) but half of the
  participants do ratings of attractiveness and half do ratings of
  trustworthiness (between).
\end{itemize}
\end{try}

For the independent t-test we will be using data from Schroeder and Epley (2015). You can take a look at the Psychological Science article here:

\href{https://doi.org/10.1177/0956797615572906}{Schroeder, J. and Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate's appeal. Psychological Science, 26, 277--891.}

The abstract from this article explains more about the different experiments conducted (we will be specifically looking at the dataset from Experiment 4, courtesy of the \href{https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities}{Open Stats Lab}:

\begin{quote}
A person's mental capacities, such as intellect, cannot be observed directly and so are instead inferred from indirect cues. We predicted that a person's intellect would be conveyed most strongly through a cue closely tied to actual thinking: his or her voice. Hypothetical employers (Experiments 1-3b) and professional recruiters (Experiment 4) watched, listened to, or read job candidates' pitches about why they should be hired. These evaluators (the employers) rated a candidate as more competent, thoughtful, and intelligent when they heard a pitch rather than read it and, as a result, had a more favourable impression of the candidate and were more interested in hiring the candidate. Adding voice to written pitches, by having trained actors (Experiment 3a) or untrained adults (Experiment 3b) read them, produced the same results. Adding visual cues to audio pitches did not alter evaluations of the candidates. For conveying one's intellect, it is important that one's voice, quite literally, be heard.
\end{quote}

To summarise, 39 professional recruiters from Fortune 500 companies evaluated job pitches of M.B.A. candidates from the University of Chicago Booth School of Business. The methods and results appear on pages 887--889 of the article if you want to look at them specifically for more details. The original data, in \textbf{wide} format, can be found at the \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOei1MM24xNndnQ00}{Open Stats Lab} website for later self-directed learning. Today however, we will be working with a modified version in ``tidy'' format which can be downloaded from Moodle.

\hypertarget{activity-1-set-up-7}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-7}}

Your task is to reproduce the results from the article (p.~887).

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 7 folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 7''.\\
\item
  Download evaluators.csv and rating.csv and save them in your Week 7 folder. Make sure that you do not change the file name at all.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads \texttt{broom}, \texttt{car}, \texttt{lsr}, and \texttt{tidyverse} using the \texttt{library()} function and loads the data into an object named \texttt{evaluators} using \texttt{read\_csv()}
\end{itemize}

\hypertarget{activity-2-explore-the-dataset}{%
\section{Activity 2: Explore the dataset}\label{activity-2-explore-the-dataset}}

There are a few things we should do to explore the dataset and make working with it a bit easier. You have done all of these tasks before, use the book search function if you can't remember how to do them. As always the solutions are at the bottom but you will learn faster if you can solve the problem yourself.

\begin{itemize}
\item
  Use \texttt{mutate()} and \texttt{recode()} to recode \texttt{sex} into a new variable \texttt{sex\_labels} so that \texttt{1} = \texttt{male} and \texttt{2} = \texttt{female}. Be careful - there are multiple functions in different packages called recode, make sure to specify \texttt{dplyr::recode()} to get the right one.
\item
  Use \texttt{mutate()} and \texttt{as.factor()} to overwrite \texttt{sex\_labels} and \texttt{condition} as factors.\\
\item
  Use \texttt{summary()} to get an overview of the missing data points in each variable.
\item
  How many participants were noted as being female: 
\item
  How many participants were noted as being male: 
\item
  How many data points are missing for \texttt{sex}? 
\end{itemize}

\hypertarget{activity-3-ratings}{%
\section{Activity 3: Ratings}\label{activity-3-ratings}}

We are now going calculate an overall intellect rating given by each evaluator - how intellectual the evaluators thought candidates were overall depending on whether or not the evaluators \textbf{read} or \textbf{listened} to the candidates' resume pitches. This is calculated by averaging the ratings of \texttt{competent}, \texttt{thoughtful} and \texttt{intelligent} for each evaluator; held within \texttt{ratings.csv}. \textbf{Note:} we are not looking at ratings to individual candidates; we are looking at overall ratings for each evaluator. This is a bit confusing but makes sense if you stop to think about it a little.

We will then combine the overall intellect rating with the overall impression ratings and overall hire ratings for each evaluator, with the end goal of having a tibble called \texttt{ratings2} - which has the following structure:

\begin{tabular}{r|l|r|l|l}
\hline
eval\_id & Category & Rating & condition & sex\_labels\\
\hline
1 & hire & 6.000 & listened & female\\
\hline
1 & impression & 7.000 & listened & female\\
\hline
1 & intellect & 6.000 & listened & female\\
\hline
2 & hire & 4.000 & listened & female\\
\hline
2 & impression & 4.667 & listened & female\\
\hline
2 & intellect & 5.667 & listened & female\\
\hline
\end{tabular}

The following steps describe how to create the above tibble - if you're feeling comfortable with R, try yourself without using our code. The trick when doing data analysis and data wrangling is to first think about what you want to achieve - the end goal - and then think about what functions you need to use to get there.

Steps 1-3 calculate the new \texttt{intellect} rating. Steps 4 and 5 combine this rating to all other information.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Load the data found in \texttt{ratings.csv} into a tibble called \texttt{ratings}.
\item
  \texttt{filter()} only the relevant variables (\textbf{thoughtful}, \textbf{competent}, \textbf{intelligent}) into a new tibble (call it what you like - we use \texttt{iratings}), and calculate a mean \texttt{Rating} for each evaluator.
\item
  Add on a new column called \texttt{Category} where every entry is the word \texttt{intellect}. This tells us that every number in this tibble is an intellect rating.
\item
  Now create a new tibble called \texttt{ratings2} and filter into it just the ``impression'' and ``hire'' ratings from the original \texttt{ratings} tibble. Next, bind this tibble with the tibble you created in step 3 to bring together the intellect, impression, and hire ratings, in \texttt{ratings2}.
\item
  Join \texttt{ratings2} with the \texttt{evaluator} tibble that we created in Task 1. Keep only the necessary columns as shown above and arrange by Evaluator and Category.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1. load in the data}
\NormalTok{ratings <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"ratings.csv"}\NormalTok{)}

\CommentTok{# 2. first step: pull out the ratings associated with intellect}
\NormalTok{iratings <-}\StringTok{ }\NormalTok{ratings }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Category }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"competent"}\NormalTok{, }\StringTok{"thoughtful"}\NormalTok{, }\StringTok{"intelligent"}\NormalTok{))}

\CommentTok{# second step: calculate means for each evaluator}
\NormalTok{imeans <-}\StringTok{ }\NormalTok{iratings }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(eval_id) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Rating =} \KeywordTok{mean}\NormalTok{(Rating))}

\CommentTok{# 3. add Category variable }
\CommentTok{# this way we can combine with 'impression' and 'hire' into a single table, very useful!}
\NormalTok{imeans2 <-}\StringTok{ }\NormalTok{imeans }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Category =} \StringTok{"intellect"}\NormalTok{)}

\CommentTok{# 4. & 5. combine into a single table}
\NormalTok{ratings2 <-}\StringTok{ }\NormalTok{ratings }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Category }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"impression"}\NormalTok{, }\StringTok{"hire"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bind_rows}\NormalTok{(imeans2) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(evaluators, }\StringTok{"eval_id"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{age, }\OperatorTok{-}\NormalTok{sex) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(eval_id, Category)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-4-visualisation}{%
\section{Activity 4: Visualisation}\label{activity-4-visualisation}}

You should \textbf{always} visualise your data before you run a statistical analysis. Not only will it help you interpret the results of the test but it will give you a better understanding of the spread of your data. For comparing two means, we can take advantage of the many plotting options R provides so we don't have to settle for a boring (and more importantly, uninformative, bar plot).

To visualise our data we are going to create a violin-boxplot.

\texttt{geom\_violin()} represents density. The fatter the plot, the more data points there are for that . The reason is is called a violin plot is because if your data are normally distributed it should look something like a violin.\\
\texttt{geom\_boxplot()} shows the median and inter-quartile range (see \href{https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51}{here} if you would like more information). The boxplot can also give you a good idea if the data are skewed - the median line should be in the middle of the box, if it's not, chances are the data are skewed.\\
\texttt{geom\_pointrange()} will show the mean and confidence intervals. If you're conducting a test that is comparing means, it's a good idea to add in the means.

\begin{itemize}
\tightlist
\item
  Run the below code to produce the plot. It is a good idea to save code `recipes' for tasks that you will likely want to repeat in the future. You do not need to memorise lines of code, you only need to understand how to alter examples to work with your specific data set.
\item
  Try setting \texttt{trim\ =\ TRUE}, \texttt{show.legend\ =\ FALSE} and altering the value of \texttt{width} to see what these arguments do.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create summary data to use with `geom_pointrange()`}
\NormalTok{summary_dat<-ratings2}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(condition)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(Rating),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(Rating) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(Rating)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }\CommentTok{#confidence intervals}
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(Rating) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(Rating)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()))}


\KeywordTok{ggplot}\NormalTok{(ratings2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{y =}\NormalTok{ Rating)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ condition), }\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_dat,}
                  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{y =}\NormalTok{ mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{), }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Look at the plot. In which condition did the evaluators give the higher ratings? listened read
\end{itemize}

\hypertarget{activity-5-assumptions}{%
\section{Activity 5: Assumptions}\label{activity-5-assumptions}}

Before we run the t-test we need to check that the data meet the assumptions for a Welch t-test.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The data are interval/ratio
\item
  The data are independent
\item
  The residuals are normally distributed for each group
\end{enumerate}

We know that 1 and 2 are true from the design of the experiment, the measures used, and by looking at the data. To test assumption 3, we can create a QQ-plot of the \textbf{residuals}. For a between-subject t-test the residuals are the difference between the mean of each group and each data point. E.g., if the mean of group A is 10 and a participant in group A scores 12, the residual for that participant is 2.

\begin{itemize}
\tightlist
\item
  Run the below code to calculate then plot the residuals. Based upon the plot, do the data meet the assumption of normality? Yes No
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ratings2 <-}\StringTok{ }\NormalTok{ratings2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(condition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{group_resid =}\NormalTok{ Rating }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Rating)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}

\KeywordTok{qqPlot}\NormalTok{(ratings2}\OperatorTok{$}\NormalTok{group_resid)}
\end{Highlighting}
\end{Shaded}

We can also use a new test that will statistically test the residuals for normality, the \textbf{Shapiro-Wilk} test. \texttt{shapiro.wilk()} from Base R assesses if the distribution is significantly different from a normal distribution, so, if the test is significant it means your data is not normal, and if it is non-significant it means it is approximately normal.

\begin{itemize}
\tightlist
\item
  Run the below code. According to the Shapiro-Wilk test, is the data normally distributed? Yes No
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ ratings2}\OperatorTok{$}\NormalTok{group_resid)}
\end{Highlighting}
\end{Shaded}

Explain this answer

The p-value is .2088 which is more than .05, the cut-off for statistical significance.

\begin{itemize}
\tightlist
\item
  Think back to the lecture. If you ran a Student's t-test instead of a Welch t-test, what would the 4th assumption be? Homogeneity of variance Homoscedascity Nominal data\\
\item
  Why should you always use a Welch test instead of a Student t-test? Because it rhymes with squelch which is a funny word Because you are more likely to obtain a signifcant p-value than with Student's t-test when sample sizes and variances are equal Because it performs better if sample sizes and variances are unequal and gives the same result when sample sizes and variances are equal.
\end{itemize}

\hypertarget{activity-6-running-the-t-test}{%
\section{Activity 6: Running the t-test}\label{activity-6-running-the-t-test}}

We are going to conduct t-tests for the Intellect, Hire and Impression ratings separately; each time comparing evaluators' overall ratings for the listened group versus overall ratings for the read group to see if there was a significant difference between the two conditions: i.e.~did the evaluators who \textbf{listened} to pitches give a significant higher or lower rating than evaluators that \textbf{read} pitches.

\begin{itemize}
\tightlist
\item
  First, calculate the mean and SD for each condition and category.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group_means <-}\StringTok{ }\NormalTok{ratings2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(condition, Category) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{m =} \KeywordTok{mean}\NormalTok{(Rating), }\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(Rating))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Next, create separate data sets for the intellect, hire, and impression data using \texttt{filter()}. We have completed intellect for you.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intellect <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(ratings2, Category }\OperatorTok{==}\StringTok{ "intellect"}\NormalTok{)}
\NormalTok{hire <-}\StringTok{ }
\NormalTok{impression <-}\StringTok{ }
\end{Highlighting}
\end{Shaded}

As you may have realised by now, most of the work of statistics involves the set-up - running the tests is generally very simple. To conduct the t-test we will use \texttt{t.test()} from Base R. This function uses a style of code you haven't come across yet but that is very important to get used to, \textbf{formula syntax}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results_intellect <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(Rating }\OperatorTok{~}\StringTok{ }\NormalTok{condition, }\DataTypeTok{paired =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ intellect) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{\textasciitilde{}} is called a tilde. It can be read as `by'.\\
\item
  The variable on the left of the tilde is the dependent or outcome variable.
\item
  The variable(s) on the right of the tilde is the independent or predictor variable.\\
\item
  You can read the below code as `run a t-test for rating score by condition'.
\item
  \texttt{paired\ =\ FALSE} indicates that we do not want to run a paired-samples test and that our data is from a between-subjects design.
\end{itemize}

Just like we did with the correlation, we are also going to use \texttt{tidy()} to convert the output to a more manageable format.

\begin{itemize}
\tightlist
\item
  Run the above code and then view the \texttt{results\_intellect}.
\end{itemize}

The output is in a nice table format that makes it easy to extract individual values but it is worth explaining what each variable means:

\begin{itemize}
\item
  \texttt{estimate} is the difference between the two means
\item
  \texttt{estimate1} is the mean of group 1\\
\item
  \texttt{estimate2} is the mean of group 2\\
\item
  \texttt{statistic} is the t-statistic\\
\item
  \texttt{p.value} is the p-value\\
\item
  \texttt{parameter} is the degrees of freedom\\
\item
  \texttt{con.low} and \texttt{conf.high} are the confidence interval of the \texttt{estimate}
\item
  \texttt{method} is the type of test, Welch, Student, paired, or one-sample
\item
  \texttt{alternative} is whether the test was one or two-tailed
\item
  Complete the code to run the t-tests for the hire and impression ratings and view the results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results_hire <-}\StringTok{ }
\NormalTok{results_impression <-}\StringTok{ }
\end{Highlighting}
\end{Shaded}

\begin{warning}
What do you do if the data don't meet the assumption of normality? There
are a few options.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Transform your data to try and normalise the distribution. We won't
  cover this but if you'd like to know more,
  \href{https://www.researchgate.net/profile/Jason_Osborne2/publication/200152356_Notes_on_the_Use_of_Data_Transformations/links/0deec5295f1eb10df8000000.pdf}{this
  page} is a good start.
\item
  Use a non-parametric test. The non-parametric equivalent of the
  independent t-test is the Mann-Whitney and the equivalent of the
  paired-samples t-test is the Wilcoxon. See the Supplementary Analyses
  chapter for more information.
\item
  Do nothing.
  \href{https://www.rips-irsp.com/articles/10.5334/irsp.82/}{Delacre,
  Lakens \& Leys, 2017} argue that with a large enough sample
  (\textgreater{}30), the Welch test is robust and that using a two-step
  process actually causes more problems than it solves.
\end{enumerate}
\end{warning}

\hypertarget{activity-8-correcting-for-multiple-comparisons}{%
\section{Activity 8: Correcting for multiple comparisons}\label{activity-8-correcting-for-multiple-comparisons}}

Because we've run three t-tests we risk inflating our chances of a Type 1 errors due to familywise error. To correct for this we can apply a correction for multiple comparisons.

To do this first of all we need to join all the results of the t-tests
together using \texttt{bind\_rows()}. First, you specify all of the individual tibbles you want to join and give them a label, and then you specify what the ID column should be named.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(}\DataTypeTok{hire =}\NormalTok{ results_hire, }\DataTypeTok{impression =}\NormalTok{ results_impression, }\DataTypeTok{intellect =}\NormalTok{ results_intellect, }\DataTypeTok{.id =} \StringTok{"test"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\hline
test & estimate & estimate1 & estimate2 & statistic & p.value & parameter & conf.low & conf.high & method & alternative\\
\hline
hire & 1.825397 & 4.714286 & 2.888889 & 2.639949 & 0.0120842 & 36.85591 & 0.4241979 & 3.226596 & Welch Two Sample t-test & two.sided\\
\hline
impression & 1.894333 & 5.968333 & 4.074000 & 2.817175 & 0.0080329 & 33.80061 & 0.5275086 & 3.261158 & Welch Two Sample t-test & two.sided\\
\hline
intellect & 1.986722 & 5.635000 & 3.648278 & 3.478555 & 0.0014210 & 33.43481 & 0.8253146 & 3.148130 & Welch Two Sample t-test & two.sided\\
\hline
\end{tabular}

Now, we're going to add on a column of adjusted p-values using \texttt{p.adj()} and \texttt{mutate()}.

\begin{itemize}
\tightlist
\item
  Run the below code and then view the adjusted p-values. Are they larger or smaller than the original values? Larger Smaller
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{p.adjusted =} \KeywordTok{p.adjust}\NormalTok{(}\DataTypeTok{p =}\NormalTok{ p.value, }\CommentTok{# the column that contains the original p-values}
                               \DataTypeTok{method =} \StringTok{"bonferroni"}\NormalTok{)) }\CommentTok{# type of correction to apply}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-7-effect-size}{%
\section{Activity 7: Effect size}\label{activity-7-effect-size}}

Before we interpret and write-up the results our last task is to calculate the effect size which for a t-test is Cohen's D. To do this, we will use the function \texttt{cohensD()} from the \texttt{lsr} package. The code is very simple, it is very similar to the syntax for \texttt{t.test()}. The only difference is rather than \texttt{paired\ =\ FALSE}, you must specify \texttt{method\ =\ "unequal"} which indicates that we conducted a Welch test (see the help documentation for more information).

\begin{itemize}
\tightlist
\item
  Run the below code and then calculate the effect sizes for hire and impression
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intellect_d <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(Rating }\OperatorTok{~}\StringTok{ }\NormalTok{condition, }\DataTypeTok{method =} \StringTok{"unequal"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ intellect)}
\NormalTok{hire_d <-}\StringTok{ }
\NormalTok{impression_d <-}\StringTok{ }
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-8-interpreting-the-results}{%
\section{Activity 8: Interpreting the results}\label{activity-8-interpreting-the-results}}

\begin{itemize}
\item
  Were your results for \texttt{hire} significant? Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places). Use the adjusted p-values:

  \begin{itemize}
  \item
    Mean \texttt{estimate1} (listened condition) = 
  \item
    Mean \texttt{estimate2} (read condition) = 
  \item
    t() = , p = 
  \end{itemize}
\item
  Were your results for \texttt{impression} significant? Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places):

  \begin{itemize}
  \item
    Mean\texttt{estimate1} (listened condition) = 
  \item
    Mean \texttt{estimate2} (read condition) = 
  \item
    t() = , p = 
  \end{itemize}
\item
  According to Cohen's (1988) guidelines, the effect sizes for all three tests are Small Medium Large
\end{itemize}

\hypertarget{activity-9-write-up}{%
\section{Activity 9: Write-up}\label{activity-9-write-up}}

Copy and paste the below \textbf{exactly} into \textbf{white space} in your R Markdown document and then knit the file to replicate the results section in the paper (p.887).

\begin{itemize}
\tightlist
\item
  Note that we haven't replicated the analysis exactly - the authors of this paper conducted Student's t-test whilst we have conducted Welch tests and we've also applied a multiple comparison correction. Look back at the paper and see what differences this makes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The pattern of evaluations by professional recruiters replicated the pattern observed }\ControlFlowTok{in}\NormalTok{ Experiments }\DecValTok{1}\NormalTok{ through 3b (see Fig. }\DecValTok{7}\NormalTok{). Bonferroni}\OperatorTok{-}\NormalTok{corrected t}\OperatorTok{-}\NormalTok{tests found that }\ControlFlowTok{in}\NormalTok{ particular, the recruiters believed that the job candidates had greater intellect}\OperatorTok{---}\NormalTok{were more competent, thoughtful, and intelligent}\OperatorTok{---}\NormalTok{when they listened to }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_intellect$estimate1%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[3], 2)}\StringTok{`}\NormalTok{) than when they read }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_intellect$estimate1%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[6], 2)}\StringTok{`}\NormalTok{), }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r round(results_intellect$parameter, 2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r round(results$statistic,2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ `}\DataTypeTok{r results$p.adjusted[3] %>% round(3)}\StringTok{`}\NormalTok{, }\DecValTok{95}\NormalTok{% CI of the difference =}\StringTok{ }\NormalTok{[}\StringTok{`}\DataTypeTok{r round(results_intellect$conf.low, 2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r round(results_intellect$conf.high, 2)}\StringTok{`}\NormalTok{], d =}\StringTok{ `}\DataTypeTok{r round(intellect_d,2)}\StringTok{`}\NormalTok{. }

\NormalTok{The recruiters also formed more positive impressions of the candidates}\OperatorTok{---}\NormalTok{rated them as more likeable and had a more positive and less negative impression of them}\OperatorTok{---}\NormalTok{when they listened to }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_impression$estimate1%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[2], 2)}\StringTok{`}\NormalTok{) than when they read }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_impression$estimate2%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[5], 2)}\StringTok{`}\NormalTok{, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r round(results_impression$parameter,2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r round(results_impression$statistic,2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ `}\DataTypeTok{r results$p.adjusted[2] %>% round(3)}\StringTok{`}\NormalTok{, }\DecValTok{95}\NormalTok{% CI of the }\DataTypeTok{difference =}\NormalTok{ [}\StringTok{`}\DataTypeTok{r round(results_impression$conf.low, 2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r round(results_impression$conf.high, 2)}\StringTok{`}\NormalTok{], }\DataTypeTok{d =} \StringTok{`}\DataTypeTok{r round(impression_d, 2)}\StringTok{`}\NormalTok{. }

\NormalTok{Finally, they also reported being more likely to hire the candidates when they listened to }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_hire$estimate1 %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[1], 2)}\StringTok{`}\NormalTok{) than when they read the same }\KeywordTok{pitches}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r results_hire$estimate2 %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r round(group_means$sd[4],2)}\StringTok{`}\NormalTok{), }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r round(results_hire$parameter,2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r round(results_hire$statistic,2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ `}\DataTypeTok{r results$p.adjusted[1] %>% round(3)}\StringTok{`}\NormalTok{, }\DecValTok{95}\NormalTok{% CI of the }\DataTypeTok{difference =}\NormalTok{ [}\StringTok{`}\DataTypeTok{r round(results_hire$conf.low, 2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r round(results_hire$conf.high, 2)}\StringTok{`}\NormalTok{], }\DataTypeTok{d =} \StringTok{`}\DataTypeTok{r round(hire_d,2)}\StringTok{`}\NormalTok{.}
\end{Highlighting}
\end{Shaded}

\begin{quote}
The pattern of evaluations by professional recruiters replicated the pattern observed in Experiments 1 through 3b (see Fig. 7). Bonferroni-corrected t-tests found that in particular, the recruiters believed that the job candidates had greater intellect---were more competent, thoughtful, and intelligent---when they listened to pitches (M = 5.63, SD = 1.61) than when they read pitches (M = 5.63, SD = 1.91), t(33.43) = 2.64, 2.82, 3.48, p \textless{} 0.004, 95\% CI of the difference = {[}0.83, 3.15{]}, d = 1.12.
\end{quote}

\begin{quote}
The recruiters also formed more positive impressions of the candidates---rated them as more likeable and had a more positive and less negative impression of them---when they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23, t(33.8) = 2.82, p \textless{} 0.024, 95\% CI of the difference = {[}0.53, 3.26{]}, d = 0.91.
\end{quote}

\begin{quote}
Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.05), t(36.86) = 2.64, p \textless{} 0.036, 95\% CI of the difference = {[}0.42, 3.23{]}, d = 0.84.
\end{quote}

\hypertarget{activity-10-paired-samples-t-test}{%
\section{Activity 10: Paired-samples t-test}\label{activity-10-paired-samples-t-test}}

For the final activity we will run a paired-samples t-test for a within-subject design but we will do this much quicker than for the Welch test and just point out the differences in the code.

For this example we will again draw from the \href{https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities}{Open Stats Lab} and look at data from the following paper:

\href{https://journals.sagepub.com/stoken/default+domain/d5HcBHg85XamSXGdYqYN/full}{Mehr, S. A., Song. L. A., \& Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27, 486-501.}

Parents often sing to their children and, even as infants, children listen to and look at their parents while they are singing. Research by Mehr, Song, and Spelke (2016) sought to explore the psychological function that music has for parents and infants, by examining the hypothesis that particular melodies convey
important social information to infants. Specifically, melodies convey information about social affiliation.

The authors argue that melodies are shared within social groups. Whereas children growing up in one culture may be exposed to certain songs as infants (e.g., ``Rock-a-bye Baby''), children growing up in other cultures (or even other groups within a culture) may be exposed to different songs. Thus, when a novel person (someone who the infant has never seen before) sings a familiar song, it may signal to the infant that this new person is a member of their social group.

To test this hypothesis, the researchers recruited 32 infants and their parents to complete an experiment. During their first visit to the lab, the parents were taught a new lullaby (one that neither they nor their infants had heard before). The experimenters asked the parents to sing the new lullaby to their child every day for the next 1-2 weeks. Following this 1-2 week exposure period, the parents and their infant returned to the lab to complete the experimental portion of the study. Infants were first shown a screen with side-by-side videos of two unfamiliar people, each of whom were silently smiling and looking at the infant. The researchers recorded the looking behaviour (or gaze) of the infants during this `baseline' phase. Next, one by one, the two unfamiliar people on the screen sang either the lullaby that the parents learned or a different lullaby (that had the same lyrics and rhythm, but a different melody). Finally, the infants saw the same silent video used at baseline, and the researchers again recorded the looking behaviour of the infants during this `test' phase. For more details on the experiment's methods, please refer to Mehr et al. (2016) Experiment 1.

\begin{itemize}
\tightlist
\item
  First, download Mehr Song and Spelke 2016 Experiment 1.csv and run the below code to load and wrangle the data into the format we need - this code selects only the data we need for the analysis and renames variables to make them easier to work with.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaze <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Mehr Song and Spelke 2016 Experiment 1.csv"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(exp1 }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(id, Baseline_Proportion_Gaze_to_Singer,Test_Proportion_Gaze_to_Singer) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{baseline =}\NormalTok{ Baseline_Proportion_Gaze_to_Singer,}
         \DataTypeTok{test =}\NormalTok{ Test_Proportion_Gaze_to_Singer)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-12-assumptions}{%
\section{Activity 12: Assumptions}\label{activity-12-assumptions}}

The assumptions for the paired-samples t-test are a little different (although very similar) to the independent t-test.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The dependent variable must be continuous (interval/ratio).\\
\item
  All participants should appear in both conditions/groups.
\item
  The difference scores should be normally distributed.
\end{enumerate}

Aside from the data being paired rather than independent, the key difference is that for the paired-samples test, the assumption of normality if that the differences between each pair of scores are normally distributed, rather than the scores themselves.

\begin{itemize}
\tightlist
\item
  Run the below code to calculate the difference scores and then conduct the Shapriro-Wilk and QQ-plot as with the independent test.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaze <-}\StringTok{ }\NormalTok{gaze }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{diff =}\NormalTok{ baseline }\OperatorTok{-}\StringTok{ }\NormalTok{test)}
\end{Highlighting}
\end{Shaded}

As you can see, from both the Shapiro-Wilk test and the QQ-plot, the data meet the assumption of normality so we can proceed.

\hypertarget{activity-13-descriptives-and-visualisations}{%
\section{Activity 13: Descriptives and visualisations}\label{activity-13-descriptives-and-visualisations}}

It made sense to keep the data in wide-form until this point to make it easy to calculate a column for the difference score, but now we will transform it to tidy data so that we can easily create descriptives and plot the data using \texttt{tidyverse} tools.

\begin{itemize}
\tightlist
\item
  Run the below code to tidy the data and calculate descriptives and then create the same violin-boxplot as you did for the independent t-test (hint: it is perfectly acceptable to copy and paste the code from above and change the data and variable names).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaze_tidy <-}\StringTok{ }\NormalTok{gaze }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ time, }\DataTypeTok{value =}\NormalTok{ looking, baseline, test) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{diff) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(time, id)}

\NormalTok{gaze_descriptives <-}\StringTok{ }\NormalTok{gaze_tidy }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(time) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_looking =} \KeywordTok{mean}\NormalTok{(looking, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{sd_looking =} \KeywordTok{sd}\NormalTok{(looking, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-14-paired-samples-t-test}{%
\section{Activity 14: Paired-samples t-test}\label{activity-14-paired-samples-t-test}}

Finally, we can calculate the t-test and the effect size. The code is almost identical to the independent code with two differences:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \texttt{t.test()} you should specify \texttt{paired\ =\ TRUE} rather than \texttt{FALSE}
\item
  In \texttt{cohensD()} you should specify \texttt{method\ =\ paired} rather than \texttt{unequal}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Run the t-test and calculate the effect size. Remember to use \texttt{tidy()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaze_test <-}\StringTok{ }
\NormalTok{gaze_d <-}\StringTok{ }
\end{Highlighting}
\end{Shaded}

\begin{warning}
When you run \texttt{cohensD} you will get a warning that tells you
``Results will be incorrect if cases do not appear in the same order for
both levels of the grouping factor''. What this means it that R has to
figure out which pairs of data belong together and it does this by
position. It will assume that the first data point in the baseline
condition will be the same participant as the first data point in the
test condition. The easiest way to ensure this is the case is to use
\texttt{arrange()} to sort your data. If you look back at the code we
used to tidy the data above you will see that we manually sorted the
data at the end. This will avoid any problems.
\end{warning}

The output of the paired-samples t-test is very similar to the independent test, with one exception. Rather than providing the means of both conditions, there is a single \texttt{estimate}. This is the mean difference score between the two conditions.

\begin{itemize}
\item
  Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places):

  \begin{itemize}
  \item
    Mean \texttt{estimate} = 
  \item
    t() = , p = 
  \end{itemize}
\end{itemize}

\hypertarget{activity-15-write-up}{%
\section{Activity 15: Write-up}\label{activity-15-write-up}}

Copy and paste the below \textbf{exactly} into \textbf{white space} in your R Markdown document and then knit the file to replicate the results section in the paper (p.489).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{At test, however, the infants selectively attended to the now}\OperatorTok{-}\NormalTok{silent singer of the song with the familiar melody; the proportion of time during which they looked toward her was...greater than the proportion at }\KeywordTok{baseline}\NormalTok{ (difference }\ControlFlowTok{in}\NormalTok{ proportion of looking}\OperatorTok{:}\StringTok{ }\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r gaze_test$estimate %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r sd(gaze$diff, na.rm = TRUE) %>% round(2)}\StringTok{`}\NormalTok{, }\DecValTok{95}\OperatorTok{% CI = [`r gaze_test$conf.low %}\ErrorTok{>}\OperatorTok{% round(2)`, `r gaze_test$conf.high %}\ErrorTok{>}\OperatorTok{% round(2)`]), t(`r gaze_test$parameter`) = `r gaze_test$statistic %}\ErrorTok{>}\OperatorTok{% round(2)`, p = `r gaze_test$p.value %}\ErrorTok{>}\OperatorTok{% round(3)`, d = `r gaze_d %}\ErrorTok{>}\NormalTok{% }\KeywordTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}\StringTok{`}\DataTypeTok{.}
\end{Highlighting}
\end{Shaded}

\begin{quote}
At test, however, the infants selectively attended to the now-silent singer of the song with the familiar melody; the proportion of time during which they looked toward her was\ldots{}greater than the proportion at baseline (difference in proportion of looking: M = -0.07, SD = 0.17, 95\% CI = {[}-0.13, -0.01{]}), t(31) = -2.42, p = 0.022, d = 0.43.
\end{quote}

\hypertarget{finished-6}{%
\subsection{Finished!}\label{finished-6}}

That was a long lab but now that you've done three types of statistical tests (chi-square, correlations, t-test) hopefully you will see that it really is true that the hardest part is the set-up and the data wrangling. As we've said before, you don't need to memorise lines of code - you just need to remember where to find examples and to understand which bits of them you need to change. Play around with the examples we have given you and see what changing the values does.

\hypertarget{activity-solutions-5}{%
\section{Activity solutions}\label{activity-solutions-5}}

\hypertarget{activity-1-3}{%
\subsection{Activity 1}\label{activity-1-3}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"lsr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\NormalTok{evaluators <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"evaluators.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-2-1}{%
\subsection{Activity 2}\label{activity-2-1}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evaluators <-}\StringTok{ }\NormalTok{evaluators }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex_labels =}\NormalTok{ dplyr}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(sex, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "male"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "female"}\NormalTok{),}
         \DataTypeTok{sex_labels =} \KeywordTok{as.factor}\NormalTok{(sex_labels),}
         \DataTypeTok{condition =} \KeywordTok{as.factor}\NormalTok{(condition))}
\KeywordTok{summary}\NormalTok{(evaluators)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-6-1}{%
\subsection{Activity 6}\label{activity-6-1}}

Activity 6

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intellect <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(ratings2, Category }\OperatorTok{==}\StringTok{ "intellect"}\NormalTok{)}
\NormalTok{hire <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(ratings2, Category }\OperatorTok{==}\StringTok{ "hire"}\NormalTok{)}
\NormalTok{impression <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(ratings2, Category }\OperatorTok{==}\StringTok{ "impression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-7-2}{%
\subsection{Activity 7}\label{activity-7-2}}

Activity 7

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intellect_d <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(Rating }\OperatorTok{~}\StringTok{ }\NormalTok{condition, }\DataTypeTok{method =} \StringTok{"unequal"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ intellect)}
\NormalTok{hire_d <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(Rating }\OperatorTok{~}\StringTok{ }\NormalTok{condition, }\DataTypeTok{method =} \StringTok{"unequal"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ hire)}
\NormalTok{impression_d <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(Rating }\OperatorTok{~}\StringTok{ }\NormalTok{condition, }\DataTypeTok{method =} \StringTok{"unequal"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ impression)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-12}{%
\subsection{Activity 12}\label{activity-12}}

Activity 12

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gaze}\OperatorTok{$}\NormalTok{diff)}

\KeywordTok{qqPlot}\NormalTok{(gaze}\OperatorTok{$}\NormalTok{diff)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-13}{%
\subsection{Activity 13}\label{activity-13}}

Activity 13

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create summary data to use with `geom_pointrange()`}
\NormalTok{summary_gaze<-gaze_tidy}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(time)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(looking),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(looking) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(looking)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }\CommentTok{#confidence intervals}
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(looking) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(looking)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()))}


\KeywordTok{ggplot}\NormalTok{(gaze_tidy, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ time, }\DataTypeTok{y =}\NormalTok{ looking)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ time), }\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_gaze,}
                  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ time, }\DataTypeTok{y =}\NormalTok{ mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{), }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{activity-14}{%
\subsection{Activity 14}\label{activity-14}}

Activity 14

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaze_test <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(looking }\OperatorTok{~}\StringTok{ }\NormalTok{time, }\DataTypeTok{paired =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gaze_tidy) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\NormalTok{gaze_d <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(looking }\OperatorTok{~}\StringTok{ }\NormalTok{time, }\DataTypeTok{method =} \StringTok{"paired"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gaze_tidy)}
\end{Highlighting}
\end{Shaded}

\textbf{click the tab to see the solution}

\hypertarget{vis}{%
\chapter{Visualisation}\label{vis}}

In this chapter we are going to focus on visualising data using \texttt{ggplot2}. You've already created a number of different plots including bar charts, scatterplots, histograms, qq-plots, and violin-boxplots, but now we will show you how to customise your plots further to give you a better idea of the range and flexibility of visualising data in R.

In this chapter, you won't be asked to write any code yourself, we will give you all the example code. Instead, play with the arguments, change TRUE to FALSE and vice-versa, change the values and colours. This will help you learn what each bit does.

For all of the activities in this chapter we are going to use data from Experiment 3 of \href{https://journals.sagepub.com/doi/abs/10.1177/0956797614542274}{Zhang, T., Kim, T., Brooks, A. W., Gino, F., \& Norton, M. I. (2014). A ``present'' for the future: The unexpected value of rediscovery. Psychological Science, 25, 1851-1860.}.

To help you understand the data we're visualising, here is the abstract:

\begin{quote}
Although documenting everyday activities may seem trivial, four studies reveal that creating records of the present generates unexpected benefits by allowing future rediscoveries. In Study 1, we used a time-capsule paradigm to show that individuals underestimate the extent to which rediscovering experiences from the past will be curiosity provoking and interesting in the future. In Studies 2 and 3, we found that people are particularly likely to underestimate the pleasure of rediscovering ordinary, mundane experiences, as opposed to extraordinary experiences. Finally, Study 4 demonstrates that underestimating the pleasure of rediscovery leads to time-inconsistent choices: Individuals forgo opportunities to document the present but then prefer rediscovering those moments in the future to engaging in an alternative fun activity. Underestimating the value of rediscovery is linked to people's erroneous faith in their memory of everyday events. By documenting the present, people provide themselves with the opportunity to rediscover mundane moments that may otherwise have been forgotten.
\end{quote}

\hypertarget{activity-1-set-up-8}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-8}}

Do the following. If you need help, consult Chapter \ref{ref3} and Chapter @(ref2).

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 8 folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 8''.\\
\item
  Download Zhang et al.~2014 Study 3.csv and save it in your Week 8 folder. Make sure that you do not change the file name at all.
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the package \texttt{tidyverse} using the \texttt{library()} function.
\item
  Run the below code to load and wrangle the data into tidy data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{zhang_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Zhang et al. 2014 Study 3.csv"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Gender, Age,Condition, T1_Predicted_Interest_Composite, T2_Actual_Interest_Composite)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{subject =} \KeywordTok{row_number}\NormalTok{())}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ time,}
         \DataTypeTok{value =}\NormalTok{ interest,}
\NormalTok{         T1_Predicted_Interest_Composite,T2_Actual_Interest_Composite)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Condition =} \KeywordTok{recode}\NormalTok{(Condition, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "Ordinary"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "Extraordinary"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{time =} \KeywordTok{recode}\NormalTok{(time, }\StringTok{"T1_Predicted_Interest_Composite"}\NormalTok{ =}\StringTok{ "time1_interest"}\NormalTok{, }\StringTok{"T2_Actual_Interest_Composite"}\NormalTok{ =}\StringTok{ "time2_interest"}\NormalTok{),}
         \DataTypeTok{Gender =} \KeywordTok{recode}\NormalTok{(Gender, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "male"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "female"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Gender }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-2-histograms}{%
\section{Activity 2: Histograms}\label{activity-2-histograms}}

First, let's create histograms for \texttt{interest} to check the distribution.
The first line of code creates the \texttt{ggplot()} object and specifies which dataset is being used, and what should be represented on the x and y-axis. Because this is a histogram, you only need to specify the variable on the x-axis because y is always frequency

\hypertarget{basic-histogram}{%
\subsection{Basic histogram}\label{basic-histogram}}

The code below will create a simple histogram with default appearance and no customisation. You wouldn't use this graph in a paper, but if you just want to quickly check your distributions, for e.g., normality, this code might be enough.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(interest))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/hist1-1} 

}

\caption{Basic histogram}\label{fig:hist1}
\end{figure}

\hypertarget{colour-and-fill}{%
\subsection{Colour and fill}\label{colour-and-fill}}

The next section of code will change the appearance. Plots in ggplot2 are highly customisable - \href{https://r4ds.had.co.nz/data-visualisation.html}{R for Data Science} has an excellent chapter on \texttt{ggplot} if you would like additional information.

Adding \texttt{binwidth} to \texttt{geom\_histogram()} changes the bins of the histogram, i.e., how wide the bars are. The default is 30. Sometimes this may be appropriate but often you will want to change the binwidth. What value you give will depend upon your data.

\texttt{colour()} changes the colour of the line around the bars. \texttt{fill()} changes the fill of the bars.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{.3}\NormalTok{, }
                 \DataTypeTok{colour =} \StringTok{"black"}\NormalTok{,  }
                 \DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/hist2-1} 

}

\caption{Histogram with colour changes}\label{fig:hist2}
\end{figure}

\hypertarget{axis-labels}{%
\subsection{Axis labels}\label{axis-labels}}

The next section of code changes the labels on the graphs. Note that the labels are an additional layer (i.e., it comes after an \texttt{+}, rather than being an argument to \texttt{geom\_histogram()}).

The function you use will depend on your data, the most common are \texttt{scale\_x/y\_continuous} and \texttt{scale\_x/y\_discrete} depending on whether you are displaying continuous or categorical data. Again, each axis is a separate layer.

These scale functions control all the information about the axis, from the label to the breaks, to the minimum and maximum values. For more information use the help documentation.

For our labelling purposes, there are two main arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{name()} controls the main name of the axis
\item
  \texttt{labels()} controls the name of the breaks
\end{enumerate}

For our histogram we will just change the main axis labels.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{.3}\NormalTok{, }
                 \DataTypeTok{colour =} \StringTok{"black"}\NormalTok{,  }
                 \DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Count"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/hist3-1} 

}

\caption{Histogram with label changes}\label{fig:hist3}
\end{figure}

\hypertarget{density-curve}{%
\subsection{Density curve}\label{density-curve}}

The following section adds a normal density curve to the histogram, which can be useful for checking the assumption of normality.

To add the line you must change the \texttt{geom\_histogram()} to use density on the y-axis (the default is count) and add a \texttt{stat\_function()} layer that draws the line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(interest))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{.3}\NormalTok{, }
                 \DataTypeTok{colour =} \StringTok{"black"}\NormalTok{, }
                 \DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{,}
                 \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..))}\OperatorTok{+}\StringTok{ }\CommentTok{# change y-axis to density}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\CommentTok{# this adds a normal density function curve}
                \DataTypeTok{colour =} \StringTok{"red"}\NormalTok{, }\CommentTok{# this makes it red}
                \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(zhang_data}\OperatorTok{$}\NormalTok{interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
                           \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(zhang_data}\OperatorTok{$}\NormalTok{interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/hist4-1} 

}

\caption{Histogram with normal density curve}\label{fig:hist4}
\end{figure}

\hypertarget{activity-3-scatterplots}{%
\section{Activity 3: Scatterplots}\label{activity-3-scatterplots}}

\hypertarget{basic-scatterplot}{%
\subsection{Basic scatterplot}\label{basic-scatterplot}}

Now let's make a scatterplot plotting \texttt{Age} and \texttt{interest} to see if there is any relationship between the two. We need to specify both the x and y-axis variables. The following code will produce a very simple scatterplot. Again, you wouldn't use this graph in a paper, but for eye-balling your data it would suffice.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest,}\DataTypeTok{y =}\NormalTok{ Age))}\OperatorTok{+}
\StringTok{       }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/scat1-1} 

}

\caption{Basic scatterplot}\label{fig:scat1}
\end{figure}

\hypertarget{axis-labels-1}{%
\subsection{Axis labels}\label{axis-labels-1}}

From this plot it doesn't look like there is much of a relationship between age and interest ratings. We can now change the labels using the same scale functions as before.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest,}\DataTypeTok{y =}\NormalTok{ Age))}\OperatorTok{+}
\StringTok{       }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Age"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/scat2-1} 

}

\caption{Scatterplot with label changes}\label{fig:scat2}
\end{figure}

\hypertarget{adding-a-regression-line}{%
\subsection{Adding a regression line}\label{adding-a-regression-line}}

It's often useful to add a regression line or line of best fit to a scatterplot. The regression line is added with \texttt{geom\_smooth()} and by default will also provide a 95\% confidence interval. You can specify what type of line you want to draw, most often you will need \texttt{method\ =\ lm}, i.e., a linear model or a straight line. Look up the help documentation for \texttt{geom\_smooth()} and see what other methods you can use.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest,}\DataTypeTok{y =}\NormalTok{ Age))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Age"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\NormalTok{lm) }\CommentTok{# if you don't want the shaded CI, add se = FALSE to this}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/scat3-1} 

}

\caption{Scatterplot with regression line}\label{fig:scat3}
\end{figure}

\hypertarget{grouped-scatterplots}{%
\subsection{Grouped scatterplots}\label{grouped-scatterplots}}

We can use ggplot to show how the relationship might differ for different populations within our data. We do this by adding \texttt{colour()} to \texttt{aes()} and setting it as whatever variable we would like to distinguish between. In this case, we will see how the relationship between age and interest differs for the male and female participants. There are a few participants with missing gender so we will first filter them out.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zhang_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Gender }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{           }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest,}\DataTypeTok{y =}\NormalTok{ Age, }\DataTypeTok{colour =}\NormalTok{ Gender))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Age"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\NormalTok{lm)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/scat4-1} 

}

\caption{Grouped scatterplot}\label{fig:scat4}
\end{figure}

And here's that plot with the labels tidied up. Notice the use of \texttt{scale\_color\_discrete()} to adjust the labels for Gender.

\begin{warning}
When you change the \texttt{labels}, R will simply overwrite the names
in the dataset. If you wanted to actually change the order of the
categories (e.g., have male as the red line) you need to change the
order of the factor. We will do this later, for now, just be sure that
you're changing the name of the right category (i.e., female comes
first))
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interest,}\DataTypeTok{y =}\NormalTok{ Age, }\DataTypeTok{colour =}\NormalTok{ Gender))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest score (1-7)"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Age"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\NormalTok{lm)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Gender"}\NormalTok{,}
                       \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/scat5-1} 

}

\caption{Grouped scatterplot with adjusted labels}\label{fig:scat5}
\end{figure}

\hypertarget{activity-4-boxplots}{%
\section{Activity 4: Boxplots}\label{activity-4-boxplots}}

\hypertarget{basic-boxplot}{%
\subsection{Basic boxplot}\label{basic-boxplot}}

The following code will produce a simple boxplot for eye-balling your data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp1-1} 

}

\caption{Basic boxplot}\label{fig:bp1}
\end{figure}

\hypertarget{adding-data-points}{%
\subsection{Adding data points}\label{adding-data-points}}

If we add another layer \texttt{geom\_point()} we can add our raw data points to our boxplots to make them more informative.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp2-1} 

}

\caption{Boxplot with overplotting}\label{fig:bp2}
\end{figure}

However, this plot suffers from \textbf{over-plotting}, that is, there are multiple data points on top of each other. We can change this by using \texttt{geom\_jitter()}, which adds a layer of points that are jittered so that each one is visible.

\texttt{height} and \texttt{width} affect how much each point is jittered. Play around with the values to see how it affects the data points.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{height =} \DecValTok{0}\NormalTok{, }\DataTypeTok{width =} \FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp3-1} 

}

\caption{Boxplot with jittered data}\label{fig:bp3}
\end{figure}

\hypertarget{adding-colour}{%
\subsection{Adding colour}\label{adding-colour}}

We may want to add colour to our graph (and for consistency, we'll sort out the labels). We do this by adding the `fill' argument to the ggplot aesthetic by specifying which variable the colour of the fill should be organised by.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{height =} \DecValTok{0}\NormalTok{, }\DataTypeTok{width =} \FloatTok{.1}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Condition"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# note the x-axis is discrete}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest rating (1-7)"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{guide =} \OtherTok{FALSE}\NormalTok{) }\CommentTok{# this suppresses the legend because we don't need it}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp4-1} 

}

\caption{Boxplot with colour}\label{fig:bp4}
\end{figure}

\hypertarget{boxplots-for-multiple-factors}{%
\subsection{Boxplots for multiple factors}\label{boxplots-for-multiple-factors}}

When you only have one IV, using the fill command to change the colour is a little redundant, as the colours don't add any additional information. It makes more sense to use colour to represent an additional IV.

For this example, we'll use \texttt{Condition} and \texttt{time} as IVs. \texttt{fill()} now specifies a second IV, rather than repeating the IV on the x-axis as in the previous plot.

With multiple IVs the command to overlay the raw data points changes as the data points also need dodged (try running the code with the previous geom\_jitter function to see what happens)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ time))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width =} \FloatTok{.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp5-1} 

}

\caption{Boxplot for two factors}\label{fig:bp5}
\end{figure}

\hypertarget{colour-blind-friendly-options}{%
\subsection{Colour-blind friendly options}\label{colour-blind-friendly-options}}

There is one more fill option that we can use. Rather than specifying \texttt{scale\_fill\_discrete()}, we can use \texttt{scale\_fill\_viridis\_d()}. This function does exactly the same thing but it uses a colour-blind friendly palette (which also prints in black and white). There are 5 different options for colours and you can see them by changing \texttt{option} to A, B, C, D or E. Personally I like option E with \texttt{alpha\ =\ .6} (to control transparency) but that's not an official School position.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ time))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width =} \FloatTok{.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/unnamed-chunk-3-1} 

}

\caption{Boxplots with friendly colours}\label{fig:unnamed-chunk-3}
\end{figure}

\hypertarget{activity-5-reordering-factors}{%
\section{Activity 5: Reordering factors}\label{activity-5-reordering-factors}}

R orders categorical variables alphabetically. For gender it didn't really matter whether male or female was represented first and for time 1 and 2 it makes sense for them to be in this order but we may want to change the order of Condition (in my mind it makes more sense for Ordinary to come first, but that may just be me).

To do this we can use \texttt{mutate()} and \texttt{fct\_level()} to change the factor levels to the order we want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zhang_data <-}\StringTok{ }\NormalTok{zhang_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Condition =} \KeywordTok{fct_relevel}\NormalTok{(Condition, }\KeywordTok{c}\NormalTok{(}\StringTok{"Ordinary"}\NormalTok{, }\StringTok{"Extraordinary"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Now we can re-run the boxplot. That's better.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ time))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width =} \FloatTok{.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bp6-1} 

}

\caption{Boxplot with reordered factors}\label{fig:bp6}
\end{figure}

\hypertarget{activity-6-bar-charts}{%
\section{Activity 6: Bar Charts}\label{activity-6-bar-charts}}

\hypertarget{basic-bar-chart}{%
\subsection{Basic bar chart}\label{basic-bar-chart}}

Bar charts should only be used for counts because they can distort your understanding of the data if you use them to represent means (see \href{https://www.autodeskresearch.com/publications/samestats}{here for a great example}.

First, we'll do a bar chart for the count of male and females in our sample.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Gender))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bc1-1} 

}

\caption{Basic bar chart}\label{fig:bc1}
\end{figure}

\hypertarget{bar-charts-with-two-factors}{%
\subsection{Bar charts with two factors}\label{bar-charts-with-two-factors}}

We can also use \texttt{fill()} to separate gender by Condition

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Gender, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# the position argument places the bars next to each other, rather than on top of each other, try removing this}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/bc2-1} 

}

\caption{Bar chart with two factors}\label{fig:bc2}
\end{figure}

\hypertarget{activity-7-violin-plots}{%
\section{Activity 7: Violin plots}\label{activity-7-violin-plots}}

\hypertarget{basic-violin-plot}{%
\subsection{Basic violin plot}\label{basic-violin-plot}}

Violin plots are so-called because with a normal distribution the shape would look something like a violin. They show density, i.e., the fatter the violin the more data points there are for that value.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/vp1-1} 

}

\caption{Basic violin plot}\label{fig:vp1}
\end{figure}

\hypertarget{violin-plots-with-raw-data-points}{%
\subsection{Violin plots with raw data points}\label{violin-plots-with-raw-data-points}}

Like the boxplot, we can also add the raw data points to our violin plot, making sure to use jitter to avoid over-plotting.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{height =} \DecValTok{0}\NormalTok{, }\DataTypeTok{width =} \FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/vp2-1} 

}

\caption{Violin plot with data points}\label{fig:vp2}
\end{figure}

\begin{warning}
It's important to remember that R is very literal. \texttt{ggplot2}
works on a system of layers. It will add new geoms on top of existing
ones and it won't stop to think whether this is a good idea. Try running
the above code but put \texttt{geom\_jitter()} first and then add
\texttt{geom\_violin()}. The order of your layers matters.
\end{warning}

\hypertarget{viobox}{%
\section{Activity 8: Violin-boxplots}\label{viobox}}

One increasingly common graph is a violin + boxplot + summary plot that shows a huge amount of information about your data in a single plot. You've already come across these when you looked at t-tests.

This plot first requires you to calculate summary data for the variables that you want to plot. We need to calculate the mean of our DV, standard error, and SD and we need to do this grouped by the IV.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_data<-zhang_data}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Condition)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{-}\StringTok{ }\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }\CommentTok{# standard error}
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{+}\StringTok{ }\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(interest)}
\NormalTok{            )}
\end{Highlighting}
\end{Shaded}

Once you've done this you can then create the plot. By now you should have a good understand of what each of the layers are doing. Change them to make the plot look how you want.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.7}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_data,}
                  \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{, }\DataTypeTok{label =} \KeywordTok{c}\NormalTok{(}\StringTok{"Ordinary"}\NormalTok{, }\StringTok{"Extraordinary"}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean interest rating (1-7)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/vbp1-1} 

}

\caption{Violin-boxplot with summary data}\label{fig:vbp1}
\end{figure}

\hypertarget{activity-9-faceting}{%
\section{Activity 9: Faceting}\label{activity-9-faceting}}

\texttt{ggplot2} contains a facet function that produces different plots for each level of a grouping variable which can be very useful when you have more than two factors, for example, for a three-way ANOVA. The following code displays produces violin-boxplots for Condition \textasciitilde{} interest, but separately for male and female participants.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ time))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(.}\DecValTok{9}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{Gender)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/facet1-1} 

}

\caption{Violin-boxplot facetted by gender}\label{fig:facet1}
\end{figure}

If you want to add \texttt{geom\_pointrange()} to this then you would need to calculate a new \texttt{summary\_data} that also grouped by Gender (because it requires the mean, se, and SD for all four combinations, rather than just two).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_data2<-zhang_data}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Condition, Gender, time)}\OperatorTok{%>%}\StringTok{ }\CommentTok{# add Gender to group_by()}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{-}\StringTok{ }\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{+}\StringTok{ }\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(interest)}
\NormalTok{            )}

\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ time))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(.}\DecValTok{9}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{Gender)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_data2,}
                  \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/sumdat2-1} 

}

\caption{Violin-boxplot facetted by gender with summary data}\label{fig:sumdat2}
\end{figure}

\hypertarget{facet-labelling}{%
\subsection{Facet labelling}\label{facet-labelling}}

Finally, changing the labels within the facets is a little more complicated - there's no additional scale layer, instead, you adjust this inside \texttt{facet\_wrap()} using \texttt{labeller}. This has always felt unintuitive to me and I have to look it up every single time so don't worry if it is confusing - just remember where to look for the example.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{Gender, }\DataTypeTok{labeller =} \KeywordTok{labeller}\NormalTok{(}\DataTypeTok{Gender =}\NormalTok{ (}\KeywordTok{c}\NormalTok{(}\DataTypeTok{female =} \StringTok{"Female"}\NormalTok{, }\DataTypeTok{male =} \StringTok{"Male"}\NormalTok{))))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_data,}
                  \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/facet2-1} 

}

\caption{Facetted plot with updated labels}\label{fig:facet2}
\end{figure}

\hypertarget{activity-10-split-violins-and-raincloud-plots}{%
\section{Activity 10: Split-violins and raincloud plots}\label{activity-10-split-violins-and-raincloud-plots}}

Finally, we're going to do something a bit snazzy. As well as the functions that are included in packages, anyone can also write custom functions and share the code. One such custom function allows us to create \textbf{raincloud plots} which are highly informative and very pretty. See \href{https://wellcomeopenresearch.org/articles/4-63}{here} for more information about their creation and function.

\hypertarget{split-violin-plots}{%
\subsection{Split-violin plots}\label{split-violin-plots}}

Because the functions we need don't exist in a package we can load, we need to create them. Copy and paste all of the below code without changing anything. You do not need to understand this code. I certainly don't. When you run this, you should see \texttt{geom\_split\_violin} appear in the Environment pane under Functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GeomSplitViolin <-}\StringTok{ }\KeywordTok{ggproto}\NormalTok{(}
  \StringTok{"GeomSplitViolin"}\NormalTok{, }
\NormalTok{  GeomViolin, }
  \DataTypeTok{draw_group =} \ControlFlowTok{function}\NormalTok{(self, data, ..., }\DataTypeTok{draw_quantiles =} \OtherTok{NULL}\NormalTok{) \{}
\NormalTok{    data <-}\StringTok{ }\KeywordTok{transform}\NormalTok{(data, }
                      \DataTypeTok{xminv =}\NormalTok{ x }\OperatorTok{-}\StringTok{ }\NormalTok{violinwidth }\OperatorTok{*}\StringTok{ }\NormalTok{(x }\OperatorTok{-}\StringTok{ }\NormalTok{xmin), }
                      \DataTypeTok{xmaxv =}\NormalTok{ x }\OperatorTok{+}\StringTok{ }\NormalTok{violinwidth }\OperatorTok{*}\StringTok{ }\NormalTok{(xmax }\OperatorTok{-}\StringTok{ }\NormalTok{x))}
\NormalTok{    grp <-}\StringTok{ }\NormalTok{data[}\DecValTok{1}\NormalTok{,}\StringTok{'group'}\NormalTok{]}
\NormalTok{    newdata <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{arrange}\NormalTok{(}
      \KeywordTok{transform}\NormalTok{(data, }\DataTypeTok{x =} \ControlFlowTok{if}\NormalTok{(grp}\OperatorTok{%%}\DecValTok{2}\OperatorTok{==}\DecValTok{1}\NormalTok{) xminv }\ControlFlowTok{else}\NormalTok{ xmaxv), }
      \ControlFlowTok{if}\NormalTok{(grp}\OperatorTok{%%}\DecValTok{2}\OperatorTok{==}\DecValTok{1}\NormalTok{) y }\ControlFlowTok{else} \OperatorTok{-}\NormalTok{y}
\NormalTok{    )}
\NormalTok{    newdata <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(newdata[}\DecValTok{1}\NormalTok{, ], newdata, newdata[}\KeywordTok{nrow}\NormalTok{(newdata), ], newdata[}\DecValTok{1}\NormalTok{, ])}
\NormalTok{    newdata[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(newdata)}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(newdata)), }\StringTok{'x'}\NormalTok{] <-}\StringTok{ }\KeywordTok{round}\NormalTok{(newdata[}\DecValTok{1}\NormalTok{, }\StringTok{'x'}\NormalTok{]) }
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(draw_quantiles) }\OperatorTok{>}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\OperatorTok{!}\NormalTok{scales}\OperatorTok{::}\KeywordTok{zero_range}\NormalTok{(}\KeywordTok{range}\NormalTok{(data}\OperatorTok{$}\NormalTok{y))) \{}
      \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{all}\NormalTok{(draw_quantiles }\OperatorTok{>=}\StringTok{ }\DecValTok{0}\NormalTok{), }\KeywordTok{all}\NormalTok{(draw_quantiles }\OperatorTok{<=}\StringTok{ }\DecValTok{1}\NormalTok{))}
\NormalTok{      quantiles <-}\StringTok{ }\NormalTok{ggplot2}\OperatorTok{:::}\KeywordTok{create_quantile_segment_frame}\NormalTok{(data, draw_quantiles)}
\NormalTok{      aesthetics <-}\StringTok{ }\NormalTok{data[}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(quantiles)), }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(data), }\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{)), drop =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\NormalTok{      aesthetics}\OperatorTok{$}\NormalTok{alpha <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(quantiles))}
\NormalTok{      both <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(quantiles, aesthetics)}
\NormalTok{      quantile_grob <-}\StringTok{ }\NormalTok{GeomPath}\OperatorTok{$}\KeywordTok{draw_panel}\NormalTok{(both, ...)}
\NormalTok{      ggplot2}\OperatorTok{:::}\KeywordTok{ggname}\NormalTok{(}\StringTok{"geom_split_violin"}\NormalTok{, }
\NormalTok{                       grid}\OperatorTok{::}\KeywordTok{grobTree}\NormalTok{(GeomPolygon}\OperatorTok{$}\KeywordTok{draw_panel}\NormalTok{(newdata, ...), quantile_grob))}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      ggplot2}\OperatorTok{:::}\KeywordTok{ggname}\NormalTok{(}\StringTok{"geom_split_violin"}\NormalTok{, GeomPolygon}\OperatorTok{$}\KeywordTok{draw_panel}\NormalTok{(newdata, ...))}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{)}

\NormalTok{geom_split_violin <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{ (}\DataTypeTok{mapping =} \OtherTok{NULL}\NormalTok{, }
                               \DataTypeTok{data =} \OtherTok{NULL}\NormalTok{, }
                               \DataTypeTok{stat =} \StringTok{"ydensity"}\NormalTok{, }
                               \DataTypeTok{position =} \StringTok{"identity"}\NormalTok{, ..., }
                               \DataTypeTok{draw_quantiles =} \OtherTok{NULL}\NormalTok{, }
                               \DataTypeTok{trim =} \OtherTok{TRUE}\NormalTok{, }
                               \DataTypeTok{scale =} \StringTok{"area"}\NormalTok{, }
                               \DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{, }
                               \DataTypeTok{show.legend =} \OtherTok{NA}\NormalTok{, }
                               \DataTypeTok{inherit.aes =} \OtherTok{TRUE}\NormalTok{) \{}
  \KeywordTok{layer}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data, }
        \DataTypeTok{mapping =}\NormalTok{ mapping, }
        \DataTypeTok{stat =}\NormalTok{ stat, }
        \DataTypeTok{geom =}\NormalTok{ GeomSplitViolin, }
        \DataTypeTok{position =}\NormalTok{ position, }
        \DataTypeTok{show.legend =}\NormalTok{ show.legend, }
        \DataTypeTok{inherit.aes =}\NormalTok{ inherit.aes, }
        \DataTypeTok{params =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{trim =}\NormalTok{ trim, }
                      \DataTypeTok{scale =}\NormalTok{ scale, }
                      \DataTypeTok{draw_quantiles =}\NormalTok{ draw_quantiles, }
                      \DataTypeTok{na.rm =}\NormalTok{ na.rm, ...)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The split-violin is a version of the violin-boxplot that is good for visualising interactions. If you look at the faceted graph we made, there's actually quite a lot of unnecessary space used up because we only need half of the violin to see the distribution - the other half is just repeating the same information.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Gender))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_split_violin}\NormalTok{(}\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.5}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(.}\DecValTok{25}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ summary_data2,}
                  \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.25}\NormalTok{),}
                  \DataTypeTok{colour =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/sv1-1} 

}

\caption{Split-violin plot}\label{fig:sv1}
\end{figure}

\hypertarget{raincloud-plots}{%
\subsection{Raincloud plots}\label{raincloud-plots}}

The second custom function is \texttt{geom\_flat\_violin}. Copy and paste all of this code and again you should see it appear in your Environment pane.

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"%||%"}\NormalTok{ <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, b) \{}
  \ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{is.null}\NormalTok{(a)) a }\ControlFlowTok{else}\NormalTok{ b}
\NormalTok{\}}

\NormalTok{geom_flat_violin <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{mapping =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{data =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{stat =} \StringTok{"ydensity"}\NormalTok{,}
                             \DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{, }\DataTypeTok{trim =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{scale =} \StringTok{"area"}\NormalTok{,}
                             \DataTypeTok{show.legend =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{inherit.aes =} \OtherTok{TRUE}\NormalTok{, ...) \{}
  \KeywordTok{layer}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ data,}
    \DataTypeTok{mapping =}\NormalTok{ mapping,}
    \DataTypeTok{stat =}\NormalTok{ stat,}
    \DataTypeTok{geom =}\NormalTok{ GeomFlatViolin,}
    \DataTypeTok{position =}\NormalTok{ position,}
    \DataTypeTok{show.legend =}\NormalTok{ show.legend,}
    \DataTypeTok{inherit.aes =}\NormalTok{ inherit.aes,}
    \DataTypeTok{params =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{trim =}\NormalTok{ trim,}
      \DataTypeTok{scale =}\NormalTok{ scale,}
\NormalTok{      ...}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{\}}

\NormalTok{GeomFlatViolin <-}
\StringTok{  }\KeywordTok{ggproto}\NormalTok{(}\StringTok{"Violinist"}\NormalTok{, Geom,}
          \DataTypeTok{setup_data =} \ControlFlowTok{function}\NormalTok{(data, params) \{}
\NormalTok{            data}\OperatorTok{$}\NormalTok{width <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{width }\OperatorTok{%||%}
\StringTok{              }\NormalTok{params}\OperatorTok{$}\NormalTok{width }\OperatorTok{%||%}\StringTok{ }\NormalTok{(}\KeywordTok{resolution}\NormalTok{(data}\OperatorTok{$}\NormalTok{x, }\OtherTok{FALSE}\NormalTok{) }\OperatorTok{*}\StringTok{ }\FloatTok{0.9}\NormalTok{)}
            
            \CommentTok{# ymin, ymax, xmin, and xmax define the bounding rectangle for each group}
\NormalTok{            data }\OperatorTok{%>%}
\StringTok{              }\KeywordTok{group_by}\NormalTok{(group) }\OperatorTok{%>%}
\StringTok{              }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ymin =} \KeywordTok{min}\NormalTok{(y),}
                     \DataTypeTok{ymax =} \KeywordTok{max}\NormalTok{(y),}
                     \DataTypeTok{xmin =}\NormalTok{ x,}
                     \DataTypeTok{xmax =}\NormalTok{ x }\OperatorTok{+}\StringTok{ }\NormalTok{width }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
            
\NormalTok{          \},}
          
          \DataTypeTok{draw_group =} \ControlFlowTok{function}\NormalTok{(data, panel_scales, coord) \{}
            \CommentTok{# Find the points for the line to go all the way around}
\NormalTok{            data <-}\StringTok{ }\KeywordTok{transform}\NormalTok{(data, }\DataTypeTok{xminv =}\NormalTok{ x,}
                              \DataTypeTok{xmaxv =}\NormalTok{ x }\OperatorTok{+}\StringTok{ }\NormalTok{violinwidth }\OperatorTok{*}\StringTok{ }\NormalTok{(xmax }\OperatorTok{-}\StringTok{ }\NormalTok{x))}
            
            \CommentTok{# Make sure it's sorted properly to draw the outline}
\NormalTok{            newdata <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(plyr}\OperatorTok{::}\KeywordTok{arrange}\NormalTok{(}\KeywordTok{transform}\NormalTok{(data, }\DataTypeTok{x =}\NormalTok{ xminv), y),}
\NormalTok{                             plyr}\OperatorTok{::}\KeywordTok{arrange}\NormalTok{(}\KeywordTok{transform}\NormalTok{(data, }\DataTypeTok{x =}\NormalTok{ xmaxv), }\OperatorTok{-}\NormalTok{y))}
            
            \CommentTok{# Close the polygon: set first and last point the same}
            \CommentTok{# Needed for coord_polar and such}
\NormalTok{            newdata <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(newdata, newdata[}\DecValTok{1}\NormalTok{,])}
            
\NormalTok{            ggplot2}\OperatorTok{:::}\KeywordTok{ggname}\NormalTok{(}\StringTok{"geom_flat_violin"}\NormalTok{, GeomPolygon}\OperatorTok{$}\KeywordTok{draw_panel}\NormalTok{(newdata, panel_scales, coord))}
\NormalTok{          \},}
          
          \DataTypeTok{draw_key =}\NormalTok{ draw_key_polygon,}
          
          \DataTypeTok{default_aes =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{weight =} \DecValTok{1}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"grey20"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{,}
                            \DataTypeTok{alpha =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"solid"}\NormalTok{),}
          
          \DataTypeTok{required_aes =} \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

This plot is similar to the split-violin, but it adds in the raw data points and looks a bit like a raincloud as a result.

First, we will run the plot for just one variable, Condition, so we'll use summary\_data. Again, try changing the arguments to see how you can control different aspects of the plot, in particular, try removing \texttt{coord\_flip()} to see what happens.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_flat_violin}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_nudge}\NormalTok{(}\DataTypeTok{x =} \FloatTok{.25}\NormalTok{, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{), }
                   \DataTypeTok{trim=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.75}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ Condition), }
             \DataTypeTok{position =} \KeywordTok{position_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.05}\NormalTok{), }
             \DataTypeTok{size =} \FloatTok{.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.75}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.1}\NormalTok{, }\DataTypeTok{outlier.shape =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ summary_data,}
    \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
    \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
    \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/rc1-1} 

}

\caption{Raincloud plot for one factor}\label{fig:rc1}
\end{figure}

\hypertarget{raincloud-plots-with-multiple-factors}{%
\subsection{Raincloud plots with multiple factors}\label{raincloud-plots-with-multiple-factors}}

Now we can run the code for a 2 x 2 plot, adding in Gender to \texttt{fill} argument and changing to \texttt{summary\_data2}. This is quite a complicated plot, do not worry if you are struggling to understand the code but remember, you just need to understand which bits to change.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(zhang_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Gender))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_flat_violin}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_nudge}\NormalTok{(}\DataTypeTok{x =} \FloatTok{.25}\NormalTok{, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{), }
                   \DataTypeTok{trim=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.05}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.05}\NormalTok{), }
             \DataTypeTok{size =} \FloatTok{.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.1}\NormalTok{, }\DataTypeTok{outlier.shape =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ summary_data2,}
    \KeywordTok{aes}\NormalTok{(Condition, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
    \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
    \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{),}
    \DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-week-8_files/figure-latex/rc2-1} 

}

\caption{Raincloud plot for two factors}\label{fig:rc2}
\end{figure}

\hypertarget{finished-7}{%
\subsection{Finished!}\label{finished-7}}

And you're done! As we've said throughout this chapter, you do not need to remember all of this code, you just need to remember what's possible and where to find the examples that you can modify.

\hypertarget{power-and-error}{%
\chapter{Power and error}\label{power-and-error}}

Until now we have mainly spent time on data-wrangling, visualising our data, and running inferential tests. In the lectures you have also learned about additional aspects of inferential testing and trying to reduce certain types of error in your analyses:

\begin{itemize}
\tightlist
\item
  \textbf{Type I error} - rejecting the null hypothesis when it is true (otherwise called \textbf{alpha} or \(\alpha\)). Probably better recalled as \textbf{False Positives}
\item
  \textbf{Type II error} - retaining the null hypothesis when it is false (otherwise called \textbf{beta} or \(\beta\)). Probably better recalled as \textbf{False Negatives}
\end{itemize}

Building from there we have started to discuss the idea of \textbf{power} (\(1-\beta\)) which you should understand as the long-run probability of correctly rejecting the null hypothesis when it is false; i.e.~saying there is an effect when there is not one. In short, \textbf{the higher the power of your study the better}, with the field standard proposed as \(power >= .8\). Often in fact Registered Reports are required to have a power of at least \(power >= .9\).

In the past a number of studies have fallen short of the field standard and it is this lack of power that is thought to be a key issue in the replication crisis. This makes sense because, if you think about it, if previous studies only have a \(power = .5\) then they only have a .5 probability of correctly rejecting the null hypothesis. As such there may be a large number of studies where the null hypothesis has been rejected when it should not have been; the field becomes noisy at that point and you are unsure which studies will replicate. It is issues like this that led us to redevelop our courses and why we really want you to understand power as much as possible.

When designing an experiment any good researcher will consider four key elements of a study. The \textbf{APES}:

\begin{itemize}
\tightlist
\item
  \textbf{alpha} - most commonly thought of as the significance level (i.e., your p-value); usually set at \(\alpha = .05\)
\item
  \textbf{power} - typically set at \(power = .8\)
\item
  \textbf{effect size} - size of the relationship/difference between two variables
\item
  \textbf{sample size} - number of participants you ran in your study
\end{itemize}

And the beautiful thing is that \textbf{if you know three of these elements then you can calculate the fourth}. The two most common calculations prior to a study would be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  to determine the appropriate sample size required to obtain the effect size that you are interested in. I.e. you know everything except the sample size. Generally, \textbf{the smaller the effect size, the more participants you will need}, assuming power and alpha are held constant at \textbf{.8} and \textbf{.05} respectively.
\item
  to determine the smallest effect size you can reliably detect given your sample size. I.e. you know everything except the effect size. For example, say you are using an open dataset (like the mini-project) and you know they have run 100 participants, you can't add any more participants, but you want to know what is the minimum effect size you could reliably detect in this dataset.
\end{enumerate}

\textbf{Note:} Most papers would discourage you from calculating what is called Observed Power. This is where you calculate the power after running the study, based on your effect size and sample size. Similarly, this would be running an analysis on an open dataset, finding the outcome, and then calculating the power based on the outcome. Avoid this. You can read more about why, here, in your own time if you like: \href{http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html}{Lakens (2014) Observed Power, and what to do if your editor asks for post-hoc power analyses}

So let's jump into this a bit now and start running some analyses to help further our understanding of alpha, power, effect sizes and sample size!

\hypertarget{effect-size}{%
\section{Effect Size}\label{effect-size}}

We will focus on effect sizes for t-tests for this worksheet. There are a number of different effect sizes to choose from in the field but today we will look at one type of effect size - \textbf{Cohen's d}: the standardised difference between two means (in units of SD). The thing to note is that the formula is slightly different depending on the type of t-test used and it can sometimes change depending on who you read. For this worksheet, let's go with the following formulas:

\begin{itemize}
\tightlist
\item
  One-sample t-test \& paired-sample t-test:
\end{itemize}

\begin{quote}
\(d = t\ / \sqrt(N)\)
\end{quote}

\begin{itemize}
\tightlist
\item
  Independent t-test:
\end{itemize}

\begin{quote}
\(d = 2t\ / \sqrt(df)\)
\end{quote}

Let's now try out some calculations. We will start with just looking at effect sizes from t-tests before calculating power in later tasks.

\hypertarget{activity-1-set-up-9}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-9}}

Do the following.

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 9 folder.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 9''.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads the packages \texttt{pwr}, \texttt{broom}, and \texttt{tidyverse} using the \texttt{library()} function. You may need to install \texttt{pwr} if you are working on your own machine but remember \textbf{never install packages on a university machine}.
\end{itemize}

\hypertarget{activity-2-effect-size-from-a-one-sample-t-test}{%
\section{Activity 2: Effect size from a one-sample t-test}\label{activity-2-effect-size-from-a-one-sample-t-test}}

\begin{itemize}
\tightlist
\item
  You run a one-sample t-test and discover a significant effect, t(25) = 3.24, p \textless{} .05. Using the above formulas, calculate \texttt{d} and determine whether the effect size is small, medium or large.
\end{itemize}

Helpful hint

\begin{itemize}
\tightlist
\item
  Use the appropriate formula from above for the one-sample t-tests.
\item
  You have been given a t-value and df (degrees of freedom), you still need to determine \texttt{n} before you calculate \texttt{d}.
\item
  According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (\textgreater{} .8).
\end{itemize}

Answering the following questions to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\tightlist
\item
  Enter, in digits, how many people were run in this study: 
\item
  Which of these codes is the appropriate calculation of \texttt{d} in this instance: d = t/sqrt(N) d = 2t/sqrt(df)
\item
  Enter the correct value of \texttt{d} for this analysis rounded to 2 decimal places: 
\item
  According to Cohen (1988), the effect size for this t-test would probably be considered: small medium large
\end{itemize}

\hypertarget{activity-3-effect-size-from-between-subjects-t-test}{%
\section{Activity 3: Effect size from between-subjects t-test}\label{activity-3-effect-size-from-between-subjects-t-test}}

\begin{itemize}
\tightlist
\item
  You run a between-subjects t-test and discover a significant effect, t(30) = 2.9, p \textless{} .05. Calculate \texttt{d} and determine whether the effect size is small, medium or large.
\end{itemize}

Helpful hint

\begin{itemize}
\tightlist
\item
  Use the appropriate formula above for between-subjects t-tests.
\item
  According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (\textgreater{} .8).
\end{itemize}

Answer the following questions to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\tightlist
\item
  Enter, in digits, how many people were run in this study: 
\item
  Which of these codes is the appropriate calculation of \texttt{d} in this instance: d = t/sqrt(N) d = 2t/sqrt(df)
\item
  Enter the correct value of \texttt{d} for this analysis rounded to 2 decimal places: 
\item
  According to Cohen (1988), the effect size for this t-test would probably be considered: small medium large
\end{itemize}

\hypertarget{activity-4-t-value-and-effect-size-for-a-between-subjects-experiment}{%
\section{Activity 4: t-value and effect size for a between-subjects Experiment}\label{activity-4-t-value-and-effect-size-for-a-between-subjects-experiment}}

\begin{itemize}
\tightlist
\item
  You run a between-subjects design study and the descriptives tell you: \textbf{Group 1}, M = 10, SD = 1.3, n = 30; \textbf{Group 2}, M = 11, SD = 1.7, n = 30. Calculate \texttt{t} and \texttt{d} for this between-subjects experiment.
\end{itemize}

Helpful hint

\begin{itemize}
\tightlist
\item
  Before you can calculate \texttt{d} (using the appropriate formula for a between-subjects experiment), you need to first calculate \texttt{t} using the formula:
\end{itemize}

\texttt{t\ =\ (Mean1\ -\ Mean2)/sqrt((var1/n1)\ +\ (var2/n2))}

\begin{itemize}
\tightlist
\item
  \texttt{var} stands for variance in the above formula. Variance is not the same as the standard deviation, right? Variance is measured in squared units. So for this equation, if you require variance to calculate \texttt{t} and you have the standard deviation, then you need to remember that \texttt{var\ =\ SD\^{}2}.
\item
  Now you have your t-value, but for calculating \texttt{d} you also need degrees of freedom. Think about how you would calculate \texttt{df} for a between-subjects experiment, taking \texttt{n} for both Group 1 and Group 2 into account.
\item
  Remember that convention is that people report the \texttt{t} and \texttt{d} values as positive.
\end{itemize}

Answer the following questions to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\item
  Enter the correct \texttt{t-value} for this test, rounded to two decimal places: 
\item
  Which of these codes is the appropriate calculation of \texttt{d} in this instance: d = t/sqrt(N) d = 2t/sqrt(df)
\item
  Based on the above t-value above, enter the correct value of \texttt{d} for this analysis rounded to 2 decimal places: 
\item
  According to Cohen (1988), the effect size for this t-test would probably be described as: small medium large
\end{itemize}

We've asked you to calculate Cohen's D by hand above to reinforce your understanding of what \texttt{d} actually means, however, if you were conducting a t-test in R, chances are that you would get R is calculate this for you.

\begin{itemize}
\tightlist
\item
  Think back to the t-test chapter. What is the name of the function for calculating Cohen's D? . What package does this come from? 
\end{itemize}

\textbf{Excellent!} Now that you are comfortable with calculating effect sizes, we will look at using them to establish appropriate sample sizes for a given power. Remember, in analysis, in nearly all occasions we should set the effect size as the minimum effect size we are interested. This can be determined through discussion, through previous studies, through pilots studies, or through rules of thumb like Cohen (1988). However, also keep in mind that the lower the effect size, the larger the sample size you will need. Everything is a trade-off.

\hypertarget{activity-5-pwr.t.test}{%
\section{\texorpdfstring{Activity 5: \texttt{pwr.t.test()}}{Activity 5: pwr.t.test()}}\label{activity-5-pwr.t.test}}

Today we will use the functions \texttt{pwr.t.test()}, \texttt{pwr.r.test()} and \texttt{pwr.chisq.test} from the package \texttt{pwr} to run power calculations for t-tests, correlations and chi-square.

Remember that for more information on this function, simply do \texttt{?pwr.t.test} in the console. On doing this you will see that \texttt{pwr.t.test()} takes a series of inputs:

\begin{itemize}
\tightlist
\item
  \textbf{n} - Number of observations (\textbf{per sample})
\item
  \textbf{d} - Effect size (Cohen's d) - difference between the means divided by the pooled standard deviation
\item
  \textbf{sig.level} - Significance level (Type I error probability) or \(\alpha\)
\item
  \textbf{power} - Power of test (1 minus Type II error probability) or \(1-\beta\)
\item
  \textbf{type} - Type of t test : \texttt{one.sample}, \texttt{two.sample}, or \texttt{paired}
\item
  \textbf{alternative} - the type of hypothesis; \texttt{"two.sided",\ "greater",\ "less"}
\end{itemize}

The function works on a leave one out principle. You give it all the information you have and it returns the element you are missing. So, for example, say you needed to know how many people per group (n) you would need to detect an effect size of \texttt{d\ =\ .4} with \texttt{power\ =\ .8}, \texttt{alpha\ =\ .05} in a \texttt{two.sample} (between-subjects) t-test on a \texttt{two.sided} hypothesis test.

\begin{itemize}
\tightlist
\item
  Run the below code:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{d =} \FloatTok{.4}\NormalTok{,}
           \DataTypeTok{power =} \FloatTok{.8}\NormalTok{,}
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{,}
           \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
           \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output tells you that you would need 99.0803248 people \textbf{per condition}. But you only get whole people and we like to be conservative on our estimates so we would actually run 100 \textbf{per condition}. That is a lot of people!!!

To make the output of \texttt{pwr.t.test()} easier to work with, we're going to amend the code to just give us exactly the number that we want.

\begin{itemize}
\tightlist
\item
  \texttt{tidy()} will tidy the output and store it in a table (you have used this before)
\item
  \texttt{pull()} will pull out a single value (in this case \texttt{n} but it could be anything)
\item
  \texttt{ceiling()} rounds up to give us the next highest whole number
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{d =} \FloatTok{.4}\NormalTok{,}
           \DataTypeTok{power =} \FloatTok{.8}\NormalTok{,}
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{,}
           \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
           \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pull}\NormalTok{(n) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ceiling}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-6-sample-size-for-standard-power-one-sample-t-test}{%
\section{Activity 6: Sample size for standard power one-sample t-test}\label{activity-6-sample-size-for-standard-power-one-sample-t-test}}

\begin{itemize}
\tightlist
\item
  Assuming you are interested in detecting a minimum Cohen's d of \textbf{d = .23}, what would be the minimum number of participants you would need in a one-sample t-test, assuming \textbf{power = .8}, \(\alpha\) \textbf{= .05}, on a two-sided hypothesis?
\end{itemize}

Using a pipeline, store the answer as a single, rounded value called \texttt{sample\_size\_t} (i.e.~use \texttt{tidy()\ \%\textgreater{}\%\ pull()\ \%\textgreater{}\%\ ceiling()}).

Helpful hint

\begin{itemize}
\tightlist
\item
  Use the list of inputs above as a kind of check-list to clearly determine which inputs are known or unknown. This can help you enter the appropriate values to your code.
\item
  The structure of the \texttt{pwr.t.test()} would be very similar to the one shown above except two.sample would become one.sample
\item
  You will also need to use \texttt{tidy()\ \%\textgreater{}\%\ pull(n)} to help you obtain the sample size and \texttt{\%\textgreater{}\%\ ceiling()} to round up to the nearest whole participant.
\end{itemize}

Answer the following question to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\tightlist
\item
  Enter the minimum number of participants you would need in this one-sample t-test: 
\end{itemize}

\hypertarget{activity-7-effect-size-from-a-high-power-between-subjects-t-test}{%
\section{Activity 7: Effect size from a high power between-subjects t-test}\label{activity-7-effect-size-from-a-high-power-between-subjects-t-test}}

\begin{itemize}
\tightlist
\item
  Assuming you run a between-subjects t-test with 50 participants per group and want a power of .9, what would be the minimum effect size you can reliably detect? Assume standard \(\alpha\) and alternative hypothesis settings.
\end{itemize}

Answer the following questions to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\tightlist
\item
  Based on the information given, what will you set \texttt{type} as in the function? one.sample two.sample
\item
  Based on the output, enter the minimum effect size you can reliably detect in this test, rounded to two decimal places: 
\item
  According to Cohen (1988), the effect size for this t-test is small medium large
\item
  Say you run the study and find that the effect size determined is d = .50. Given what you know about power, select the statement that is true: the study is sufficiently powered as the analysis indicates you can detect only effect sizes smaller than d = .65 the study is underpowered as the analysis indicates you can detect only effect sizes larger than d = .65
\end{itemize}

\hypertarget{activity-8-sample-size-for-a-correlation}{%
\section{Activity 8: Sample size for a correlation}\label{activity-8-sample-size-for-a-correlation}}

Now, we're going to do the same thing but for a correlation analysis using \texttt{pwr.r.test}. The structure of this function is very similar to \texttt{pwr.t.test()} and works on the same leave-one-out principle:

\begin{itemize}
\item
  \textbf{n} - Number of observations
\item
  \textbf{r} - Correlation coefficient
\item
  \textbf{sig.level} - Significance level (Type I error probability)
\item
  \textbf{power} - Power of test (1 minus Type II error probability)
\item
  \textbf{alternative} - a character string specifying the alternative hypothesis, must be one of \texttt{two.sided} (default), \texttt{greater} (a positive correlation) or \texttt{less} (a negative correlation).
\item
  Assuming you are interested in detecting a minimum correlation of \textbf{r = .4} (in either direction), what would be the minimum number of participants you would need for a correlation analysis, assuming \textbf{power = .8}, \(\alpha\) \textbf{= .05}?
\end{itemize}

Using a pipeline, store the answer as a single, rounded value called \texttt{sample\_size\_r} (i.e.~use \texttt{tidy()\ \%\textgreater{}\%\ pull()\ \%\textgreater{}\%\ ceiling()}).

\begin{itemize}
\tightlist
\item
  Enter the minimum number of participants you would need in this correlation: 
\end{itemize}

\hypertarget{activity-9-effect-size-for-a-correlation-analysis}{%
\section{Activity 9: Effect size for a correlation analysis}\label{activity-9-effect-size-for-a-correlation-analysis}}

\begin{itemize}
\tightlist
\item
  You run a correlation analysis with 50 participants and the standard power and alpha levels and you have hypothesised a positive correlation, what would be the minimum effect size you can reliably detect?
\end{itemize}

Answer the following questions to check your answers. The solutions are at the bottom if you need them:

\begin{itemize}
\tightlist
\item
  Based on the information given, what will you set \texttt{alternative} as in the function? two.sided greater less
\item
  Based on the output, enter the minimum effect size you can reliably detect in this test, rounded to two decimal places: 
\item
  According to Cohen (1988), the effect size for this correlation is small medium large
\item
  Say you run the study and find that the effect size determined is d = .24. Given what you know about power, select the statement that is true: the study is sufficiently powered as the analysis indicates you can detect only effect sizes smaller than d = .24 the study is underpowered as the analysis indicates you can detect only effect sizes larger than d = .34
\end{itemize}

\hypertarget{activity-10-effect-size-for-chi-square}{%
\section{Activity 10: Effect size for chi-square}\label{activity-10-effect-size-for-chi-square}}

Again, \texttt{pwr.chisq.test()} is very similar to the t-test and correlation functions.

\begin{itemize}
\tightlist
\item
  \textbf{w} - Effect size (you can enter Cramer's V which we calculated in Chapter \ref{cramer})
\item
  \textbf{N } - Total number of observations
\item
  \textbf{df} - degree of freedom
\item
  \textbf{sig.level} - Significance level (Type I error probability)
\item
  \textbf{power} - Power of test (1 minus Type II error probability)
\end{itemize}

Imagine you were conducting a chi-square analysis with 30 observations, 1 df, and conventional alpha and power.

\begin{itemize}
\tightlist
\item
  Using \texttt{pwr.chisq.test()}, what is the smallest effect size that you could reliably detect, rounded to 2 decimal places? 
\end{itemize}

\hypertarget{activity-11-power-of-published-research}{%
\section{Activity 11: Power of published research}\label{activity-11-power-of-published-research}}

Thus far we have used hypothetical situations - now go look at the paper on the \href{https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities}{Open Stats Lab} website called Does Music Convey Social Information to Infants? (we have used this dataset in the t-test chapter). You can download the pdf and look at it, but here we will determine the power of the significant t-tests reported in Experiment 1 under the Results section on Pg489. There is a one-sample t-test and a paired-samples t-test to consider, summarised below. Assume testing was at power = .8, alpha = .05. Based on your calculations are either of the stated effects underpowered?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  one-sample: t(31) = 2.96, p = .006, d = 0.52
\item
  paired t-test: t(31) = 2.42, p = .022, d= 0.43
\end{enumerate}

Helpful hint

\begin{itemize}
\tightlist
\item
  To calculate n: \texttt{n\ =\ df\ +\ 1}.
\end{itemize}

Which of the t-tests do you believe to be underpowered? Why do you think this may be? Additional information about this can be found in the solution to task 8 at the end of this activity.

\hypertarget{finished-8}{%
\subsection{Finished!}\label{finished-8}}

\textbf{Great!} Hopefully you are now starting to see the interaction between alpha, power, effect sizes, and sample size. We should always want really high powered studies and depending on the size of the effect we are interested in (small to large), and our \(\alpha\) level, this will mean we will need to run more or less participants to make sure our study is well powered. Points to note:

\begin{itemize}
\tightlist
\item
  Lowering the \(\alpha\) level (e.g. .05 to .01) will reduce the power.
\item
  Lowering the effect size (e.g. .8 to .2) will reduce the power.
\item
  Increasing power (.8 to .9) will require more participants.
\end{itemize}

A high-powered study looking to detect a small effect size at a low alpha will require a large number of participants!

There are additional functions in the \texttt{pwr} package for other types of statistical analyses. We will include these calculates as part of the ANOVA and regression chapters.

If you want more examples of power to reinforce your understanding, go back and calculate the power of the t-tests, correlations, and chi-squares from earlier chapters.

\hypertarget{activity-solutions-6}{%
\section{Activity solutions}\label{activity-solutions-6}}

Below you will find possible solutions to the above tasks. But first, be sure to try the tasks before looking at the solutions and only look at them when you have exhausted all possibilities and yourself. If that is the case, and you are sure you want to do this, then here are the potential solutions.

\hypertarget{activity-1-4}{%
\subsection{Activity 1}\label{activity-1-4}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pwr)}
\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Click on the tab to reveal the solution

\hypertarget{activity-2-2}{%
\subsection{Activity 2}\label{activity-2-2}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\FloatTok{3.24} \OperatorTok{/}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{25} \OperatorTok{+}\DecValTok{1}\NormalTok{)}

\CommentTok{# effect is medium to large; d = .64}
\end{Highlighting}
\end{Shaded}

Click on the tab to reveal the solution

\hypertarget{activity-3-3}{%
\subsection{Activity 3}\label{activity-3-3}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\NormalTok{(}\DecValTok{2}\OperatorTok{*}\FloatTok{2.9}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{30}\NormalTok{)}

\CommentTok{# effect is large; d = 1.06}
\end{Highlighting}
\end{Shaded}

Click on the tab to reveal the solution

\hypertarget{activity-4-2}{%
\subsection{Activity 4}\label{activity-4-2}}

Activity 4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t =}\StringTok{ }\NormalTok{(}\DecValTok{10} \OperatorTok{-}\StringTok{ }\DecValTok{11}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{((}\FloatTok{1.3}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\DecValTok{30}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\FloatTok{1.7}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\DecValTok{30}\NormalTok{))}

\NormalTok{d =}\StringTok{ }\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{t)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{((}\DecValTok{30-1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{30-1}\NormalTok{))}

\CommentTok{# t = 2.56}
\CommentTok{# d = .67}

\CommentTok{# Remember that convention is that people report the t and d as positive.}
\end{Highlighting}
\end{Shaded}

Click on the tab to reveal the solution

\hypertarget{activity-6-2}{%
\subsection{Activity 6}\label{activity-6-2}}

Activity 6

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_size_t <-}\StringTok{ }\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{d =} \FloatTok{.23}\NormalTok{,}
                            \DataTypeTok{power =} \FloatTok{.8}\NormalTok{, }
                            \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }
                            \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
                            \DataTypeTok{type =} \StringTok{"one.sample"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(n) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ceiling}\NormalTok{()}

\NormalTok{sample_size_t}
\end{Highlighting}
\end{Shaded}

{[}1{]} 151

Click on the tab to reveal the solution

\hypertarget{activity-7-3}{%
\subsection{Activity 7}\label{activity-7-3}}

Activity 7

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{n =} \DecValTok{50}\NormalTok{,}
           \DataTypeTok{power =} \FloatTok{.9}\NormalTok{, }
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }
           \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
           \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Two-sample t test power calculation 

          n = 50
          d = 0.654752
  sig.level = 0.05
      power = 0.9
alternative = two.sided
\end{verbatim}

NOTE: n is number in \emph{each} group

Click on the tab to reveal the solution

\hypertarget{activity-8-3}{%
\subsection{Activity 8}\label{activity-8-3}}

Activity 8

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_size_r <-}\StringTok{ }\KeywordTok{pwr.r.test}\NormalTok{(}\DataTypeTok{r =} \FloatTok{.4}\NormalTok{, }
                            \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }
                            \DataTypeTok{power =} \FloatTok{.8}\NormalTok{, }
                            \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(n) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ceiling}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Click on the tab to reveal the solution

\hypertarget{activity-9-2}{%
\subsection{Activity 9}\label{activity-9-2}}

Activity 9

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.r.test}\NormalTok{(}\DataTypeTok{n =} \DecValTok{50}\NormalTok{,}
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }
           \DataTypeTok{power =} \FloatTok{.8}\NormalTok{, }
           \DataTypeTok{alternative =} \StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 approximate correlation power calculation (arctangh transformation) 

          n = 50
          r = 0.3443671
  sig.level = 0.05
      power = 0.8
alternative = greater
\end{verbatim}

Click on the tab to reveal the solution

\hypertarget{activity-10}{%
\subsection{Activity 10}\label{activity-10}}

Activity 10

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.chisq.test}\NormalTok{(}\DataTypeTok{N =} \DecValTok{30}\NormalTok{,}
               \DataTypeTok{df =} \DecValTok{1}\NormalTok{,}
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }
           \DataTypeTok{power =} \FloatTok{.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Chi squared power calculation 

          w = 0.5114954
          N = 30
         df = 1
  sig.level = 0.05
      power = 0.8
\end{verbatim}

NOTE: N is the number of observations

Click on the tab to reveal the solution

\hypertarget{activity-11}{%
\subsection{Activity 11}\label{activity-11}}

Activity 11

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Achievable Cohen d for Example 1}
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{.8}\NormalTok{, }
           \DataTypeTok{n =} \DecValTok{32}\NormalTok{, }
           \DataTypeTok{type =} \StringTok{"one.sample"}\NormalTok{, }
           \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 One-sample t test power calculation 

          n = 32
          d = 0.5112738
  sig.level = 0.05
      power = 0.8
alternative = two.sided
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This study seems ok as the authors could achieve an effect size as low as .51 and found an effect size at .52}

\CommentTok{# Achievable Cohen d for Example 2}
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{.8}\NormalTok{, }
           \DataTypeTok{n =} \DecValTok{32}\NormalTok{, }
           \DataTypeTok{type =} \StringTok{"paired"}\NormalTok{, }
           \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
           \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Paired t test power calculation 

          n = 32
          d = 0.5112738
  sig.level = 0.05
      power = 0.8
alternative = two.sided
\end{verbatim}

NOTE: n is number of \emph{pairs}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this effect might not be reliable given that the effect size found was much lower than the achievable effect size. The issue here is that the researchers established their sample size based on a previous effect size and not on the minimum effect size that they would find important. If an effect size as small as .4 was important then they should have powered all studies to that level and ran the appropriate n ~52 babies (see below). Flipside of course is that obtaining 52 babies isnt easy; hence why some people consider the Many Labs approach a good way ahead.}

\CommentTok{# Below you could calculate the actual sample size required to achieve a power of .8:}

\NormalTok{sample_size <-}\StringTok{ }\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{.8}\NormalTok{,}
                          \DataTypeTok{d =} \FloatTok{.4}\NormalTok{, }
                          \DataTypeTok{type =} \StringTok{"paired"}\NormalTok{, }
                          \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
                          \DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{) }\OperatorTok{%>%}
\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(n) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ceiling}\NormalTok{()}

\NormalTok{sample_size}
\end{Highlighting}
\end{Shaded}

{[}1{]} 52

Click on the tab to reveal the solution.

\hypertarget{one-way-anova}{%
\chapter{One-way ANOVA}\label{one-way-anova}}

\hypertarget{background-intrusive-memories}{%
\subsection{Background: Intrusive memories}\label{background-intrusive-memories}}

In the lecture we worked through calculating an ANOVA by hand in order to gain a conceptual understanding. However, when you run an ANOVA, typically the computer does all of these calculations for you. In this chapter we'll show you how to run a one-factor and factorial ANOVA using the \texttt{afex} package and post-hoc tests using a package called \texttt{emmeans}.

In this example we will be using data from experiment 2 of \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4526368/}{James, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, A. L., \& Holmes, E. A. (2015). Computer game play reduces intrusive memories of experimental trauma via reconsolidation-update mechanisms. Psychological Science, 26, 1201-1215}.

The abstract for the paper is as follows:

\begin{quote}
Memory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind's eye and cause distress. We investigated whether reconsolidation - the process during which memories become malleable when recalled - can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, non-invasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., \textgreater{} 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events.
\end{quote}

\hypertarget{activity-1-set-up-10}{%
\subsection{Activity 1: Set-up}\label{activity-1-set-up-10}}

Do the following.\\
* Open R Studio and set the working directory to your Week 10 folder. Ensure the environment is clear.\\
* Open a new R Markdown document and save it in your working directory. Call the file ``Week 10''.\\
* Download James Holmes\_Expt 2\_DATA.csv and extract the files in to your Week 10 folder.\\
* In a new code chunk, type and run the code that loads \texttt{pwr}, \texttt{lsr}, \texttt{car}, \texttt{broom}, \texttt{afex}, \texttt{emmeans} and \texttt{tidyverse} using the \texttt{library()} function and loads the data into an object named \texttt{dat} using \texttt{read\_csv()}. If you are working on your own machine you may need to install \texttt{afex} and \texttt{emmeans} but as always \textbf{do not install packages on university machines}.\\
* Add (hint: mutate) a column to \texttt{dat} called \texttt{subject}that equals \texttt{row\_number()} to act as a participant ID which is currently missing from the data set.

\hypertarget{activity-2-data-wrangling}{%
\subsection{Activity 2: Data wrangling}\label{activity-2-data-wrangling}}

There are a lot of columns in this data set that we don't need for this analysis and the names of the variable are also long and difficult to work with.

\begin{itemize}
\tightlist
\item
  Create a new object called \texttt{dat2} that just has the three columns we need - use \texttt{select()} to select the columns \texttt{subject}, \texttt{Condition}, and \texttt{Days\_One\_to\_Seven\_Image\_Based\_Intrusions\_in\_Intrusion\_Diary}
\item
  Use \texttt{rename()} to rename \texttt{Days\_One\_to\_Seven\_Image\_Based\_Intrusions\_in\_Intrusion\_Diary} to \texttt{intrusions}
\item
  See if you can do this all in one pipeline
\item
  Hint: \texttt{new\_name\ =\ old\_name}
\end{itemize}

\hypertarget{activity-3-numbers-and-factors}{%
\subsection{Activity 3: Numbers and factors}\label{activity-3-numbers-and-factors}}

In addition to the names of the variables being too long, the levels of \texttt{Condition} are named 1,2,3,4 which R will think is a number rather than a category. We're going to overwrite the column \texttt{Condition} with a column that recodes these numbers as a factor. Copy and paste the code below into your Markdown and then run it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 <-}\StringTok{ }\NormalTok{dat2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Condition =} \KeywordTok{as.factor}\NormalTok{(Condition))}
\end{Highlighting}
\end{Shaded}

\textbf{This is a really important step}. If you forget to recode variables as factors and R treats them as numbers, a lot of things won't work. Trust us, we've spent a lot of time trying to figure out what was wrong because we forgot to do this step!

\hypertarget{activity-4-create-summary-statistics}{%
\subsection{Activity 4: Create summary statistics}\label{activity-4-create-summary-statistics}}

Next we want to calculate some descriptive statistics. We're really interested in the scores from each experimental group rather than overall.

\begin{itemize}
\tightlist
\item
  Create an object called \texttt{sum\_dat}that contains the mean, standard deviation and standard error for the number of intrusions grouped by \texttt{Condition}\\
\item
  Use the pipe to achieve this in one pipeline\\
\item
  Your table should have four columns, \texttt{Condition}, \texttt{mean}, \texttt{sd}, and \texttt{se}.
\end{itemize}

Hint

\begin{verbatim}
## 
## 
## * Use group_by(some_grouping_variable) %>% summarise(...)
## * standard error = sd/sqrt(n) =  sd/sqrt(length(some_variable_name)
\end{verbatim}

\hypertarget{activity-5-visualisation}{%
\subsection{Activity 5: Visualisation}\label{activity-5-visualisation}}

Now we can visualise the data. In the original paper they use a bar plot, which we will reproduce later but for now let's use a better plot that gives us more information about the data.

\begin{itemize}
\tightlist
\item
  Create the below violin-boxplot with number of intrusions on the y-axis and condition on the x-axis (see the Visualisation chapter for more info).
\item
  Change the labels on the x-axis to something more informative (hint: \texttt{scale\_x\_discrete(labels\ =\ c("label\ names")})
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{13-week-10_files/figure-latex/violin-plot-1} 

}

\caption{Number of intrusions by condition}\label{fig:violin-plot}
\end{figure}

We can see from this plot that there are outliers in each of the groups. This information isn't present in the bar plot, which is why it's not a good idea to use them but we will reproduce it anyway.The below code shows you how to produce the bar plot that is presented in the paper. Try and figure out what each bit of code is doing in the plot (remember to use the help documentation for each function) and see what happens when you change the values for each argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sum_dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ mean, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_summary}\NormalTok{(}\DataTypeTok{fun.y =}\NormalTok{ mean, }\DataTypeTok{geom =} \StringTok{"bar"}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =}\NormalTok{ mean }\OperatorTok{-}\StringTok{ }\NormalTok{se, }\DataTypeTok{ymax =}\NormalTok{ mean }\OperatorTok{+}\StringTok{ }\NormalTok{se), }\DataTypeTok{width =} \FloatTok{0.25}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{), }
                     \DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{), }
                     \DataTypeTok{name =} \StringTok{"Intrusive-Memory Frequency (Mean for the Week"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"No-task control"}\NormalTok{, }\StringTok{"Reactivation plus Tetris"}\NormalTok{, }\StringTok{"Tetris only"}\NormalTok{,}
                                \StringTok{"Reactivation only"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{13-week-10_files/figure-latex/bar-plot-1} 

}

\caption{Bar plot of instrusions by condition}\label{fig:bar-plot}
\end{figure}

\hypertarget{activity-6-one-way-anova}{%
\subsection{Activity 6: One-way ANOVA}\label{activity-6-one-way-anova}}

Now we can run the one-way ANOVA using \texttt{aov\_ez} from the \texttt{afex} package and save it to the object \texttt{mod}. As well as running the ANOVA, the \texttt{aov\_ez} function also conducts a Levene's test for homogeneity of variance so that we can test our final assumption.

\texttt{aov\_ez()} will likely produce some messages that look like errors, do not worry about these, they are just letting you know what it's done.

\begin{itemize}
\tightlist
\item
  Copy and paste the below code to run and then view the results of the ANOVA using \texttt{anova(mod)}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{aov_ez}\NormalTok{(}\DataTypeTok{id =} \StringTok{"subject"}\NormalTok{, }\CommentTok{# the column containing the subject IDs}
              \DataTypeTok{dv =} \StringTok{"intrusions"}\NormalTok{, }\CommentTok{# the DV }
              \DataTypeTok{between =} \StringTok{"Condition"}\NormalTok{, }\CommentTok{# the between-subject variable}
              \DataTypeTok{es =} \StringTok{"pes"}\NormalTok{, }\CommentTok{# sets effect size to partial eta-squared}
              \DataTypeTok{type =} \DecValTok{3}\NormalTok{, }\CommentTok{# this affects how the sum of squares is calculated, set this to 3}
              \DataTypeTok{data =}\NormalTok{ dat2)}

\KeywordTok{anova}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

Just like with the t-tests and correlations, we can use \texttt{tidy()} to make the output easier to work with.

\begin{itemize}
\tightlist
\item
  Run the below code to transform the output. Don't worry about the warning message, it is just telling you it doesn't know how to automatically rename the columns so it will keep the original names.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_output <-}\StringTok{ }\NormalTok{(mod}\OperatorTok{$}\NormalTok{anova_table) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in tidy.anova(.): The following column names in ANOVA output were
## not recognized or transformed: num.Df, den.Df, MSE, ges
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{term} = the IV\\
\item
  \texttt{num.Df} = degrees of freedom effect
\item
  \texttt{den.Df} = degrees of freedom residuals
\item
  \texttt{MSE} = Mean-squared errors
\item
  \texttt{statistic} = F-statistic
\item
  \texttt{ges} = effect size\\
\item
  \texttt{p.value} = p.value
\end{itemize}

You should refer to the lecture for more information on what each variable means and how it is calculated.

\begin{itemize}
\tightlist
\item
  Is the overall effect of Condition significant? Yes No
\item
  What is the F-statistics to 2 decimal places? 
\item
  According to the rules of thumb, the effect size is Small Medium Large
\end{itemize}

\hypertarget{activity-7-assumption-checking}{%
\subsection{Activity 7: Assumption checking}\label{activity-7-assumption-checking}}

You may be wondering why we haven't yet checked the assumptions. Well, unlike the t-tests and correlations, in order to test the assumptions we need to use the model we created with \texttt{aov\_ez()}, so we couldn't assess them all until this point. For a one-way independent ANOVA, the assumptions are the same as for a Student's t-test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The DV is interval or ratio data
\item
  The observations should be independent
\item
  The residuals should be normally distributed
\item
  There should be homogeneity of variance between the groups
\end{enumerate}

We know that 1 and 2 are met because of the design of our study. To test 3, as we have done before we can look at the QQ-plot of the residuals and test for normality with the Shapiro-Wilk test. The residuals have been stored as one of the components of \texttt{mod}. To access them we specify \texttt{mod\$aov\$residuals}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqPlot}\NormalTok{(mod}\OperatorTok{$}\NormalTok{aov}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{13-week-10_files/figure-latex/fig-qq-1} 

}

\caption{qq-plot for model residuals}\label{fig:fig-qq}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(mod}\OperatorTok{$}\NormalTok{aov}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11 60
## 
##  Shapiro-Wilk normality test
## 
## data:  mod$aov$residuals
## W = 0.87739, p-value = 4.252e-06
\end{verbatim}

There are a few things to note about the assumption test results. First, look at the p-value for the Shapiro-Wilk test - \texttt{4.252e-06}. Whenever you see the \texttt{e} at the end of a number it means that R is using \textbf{scientific notation}. Scientific notation is a way of writing very large or very small numbers. Because the number after the \texttt{e} is negative it means the number should be divided by 10 to the power of six. Put simply, move the decimal place six places to the left and you will get the standard number. When reporting p-values in your results section, you should not use scientific notation, instead you should round to 3 decimal places.

\begin{itemize}
\tightlist
\item
  What is the value of \texttt{4.252e-06}? .004252 42.52 .000004252
\end{itemize}

If you want R to round this for you to make it easier to read, you could use the below code to save it to an object, tidy it and then round the p.value. Just remember that in APA style you should never write ``p = 0'', instead, you should write ``p \textless{} .001'' (because p will never equal actual zero, it can just be very, very, very small).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shapiro <-}\StringTok{ }\KeywordTok{shapiro.test}\NormalTok{(mod}\OperatorTok{$}\NormalTok{aov}\OperatorTok{$}\NormalTok{residuals) }\OperatorTok{%>%}\StringTok{ }\CommentTok{#run the test}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\CommentTok{# tidy the output}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{p.value =} \KeywordTok{round}\NormalTok{(p.value, }\DataTypeTok{digits =} \DecValTok{3}\NormalTok{)) }\CommentTok{# overwrite the p-value with one rounded to 3 decimal places}
\end{Highlighting}
\end{Shaded}

The second thing to note is that from both the qq-plot and the Shapiro-Wilk test it is clear that the assumption of normality has not been met. Is this a problem? Well, Field et al. (2009) say that if the sample sizes for each group are equal then ANOVA is robust to violations of both normality and of homogeneity of variance. There's also a good discussion of this \href{https://link.springer.com/article/10.3758/s13428-017-0918-2}{here} but it is a bit technical. We can check how many participants are in each condition using \texttt{count()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(Condition)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c}
\hline
Condition & n\\
\hline
1 & 18\\
\hline
2 & 18\\
\hline
3 & 18\\
\hline
4 & 18\\
\hline
\end{tabular}

Thankfully the sample sizes are equal so we should be OK to proceed with the ANOVA. It is not clear whether normality was checked in the original paper.

For the last assumption, we can test homogeneity of variance with Levene's test and the function \texttt{test\_levene()} from \texttt{afex}. The code for this is very simple, you just need to supply the ANOVA model we created earlier \texttt{mod}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_levene}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Levene's Test for Homogeneity of Variance (center = center)
##       Df F value  Pr(>F)  
## group  3  2.9551 0.03854 *
##       68                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The results from Levene's test show that the assumption of homogeneity of variance has also not been met. The paper does indicate this might be the case as it specifies that the ANOVAs do not assume equal variance, however, the results of the ANOVA that are reported are identical to our results above where no correction has been made although the post-hoc tests are Welch tests (you can tell this because the degrees of freedom have been adjusted and are not whole numbers).

Whilst all of this might seem very confusing - we imagine you might be wondering what the point of assumption testing is given that it seems to be ignored - we're showing you this for three reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To reassure you that sometimes the data can fail to meet the assumptions and it is still ok to use the test. To put this in statistical terms, many tests are \textbf{robust} to mild deviations of normality and unequal variance, particularly with equal sample sizes.
\item
  As a critical thinking point, to remind you that just because a piece of research has been published does not mean it is perfect and you should always evaluate whether the methods used are appropriate.
\item
  To reinforce the importance of pre-registration where these decisions could be made in advance, and/or open data and code so that analyses can be reproduced exactly to avoid any ambiguity about exactly what was done. In this example, given the equal sample sizes and the difference in variance between the groups isn't too extreme, it looks like it is still appropriate to use an ANOVA but the decisions and justification for those decisions could have been more transparent.
\end{enumerate}

\hypertarget{activity-8-post-hoc-tests}{%
\subsection{Activity 8: Post-hoc tests}\label{activity-8-post-hoc-tests}}

For post-hoc comparisons, as mentioned, the paper appears to have computed Welch t-tests but there is no mention of any multiple comparison correction. We could reproduce these results by using \texttt{t.test} for each of the contrasts.

For example, to compare condition 1 (the control group) with condition 2 (the reactivation plus tetris group) we could run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{droplevels}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{t.test}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }\DataTypeTok{data =}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

\begin{info}
Because \texttt{Condition} has four levels, we can't just specify
\texttt{intrustion\ \textasciitilde{}\ Condition} because a t-test
compares two groups and it wouldn't know which of the four to compare so
first we have to filter the data and use a new function
\texttt{droplevels()}. It's important to remember that when it comes to
R there are two things to consider, the data you can see and the
underlying structure of that data. In the above code we use
\texttt{filter()} to select only conditions 1 and 2 so that we can
compare them. However, that doesn't change the fact that R ``knows''
that \texttt{Condition} has four levels - it doesn't matter if two of
those levels don't have any observations any more, the underlying
structure still says there are four groups. \texttt{droplevels()} tells
R to remove any unused levels from a factor. Try running the above code
but without \texttt{droplevels()} and see what happens.
\end{info}

However, a quicker and better way of doing this that allows you apply a correction for multiple comparisons easily is to use \texttt{emmeans()} which computes all possible pairwise comparison t-tests and applies a correction to the p-value.

First, we use \texttt{emmeans()} to run the comparisons and then we can pull out the contrasts and use \texttt{tidy()} to make it easier to work with.

\begin{itemize}
\tightlist
\item
  Run the code below. Which conditions are significantly different from each other? Are any of the comparisons different from the ones reported in the paper now that a correction for multiple comparisons has been applied?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_pairwise <-}\KeywordTok{emmeans}\NormalTok{(mod}\OperatorTok{$}\NormalTok{aov, pairwise }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }\DataTypeTok{adjust =} \StringTok{"bonferroni"}\NormalTok{)}
\NormalTok{mod_contrasts <-}\StringTok{ }\NormalTok{mod_pairwise}\OperatorTok{$}\NormalTok{contrasts }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{warning}
The inquisitive amongst you may have noticed that \texttt{mod} is a list
of 5 and seemingly contains the same thing three times:
\texttt{anova\_table}, \texttt{aov} and \texttt{Anova}. The reasons
behind the differences are too complex to go into detail on this course
(see \href{https://rcompanion.org/rcompanion/d_04.html}{here} for more
info) but the simple version is that \texttt{anova\_table} and
\texttt{Anova}use one method of calculating the results (type 3 sum of
squares) and \texttt{aov} uses a different method (type 1 sum of
squares). What's important for your purposes is that you need to use
\texttt{anova\_table} to view the overall results (and replicate the
results from papers) and \texttt{aov}to run the follow-up tests and to
get access to the residuals (or \texttt{lm()} for factorial ANOVA). As
always, precision and attention to detail is key.
\end{warning}

\hypertarget{activity-9-power-and-effect-size}{%
\subsection{Activity 9: Power and effect size}\label{activity-9-power-and-effect-size}}

Finally, we can replicate their power analysis using \texttt{pwr.anova.test}.

\begin{quote}
On the basis of the effect size of d = 1.14 from Experiment 1, we assumed a large effect size of f = 0.4. A sample size of 18 per condition was required in order to ensure an 80\% power to detect this difference at the 5\% significance level.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.anova.test}\NormalTok{(}\DataTypeTok{k =} \DecValTok{4}\NormalTok{, }\DataTypeTok{f =} \FloatTok{.4}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }\DataTypeTok{power =} \FloatTok{.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Balanced one-way analysis of variance power calculation 
## 
##               k = 4
##               n = 18.04262
##               f = 0.4
##       sig.level = 0.05
##           power = 0.8
## 
## NOTE: n is number in each group
\end{verbatim}

We've already got the effect size for the overall ANOVA, however, we should also really calculate Cohen's D using \texttt{cohensD} from \texttt{lsr} for each of the pairwise comparisons. This code is a little complicated because you need to do it separately for each comparison, bind them all together and then add them to \texttt{mod\_contrasts} - just make sure your understand which bits of the code you would need to change to run this on different data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d_}\DecValTok{1}\NormalTok{_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{())}

\NormalTok{d_}\DecValTok{1}\NormalTok{_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{()) }

\NormalTok{d_}\DecValTok{1}\NormalTok{_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{())}

\NormalTok{d_}\DecValTok{2}\NormalTok{_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{())}

\NormalTok{d_}\DecValTok{2}\NormalTok{_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{())}

\NormalTok{d_}\DecValTok{3}\NormalTok{_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(intrusions }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                 \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(dat2, Condition }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{                   }\KeywordTok{droplevels}\NormalTok{())}

\NormalTok{pairwise_ds <-}\StringTok{ }\KeywordTok{c}\NormalTok{(d_}\DecValTok{1}\NormalTok{_}\DecValTok{2}\NormalTok{,d_}\DecValTok{1}\NormalTok{_}\DecValTok{3}\NormalTok{,d_}\DecValTok{1}\NormalTok{_}\DecValTok{4}\NormalTok{,d_}\DecValTok{2}\NormalTok{_}\DecValTok{3}\NormalTok{,d_}\DecValTok{2}\NormalTok{_}\DecValTok{4}\NormalTok{,d_}\DecValTok{3}\NormalTok{_}\DecValTok{4}\NormalTok{)}

\NormalTok{mod_contrasts <-}\StringTok{ }\NormalTok{mod_contrasts }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{eff_size =}\NormalTok{ pairwise_ds)}
\end{Highlighting}
\end{Shaded}

\begin{warning}
What are your options if the data don't meet the assumptions and it's
really not appropriate to continue with a regular one-way ANOVA? As
always, there are multiple options and it is a judgement call.

\begin{enumerate}
\def\arabic{enumi}.{\arabic{enumi}.}
\tightlist
\item
  You could run a non-parametric test, the Kruskal-Wallis for
  between-subject designs and the Friedman test for within-subject
  designs. There is more information about how to do this in Chapter
  \ref{refsup}.
\item
  If normality is the problem, you could try transforming the data.
  Field et al. (2009) has a good section on data transformation.
\item
  You could use bootstrapping, which is not something we will cover in
  this course at all. Again, Field et al. (2009) covers this although it
  is a little complicated.
\end{enumerate}
\end{warning}

\hypertarget{activity-10-write-up}{%
\subsection{Activity 10: Write-up}\label{activity-10-write-up}}

The below code replicates the write-up in the paper, although has changed the Welch t-test for the pairwise comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Second, and critically, }\ControlFlowTok{for}\NormalTok{ the }\DecValTok{7}\OperatorTok{-}\NormalTok{day diary postintervention, there was a significant difference between groups }\ControlFlowTok{in}\NormalTok{ overall intrusion frequency }\ControlFlowTok{in}\NormalTok{ daily life, }\KeywordTok{F}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_output$num.Df}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r mod_output$den.Df}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_output$statistic %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_output$p.value %>% round(3)}\StringTok{`}\NormalTok{, ηp2 =}\StringTok{ }\NormalTok{.}\StringTok{`}\DataTypeTok{r mod_output$ges %>% round(2)}\StringTok{`}\NormalTok{. Planned comparisons demonstrated that relative to the no}\OperatorTok{-}\NormalTok{task control group, only those }\ControlFlowTok{in}\NormalTok{ the reactivation}\OperatorTok{-}\NormalTok{plus}\OperatorTok{-}\NormalTok{Tetris group, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_contrasts$df[1]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_contrasts$statistic[1] %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_contrasts$p.value[1] %>% round(2)}\StringTok{`}\NormalTok{, d =}\StringTok{ `}\DataTypeTok{r mod_contrasts$eff_size[1] %>% round(2)}\StringTok{`}\NormalTok{, experienced significantly fewer intrusive memories; this finding replicated Experiment }\FloatTok{1.}\NormalTok{ Critically, as predicted by reconsolidation theory, the reactivation}\OperatorTok{-}\NormalTok{plus}\OperatorTok{-}\NormalTok{Tetris group had significantly fewer intrusive memories than the Tetris}\OperatorTok{-}\NormalTok{only group, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_contrasts$df[4]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_contrasts$statistic[4] %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_contrasts$p.value[4] %>% round(2)}\StringTok{`}\NormalTok{, d =}\StringTok{ `}\DataTypeTok{r mod_contrasts$eff_size[4] %>% round(2)}\StringTok{`}\NormalTok{, as well as the reactivation}\OperatorTok{-}\NormalTok{only group, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_contrasts$df[5]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_contrasts$statistic[5] %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_contrasts$p.value[5] %>% round(2)}\StringTok{`}\NormalTok{, d =}\StringTok{ `}\DataTypeTok{r mod_contrasts$eff_size[5] %>% round(2)}\StringTok{`}\NormalTok{. Further, there were no significant differences between the no}\OperatorTok{-}\NormalTok{task control group and the reactivation}\OperatorTok{-}\NormalTok{only group, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_contrasts$df[3]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_contrasts$statistic[3] %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_contrasts$p.value[3] %>% round(2)}\StringTok{`}\NormalTok{, or between the no}\OperatorTok{-}\NormalTok{task control group and the Tetris}\OperatorTok{-}\NormalTok{only group, }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_contrasts$df[2]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_contrasts$statistic[2] %>% round(2)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r mod_contrasts$p.value[2] %>% round(2)}\StringTok{`}
\end{Highlighting}
\end{Shaded}

Second, and critically, for the 7-day diary postintervention, there was a significant difference between groups in overall intrusion frequency in daily life, F(3, 68) = 3.79, p = 0.014, ηp2 = .0.14. Planned comparisons demonstrated that relative to the no-task control group, only those in the reactivation-plus-Tetris group, t(68) = 3.04, p = 0.02, d = 1, experienced significantly fewer intrusive memories; this finding replicated Experiment 1. Critically, as predicted by reconsolidation theory, the reactivation-plus-Tetris group had significantly fewer intrusive memories than the Tetris-only group, t(68) = -1.89, p = 0.38, d = 0.84, as well as the reactivation-only group, t(68) = -2.78, p = 0.04, d = 1.11. Further, there were no significant differences between the no-task control group and the reactivation-only group, t(68) = 0.26, p = 1, or between the no-task control group and the Tetris-only group, t(68) = 1.15, p = 1

\hypertarget{activity-solutions-7}{%
\subsection{Activity solutions}\label{activity-solutions-7}}

Below this line you will find the solutions to the above tasks. Only look at them after giving the tasks a good try and speaking to the tutor about any issues

\hypertarget{activity-1-5}{%
\subsubsection{Activity 1}\label{activity-1-5}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pwr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"lsr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"afex"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}

\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"James Holmes_Expt 2_DATA.csv"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{subject =} \KeywordTok{row_number}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-2-3}{%
\subsubsection{Activity 2}\label{activity-2-3}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 <-}\StringTok{ }\NormalTok{dat}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(subject,Condition,Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{intrusions =}\NormalTok{ Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-4-3}{%
\subsubsection{Activity 4}\label{activity-4-3}}

Activity 4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_dat<-dat2}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Condition)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(intrusions),}
            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(intrusions),}
            \DataTypeTok{se =}\NormalTok{ sd}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(intrusions)))}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-5-4}{%
\subsubsection{Activity 5}\label{activity-5-4}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Condition, }\DataTypeTok{y =}\NormalTok{ intrusions))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{width =} \FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{factorial-anova}{%
\chapter{Factorial ANOVA}\label{factorial-anova}}

For the second week of ANOVA we're going to look at an example of a factorial ANOVA. You'll learn more about interpreting these in the second lecture, but for now, we'll just focus on the code.

We're going to reproduce the analysis from Experiment 3 of \href{https://journals.sagepub.com/doi/abs/10.1177/0956797614542274}{Zhang, T., Kim, T., Brooks, A. W., Gino, F., \& Norton, M. I. (2014). A ``present'' for the future: The unexpected value of rediscovery. Psychological Science, 25, 1851-1860.}. You may remember this study from the Chapter \ref{vis} pre-class activity.

This experiment has a 2 x 2 mixed design:

\begin{itemize}
\tightlist
\item
  The first IV is time (time1, time2) and is within-subjects
\item
  The second IV is type of event (ordinary vs.~extraordinary) and is a between-subjects factor
\item
  The DV we will use is \texttt{interest}
\end{itemize}

\hypertarget{activity-1-set-up-11}{%
\subsection{Activity 1: Set-up}\label{activity-1-set-up-11}}

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 10 folder. Ensure the environment is clear.
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 11''.
\item
  Download Zhang et al.~2014 Study 3.csv and extract the files in to your Week 11 folder.
\item
  If you are working on your own computer, install the package \texttt{rcompanion}. Remember \textbf{do not install packages on university computers, they are already installed}.
\item
  Type and run the code that loads \texttt{pwr}, \texttt{rcompanion}, \texttt{lsr}, \texttt{car}, \texttt{broom}, \texttt{afex}, \texttt{emmeans} and \texttt{tidyverse} using the \texttt{library()} function.
\end{itemize}

Run the below code to load the data and wrangle it into the format we need. You don't need to write this code yourself but do make sure you can understand what each line is doing - a good way to do this when the code uses pipes (\texttt{\%\textgreater{}\%}) is to highlight and run each line progressively so you can see how it builds up. Line-by-line the code:

\begin{itemize}
\tightlist
\item
  Reads in the data file
\item
  Select the three columns we need\\
\item
  Adds on a column of subject IDs\\
\item
  Tidies the data
\item
  Recodes the values of \texttt{Condition} from numeric to text labels
\item
  Recodes the values of \texttt{time} to be easier to read/write
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factorial <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Zhang et al. 2014 Study 3.csv"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Condition, T1_Predicted_Interest_Composite, T2_Actual_Interest_Composite)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{subject =} \KeywordTok{row_number}\NormalTok{())}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ time,}
         \DataTypeTok{value =}\NormalTok{ interest,}
\NormalTok{         T1_Predicted_Interest_Composite,T2_Actual_Interest_Composite)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Condition =}\NormalTok{ dplyr}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(Condition, }\StringTok{"1"}\NormalTok{ =}\StringTok{ "Ordinary"}\NormalTok{, }\StringTok{"2"}\NormalTok{ =}\StringTok{ "Extraordinary"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{time =}\NormalTok{ dplyr}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(time, }\StringTok{"T1_Predicted_Interest_Composite"}\NormalTok{ =}\StringTok{ "time1_interest"}\NormalTok{,}
                       \StringTok{"T2_Actual_Interest_Composite"}\NormalTok{ =}\StringTok{ "time2_interest"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Condition =} \KeywordTok{as.factor}\NormalTok{(Condition))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-2-descriptive-statistics}{%
\subsection{Activity 2: Descriptive statistics}\label{activity-2-descriptive-statistics}}

\begin{itemize}
\tightlist
\item
  Calculate descriptive statistics (mean, standard error min and max, SD) for \texttt{interest} for each \texttt{Condition} for each \texttt{time} (hint: you will need to \texttt{group\_by()} two variables) and store it in an object named \texttt{sum\_dat\_factorial}. These are known as the cells means.
\item
  Hint: for the standard error code, refer back to \ref{viobox}
\end{itemize}

\hypertarget{activity-3-violin-boxplots}{%
\subsection{Activity 3: Violin-boxplots}\label{activity-3-violin-boxplots}}

We're going to produce two kinds of plots to visualise our data. First, we'll produce violin-boxplots so that we can see the distribution of our data.

\begin{itemize}
\tightlist
\item
  Write the code that produces the below violin-boxplots for the scores in each group.

  \begin{itemize}
  \tightlist
  \item
    Hint 1: you will need to add in the second IV in the first call to ggplot as a fill argument (aes(x,y,fill)).
  \item
    Hint 2: you will need to add \texttt{position\ =\ position\_dodge(.9)} to geom\_boxplot to get the plots to align.
  \end{itemize}
\end{itemize}

You don't need to replicate the exact colour scheme used below, see if you can play around with the settings to whatever colour scheme you think works best.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{14-week-11_files/figure-latex/plt1-1} 

}

\caption{Violin-boxplot by condition and time}\label{fig:plt1}
\end{figure}

\hypertarget{activity-4-interaction-plots}{%
\subsection{Activity 4: Interaction plots}\label{activity-4-interaction-plots}}

Now we're going to produce an interaction plot that makes it easier to see how the IVs are interacting, which requires some ggplot2 functions we haven't come across yet. Rather than using the raw data in \texttt{dat\_factorial}, we use the means that we produced in \texttt{sum\_dat\_factorial}. This type of plot requires two geoms, one to draw the points, and one to draw the lines that connect them.

This plot reproduces the plot used in the paper.

\begin{itemize}
\tightlist
\item
  Run the code and then play around with how it looks by changing the arguments for e.g., colour, line-type, and the theme.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sum_dat_factorial, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ time, }\DataTypeTok{y =}\NormalTok{ mean, }\DataTypeTok{group =}\NormalTok{ Condition, }\DataTypeTok{shape =}\NormalTok{ Condition)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{linetype =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Time 1"}\NormalTok{, }\StringTok{"Time 2"}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{14-week-11_files/figure-latex/plot2-1} 

}

\caption{Interaction plot}\label{fig:plot2}
\end{figure}

\hypertarget{activity-5-anova}{%
\subsection{Activity 5: ANOVA}\label{activity-5-anova}}

\begin{itemize}
\item
  Complete the below code to run the factorial ANOVA. Remember that you will need to specify both IVs and that one of them is between-subjects and one of them is within-subjects. Look up the help documentation for \texttt{aov\_ez} to find out how to do this.
\item
  Save the ANOVA model to an object called \texttt{mod\_factorial}
\item
  Use \texttt{tidy()} and save the output to an object named \texttt{factorial\_output}.
\item
  Hint: refer to pre-class Activity 6
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_factorial <-}\StringTok{ }\KeywordTok{aov_ez}\NormalTok{(}\DataTypeTok{id =} \StringTok{"NULL"}\NormalTok{,}
               \DataTypeTok{data =} \OtherTok{NULL}\NormalTok{, }
               \DataTypeTok{between =} \StringTok{"NULL"}\NormalTok{, }
               \DataTypeTok{within =} \StringTok{"NULL"}\NormalTok{,}
               \DataTypeTok{dv =} \StringTok{"NULL"}\NormalTok{, }
               \DataTypeTok{type =} \DecValTok{3}\NormalTok{,}
               \DataTypeTok{es =} \StringTok{"NULL"}\NormalTok{) }

\NormalTok{factorial_output <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

Look at the results. Remember the pre-class information about how to read p-values in scientific notation.

\begin{itemize}
\tightlist
\item
  Is the main effect of condition significant? Yes No
\item
  Is the main effect of time significant? Yes No
\item
  Is the two-way interaction significant? Yes No
\end{itemize}

\hypertarget{activity-6-assumption-checking}{%
\subsection{Activity 6: Assumption checking}\label{activity-6-assumption-checking}}

The assumptions for a factorial ANOVA are the same as the one-way ANOVA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The DV is interval or ratio data
\item
  The observations should be independent
\item
  The residuals should be normally distributed
\item
  There should be homogeneity of variance between the groups
\end{enumerate}

As before, we know assumption 2 is met from the design of the study. Assumption 1 throws up an interesting issue which is the problem of ordinal data. Ordinal data is the kind of data that comes from Likert scales and is very, very common in psychology. The problem is that ordinal data isn't interval or ratio data, there's a fixed number of values it can take (the values of the Likert scale) and you can't claim that the distance between the values is equal (is the difference between strongly agree and agree the same as the difference between agree and neutral?).

Technically, we shouldn't use an ANOVA to analyse ordinal data - \emph{but almost everyone does}. Most people would argue that if there are multiple Likert scale items that are averaged (which is the case in our study) and this averaged data is normally distributed, then it is not a problem. There is a minority (who are actually correct) that argue you should use non-parametric methods or more complicated tests such as ordinal regression for this type of data. Whichever route you choose, you should understand the data you have and you should be able to justify your decision.

\begin{itemize}
\item
  To test assumption 3, extract the residuals from the model (\texttt{mod\_factorial\$lm\$residuals}), create a qq-plot and conduct a Shapiro-Wilk test.
\item
  Are the residuals normally distributed? Yes No No, but given the sample it is probably acceptable to proceed
\end{itemize}

For the final assumption, we can again use \texttt{test\_levene()} to test homogeneity of variance.

\begin{itemize}
\tightlist
\item
  Conduct Levene's test. Is assumption 4 met? Yes No
\end{itemize}

\hypertarget{activity-7-post-hoc-tests}{%
\subsection{Activity 7: Post-hoc tests}\label{activity-7-post-hoc-tests}}

Because the interaction is significant, we should follow this up with post-hoc tests using \texttt{emmeans()} to determine which comparisons are significant. If the overall interaction is not significant, you should not conduct additional tests.

\texttt{emmeans()} requires you to specify the \texttt{aov} object, and then the factors you want to contrast. For an interaction, we use the notation \texttt{pairwise\ \textasciitilde{}\ IV1\ \textbar{}\ IV2} and you specify which multiple comparison correction you want to apply. Finally, you can use \texttt{tidy()} to tidy up the output of the contrasts and save it into a tibble.

\begin{itemize}
\tightlist
\item
  Run the below code and view the results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# run the tests}
\NormalTok{posthoc_factorial <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(mod_factorial}\OperatorTok{$}\NormalTok{aov, }
\NormalTok{                             pairwise }\OperatorTok{~}\StringTok{ }\NormalTok{time}\OperatorTok{|}\StringTok{ }\NormalTok{Condition, }
                             \DataTypeTok{adjust =} \StringTok{"bonferroni"}\NormalTok{)}

\CommentTok{# tidy up the output of the tests}
\NormalTok{contrasts_factorial <-}\StringTok{ }\NormalTok{posthoc_factorial}\OperatorTok{$}\NormalTok{contrasts }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Note that because there are two factors, we could also reverse the order of the IVs. Above, we get the results contrasting time 1 and time 2 for each condition. Instead, we could look at the difference between ordinary and extraordinary events at each time point.

\begin{itemize}
\tightlist
\item
  Run the below code and look at the output of \texttt{contrast\_factorial} and \texttt{contrasts\_factorial2} carefully making sure you understand how to interpret the results. You will find it useful to refer to the interaction plot we made earlier.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posthoc_factorial2 <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(mod_factorial}\OperatorTok{$}\NormalTok{aov, }
\NormalTok{                             pairwise }\OperatorTok{~}\StringTok{ }\NormalTok{Condition}\OperatorTok{|}\StringTok{ }\NormalTok{time, }
                             \DataTypeTok{adjust =} \StringTok{"bonferroni"}\NormalTok{) }

\NormalTok{contrasts_factorial2 <-}\StringTok{ }\NormalTok{posthoc_factorial2}\OperatorTok{$}\NormalTok{contrasts }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Because our main effects (condition and time) only have two levels, we don't need to do any post-hoc tests to determine which conditions differ from each other, however, if one of our factors had three levels then we could use \texttt{emmeans()} to calculate the contrast for the main effects, like we did for the one-way ANOVA.

Finally, to calculate effect size for the pairwise comparisons we again need to do this individually using 'cohensD()\texttt{from}lsr`.

\begin{itemize}
\tightlist
\item
  Run the below code to add on effect sizes to \texttt{contrasts\_factorial} and \texttt{contrasts\_factorial2}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d_extra_t1_t2 <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(interest }\OperatorTok{~}\StringTok{ }\NormalTok{time, }
                         \DataTypeTok{data =}\NormalTok{ (}\KeywordTok{filter}\NormalTok{(factorial, Condition }\OperatorTok{==}\StringTok{ "Extraordinary"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{droplevels}\NormalTok{())) }

\NormalTok{d_ord_t1_t2 <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(interest }\OperatorTok{~}\StringTok{ }\NormalTok{time, }
                         \DataTypeTok{data =}\NormalTok{ (}\KeywordTok{filter}\NormalTok{(factorial, Condition }\OperatorTok{==}\StringTok{ "Ordinary"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{droplevels}\NormalTok{())) }


\NormalTok{Condition_ds <-}\StringTok{ }\KeywordTok{c}\NormalTok{(d_extra_t1_t2, d_ord_t1_t2)}

\NormalTok{contrasts_factorial <-}\StringTok{ }\NormalTok{contrasts_factorial }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{eff_size =}\NormalTok{ Condition_ds)}

\NormalTok{d_time1_extra_ord <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(interest }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                         \DataTypeTok{data =}\NormalTok{ (}\KeywordTok{filter}\NormalTok{(factorial, time }\OperatorTok{==}\StringTok{ "time1_interest"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{droplevels}\NormalTok{())) }

\NormalTok{d_time2_extra_ord <-}\StringTok{ }\KeywordTok{cohensD}\NormalTok{(interest }\OperatorTok{~}\StringTok{ }\NormalTok{Condition, }
                         \DataTypeTok{data =}\NormalTok{ (}\KeywordTok{filter}\NormalTok{(factorial, time }\OperatorTok{==}\StringTok{ "time2_interest"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{droplevels}\NormalTok{()))}


\NormalTok{time_ds <-}\StringTok{ }\KeywordTok{c}\NormalTok{(d_time1_extra_ord, d_time2_extra_ord)}

\NormalTok{contrasts_factorial2 <-}\StringTok{ }\NormalTok{contrasts_factorial }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{eff_size =}\NormalTok{ time_ds)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-8-write-up}{%
\subsection{Activity 8: Write-up}\label{activity-8-write-up}}

\begin{itemize}
\tightlist
\item
  p-values of \textless{} .001 have been entered manually. There is a way to get R to produce this formatting but it's overly complicated for our purposes. If you want to push yourself, look up the \href{https://github.com/crsh/papaja}{papaja} package.
\item
  The values of partial eta-squared do not match between our analysis and those reported in the paper. I haven't figured out why this is yet - if you know, please get in touch!
\item
  We have replaced the simple effects in the main paper with our pairwise comparisons.
\end{itemize}

First we need to calculate descriptives for the main effect of time as we didn't do this earlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time_descrip <-}\StringTok{ }\NormalTok{factorial }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(time) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_interest =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{sd_interest =} \KeywordTok{sd}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

Copy and paste the below into \textbf{white-space}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{We conducted the same repeated measures ANOVA with interest as the dependent measure and again found a main effect of time, }\KeywordTok{F}\NormalTok{(}\StringTok{`}\DataTypeTok{r factorial_output$num.Df[2]}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r factorial_output$den.Df[2]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r factorial_output$statistic[2] %>% round(2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ }\FloatTok{.001}\NormalTok{, ηp2 =}\StringTok{ `}\DataTypeTok{r factorial_output$ges[2] %>% round(3)}\StringTok{`}\NormalTok{; anticipated interest at Time }\DecValTok{1}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r time_descrip$mean_interest[1] %>% round(2)}\StringTok{`}\NormalTok{), SD =}\StringTok{ `}\DataTypeTok{r time_descrip$sd_interest[1]%>% round(2)}\StringTok{`}\ErrorTok{)}\NormalTok{, }\DecValTok{95}\OperatorTok{% CI = [`r time_descrip$min[1]%}\ErrorTok{>}\OperatorTok{% round(2)`, `r time_descrip$max[1]%}\ErrorTok{>}\OperatorTok{% round(2)`]) was lower than actual interest at Time 2 (M = `r time_descrip$mean_interest[2]%}\ErrorTok{>}\OperatorTok{% round(2)`, SD = `r time_descrip$sd_interest[2]%}\ErrorTok{>}\OperatorTok{% round(2)`, 95%}\StringTok{ }\NormalTok{CI =}\StringTok{ }\NormalTok{[}\StringTok{`}\DataTypeTok{r time_descrip$min[2]%>% round(2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r time_descrip$max[2]%>% round(2)}\StringTok{`}\NormalTok{]}\ErrorTok{)}\NormalTok{.We also observed an interaction between time and type of experience, }\KeywordTok{F}\NormalTok{(}\StringTok{`}\DataTypeTok{r factorial_output$num.Df[3]}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r factorial_output$den.Df[3]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r factorial_output$statistic[3] %>% round(3)}\StringTok{`}\NormalTok{, p =}\StringTok{ `}\DataTypeTok{r factorial_output$p.value[3] %>% round(2)}\StringTok{`}\NormalTok{, ηp2 =}\StringTok{ `}\DataTypeTok{r factorial_output$ges[3] %>% round(3)}\StringTok{`}\NormalTok{. Pairwise comparisons revealed that }\ControlFlowTok{for}\NormalTok{ ordinary events, predicted interest at Time }\DecValTok{1}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r sum_dat_factorial$mean[3]%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r sum_dat_factorial$sd[3]%>% round(2)}\StringTok{`}\NormalTok{, }\DecValTok{95}\OperatorTok{% CI = [`r sum_dat_factorial$min[3]%}\ErrorTok{>}\OperatorTok{% round(2)`, `r sum_dat_factorial$max[3]%}\ErrorTok{>}\OperatorTok{% round(2)`]) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[4]%}\ErrorTok{>}\OperatorTok{% round(2)`, SD = `r sum_dat_factorial$sd[4]%}\ErrorTok{>}\OperatorTok{% round(2)`, 95%}\StringTok{ }\DataTypeTok{CI =}\NormalTok{ [}\StringTok{`}\DataTypeTok{r sum_dat_factorial$min[4]%>% round(2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r sum_dat_factorial$max[4]%>% round(2)}\StringTok{`}\NormalTok{]), }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r contrasts_factorial$df[2]%>% round(2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r contrasts_factorial$statistic[2]%>% round(2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ }\FloatTok{.001}\NormalTok{, d =}\StringTok{ `}\DataTypeTok{r contrasts_factorial$eff_size[2]%>% round(2)}\StringTok{`}\NormalTok{. Although predicted interest }\ControlFlowTok{for}\NormalTok{ extraordinary events at Time }\DecValTok{1}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r sum_dat_factorial$mean[1]%>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r sum_dat_factorial$sd[1]%>% round(2)}\StringTok{`}\NormalTok{, }\DecValTok{95}\OperatorTok{% CI = [`r sum_dat_factorial$min[1]%}\ErrorTok{>}\OperatorTok{% round(2)`, `r sum_dat_factorial$max[1]%}\ErrorTok{>}\OperatorTok{% round(2)`]) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[2]%}\ErrorTok{>}\OperatorTok{% round(2)`, SD = `r sum_dat_factorial$sd[2]%}\ErrorTok{>}\OperatorTok{% round(2)`, 95%}\StringTok{ }\DataTypeTok{CI =}\NormalTok{ [}\StringTok{`}\DataTypeTok{r sum_dat_factorial$min[2]%>% round(2)}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r sum_dat_factorial$max[2]%>% round(2)}\StringTok{`}\NormalTok{), }\KeywordTok{t}\NormalTok{(}\StringTok{`}\DataTypeTok{r contrasts_factorial$df[1]%>% round(2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r contrasts_factorial$statistic[1]%>% round(2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ }\FloatTok{.001}\NormalTok{, d =}\StringTok{ `}\DataTypeTok{r contrasts_factorial$eff_size[1]%>% round(2)}\StringTok{`}\NormalTok{ , the magnitude of underestimation was smaller than }\ControlFlowTok{for}\NormalTok{ ordinary events.}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We conducted the same repeated measures ANOVA with interest as the dependent measure and again found a main effect of time, F(1, 128) = 25.88, p \textless{} .001, ηp2 = 0.044; anticipated interest at Time 1 (M = 4.2), SD = 1.12), 95\% CI = {[}4.01, 4.4{]}) was lower than actual interest at Time 2 (M = 4.69, SD = 1.19, 95\% CI = {[}4.49, 4.9{]}).We also observed an interaction between time and type of experience, F(1, 128) = 4.445, p = 0.04, ηp2 = 0.008. Pairwise comparisons revealed that for ordinary events, predicted interest at Time 1 (M = 4.04, SD = 1.09, 95\% CI = {[}3.9, 4.18{]}) was lower than experienced interest at Time 2 (M = 4.73, SD = 1.24, 95\% CI = {[}4.58, 4.89{]}), t(128) = -5.05, p \textless{} .001, d = 0.59. Although predicted interest for extraordinary events at Time 1 (M = 4.36, SD = 1.13, 95\% CI = {[}4.22, 4.5{]}) was lower than experienced interest at Time 2 (M = 4.65, SD = 1.14, 95\% CI = {[}4.51, 4.79), t(128) = -2.12, p \textless{} .001, d = 0.25 , the magnitude of underestimation was smaller than for ordinary events.
\end{quote}

\hypertarget{activity-9-transforming-data}{%
\subsection{Activity 9: Transforming data}\label{activity-9-transforming-data}}

In this chapter we decided that the violation of the assumption of normality was ok so that we could replicate the results in the paper. But what if we weren't happy with this or if the violation had been more extreme? One option to deal with normality is to \textbf{transform your data}. If you want more information on this you should consult Chapter \ref{refsup}.

There are various options for how you can transform data but we're going to use Tukeys Ladder of Powers transformation. This finds the power transformation that makes the data fit the normal distribution as closely as possible with this type of transformation.

\begin{itemize}
\tightlist
\item
  Run the below code. This will use \texttt{mutate()} to add a new variable to the data-set, \texttt{interest\_tukey} which is going to be our transformed DV. The function \texttt{transformTukey()} is from the \texttt{rcompanion} package. Setting \texttt{plotit\ =\ TRUE} will automatically create qqPlots and histograms so that we can immediately visualise the new variable.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factorial <-}\StringTok{ }\NormalTok{factorial }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{interest_tukey =} \KeywordTok{transformTukey}\NormalTok{(interest, }\DataTypeTok{plotit=}\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now that you've transformed the DV we can re-run the ANOVA with this new variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tukey_factorial <-}\StringTok{ }\KeywordTok{aov_ez}\NormalTok{(}\DataTypeTok{id =} \StringTok{"subject"}\NormalTok{,}
               \DataTypeTok{data =}\NormalTok{ factorial, }
               \DataTypeTok{between =} \StringTok{"Condition"}\NormalTok{, }
               \DataTypeTok{within =} \StringTok{"time"}\NormalTok{,}
               \DataTypeTok{dv =} \StringTok{"interest_tukey"}\NormalTok{, }
               \DataTypeTok{type =} \DecValTok{3}\NormalTok{)}

\NormalTok{tukey_factorial}
\end{Highlighting}
\end{Shaded}

Notice that doing this hasn't changed the pattern of the ANOVA results, the p-values for the main effects and interactions are very slightly different but the overall conclusions remain the same. This is likely because the violations of normality was quite mild and there is a large sample size, however, with the transformation we can be more confident in our results and it may not always be the case that the transformed ANOVA is the same if the violations were more extreme.

\hypertarget{finished-9}{%
\subsubsection{Finished!}\label{finished-9}}

And we're done! There's only one more week of R left. I know that for some of you, you will be breathing a sigh of relief but we really want you to reflect on just how far you've come and the skills that you've learned. Even if you don't continue with quantitative research, your understanding of how data is manipulated and how the final results of journal articles actually happen will have substantially increased and that level of critical awareness is an all-round good skill to have.

\hypertarget{activity-solutions-8}{%
\subsection{Activity solutions}\label{activity-solutions-8}}

\hypertarget{activity-1-6}{%
\subsubsection{Activity 1}\label{activity-1-6}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pwr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rcompanion"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"afex"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-2-4}{%
\subsubsection{Activity 2}\label{activity-2-4}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_dat_factorial<-factorial}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Condition, time)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(interest) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(interest)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(interest)}
\NormalTok{            )}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-3-4}{%
\subsubsection{Activity 3}\label{activity-3-4}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(factorial, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ time , }\DataTypeTok{y =}\NormalTok{ interest, }\DataTypeTok{fill =}\NormalTok{ Condition))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{trim =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.6}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(.}\DecValTok{9}\NormalTok{), }\DataTypeTok{width =} \FloatTok{.2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.7}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Time 1"}\NormalTok{, }\StringTok{"Time 2"}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_d}\NormalTok{(}\DataTypeTok{option =} \StringTok{"E"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ sum_dat_factorial,}
                  \KeywordTok{aes}\NormalTok{(time, mean, }\DataTypeTok{ymin=}\NormalTok{min, }\DataTypeTok{ymax=}\NormalTok{max),}
                  \DataTypeTok{shape =} \DecValTok{20}\NormalTok{, }
                  \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-5-5}{%
\subsubsection{Activity 5}\label{activity-5-5}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_factorial <-}\StringTok{ }\KeywordTok{aov_ez}\NormalTok{(}\DataTypeTok{id =} \StringTok{"subject"}\NormalTok{,}
               \DataTypeTok{data =}\NormalTok{ factorial, }
               \DataTypeTok{between =} \StringTok{"Condition"}\NormalTok{, }
               \DataTypeTok{within =} \StringTok{"time"}\NormalTok{,}
               \DataTypeTok{dv =} \StringTok{"interest"}\NormalTok{, }
               \DataTypeTok{type =} \DecValTok{3}\NormalTok{) }

\NormalTok{factorial_output <-}\StringTok{ }\KeywordTok{anova}\NormalTok{(mod_factorial) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-6-3}{%
\subsubsection{Activity 6}\label{activity-6-3}}

Activity 6

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# normality testing}
\KeywordTok{qqPlot}\NormalTok{(mod_factorial}\OperatorTok{$}\NormalTok{lm}\OperatorTok{$}\NormalTok{residuals)}
\KeywordTok{shapiro.test}\NormalTok{(mod_factorial}\OperatorTok{$}\NormalTok{lm}\OperatorTok{$}\NormalTok{residuals)}

\CommentTok{# levene's test}
\KeywordTok{test_levene}\NormalTok{(mod_factorial)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{regression}{%
\chapter{Regression}\label{regression}}

In this activity, you will be working with real data and using regression to explore the question of whether there is a relationship between statistics anxiety and engagement in course activities. The hypothesis is that students who are more anxious about statistics are less likely to engage in course-related activities. This avoidance behaviour could ultimately be responsible for lower performance for these students (although we won't be examining the assessment scores in this activity).

We are going to analyse data from the STARS Statistics Anxiety Survey, which was administered to students in the third-year statistics course in Psychology at the University of Glasgow. All the responses have been anonymised by associating the responses for each student with an arbitrary ID number (integer).

The STARS survey (Cruise, Cash, \& Bolton, 1985) is a 51-item questionnaire, with each response on a 1 to 5 scale, with higher numbers indicating greater anxiety.

Cruise, R. J., Cash, R. W., \& Bolton, D. L. (1985). Development and validation of an instrument to measure statistical anxiety. \emph{Proceedings of the American Statistical Association, Section on Statistical Education}, Las Vegas, NV.

\begin{figure}
\centering
\includegraphics{images/stars_survey.png}
\caption{\emph{Example items from the STARS survey (Cruise, Cash, \& Bolton, 1985)}}
\end{figure}

As a measure of engagement in the course, we will use data from Moodle usage analytics. Over the course of the term, there were eight optional weekly on-line sessions that students could attend for extra support. The variable \texttt{n\_weeks} in the \texttt{psess.csv} file tells you how many (out of eight) a given student attended.

Our hypothesis was that greater anxiety would be reflected in lower engagement. Answer the following question.

If our hypothesis is correct then there should be a positive no a negative correlation between students' mean anxiety levels and \texttt{n\_weeks}.

\hypertarget{activity-1-setup}{%
\section{Activity 1: Setup}\label{activity-1-setup}}

Do the following. If you need help, consult Chapter @\ref(ref3) and Chapter @(ref2).

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 12 folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 12''.\\
\item
  Download L3\_stars.csv and psess.csv and save them in your Week 12 folder. Make sure that you do not change the file name at all.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads \texttt{pwr}, \texttt{car}, \texttt{broom}, and \texttt{tidyverse} using the \texttt{library()} function.
\item
  Load the two CSV datasets into variables called \texttt{stars} and \texttt{engage} using \texttt{read\_csv()}.
\end{itemize}

\hypertarget{activity-2-tidy-the-data}{%
\section{Activity 2: Tidy the data}\label{activity-2-tidy-the-data}}

\begin{itemize}
\tightlist
\item
  Take a look at both of the datasets you loaded in (you can use just type the name to output them out to the console).
\end{itemize}

The next thing we need to do is to calculate a mean anxiety score for each student (recall that individual students are identified by the \texttt{ID} variable).

Recall the difference between \emph{wide} and \emph{tidy} data. In wide data, each row represents an individual case, with observations for that case in separate columns; in tidy data, each row represents a single observation, and the observations are grouped together into cases based on the value of a variable (for these data, the \texttt{ID} variable).

\begin{itemize}
\tightlist
\item
  The STARS data are currently in wide tidy format.
\end{itemize}

Before we calculate means, you need to use \texttt{gather()} to restructure the STARS data into the appropriate ``tidy'' format; i.e., so that it looks like the table below.

\begin{tabular}{c|c|c}
\hline
ID & Question & Score\\
\hline
3 & Q01 & 1\\
\hline
3 & Q02 & 1\\
\hline
3 & Q03 & 1\\
\hline
3 & Q04 & 1\\
\hline
3 & Q05 & 1\\
\hline
3 & Q06 & 1\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Write and run the code to do tidy the STARS data, and store the resulting table as \texttt{stars2}.
\end{itemize}

\hypertarget{activity-3-calculate-mean-anxiety-for-each-student}{%
\section{Activity 3: Calculate mean anxiety for each student}\label{activity-3-calculate-mean-anxiety-for-each-student}}

\begin{itemize}
\tightlist
\item
  Now that you've got the data into a tidy format, use \texttt{summarise()} and \texttt{group\_by()} to calculate mean anxiety scores (\texttt{mean\_anxiety}) for each student (\texttt{ID}). Store the resulting table in a variable named \texttt{stars\_means}.
\end{itemize}

\hypertarget{activity-4-join-the-datasets-together}{%
\section{Activity 4: Join the datasets together}\label{activity-4-join-the-datasets-together}}

\begin{itemize}
\tightlist
\item
  In order to perform the regression analysis, combine the data from \texttt{stars\_means} with \texttt{engage} using \texttt{inner\_join()}. Call the resulting table \texttt{joined}. It should look like this:
\end{itemize}

\hypertarget{activity-5-calculate-descriptives-for-the-variables-overall}{%
\section{Activity 5: Calculate descriptives for the variables overall}\label{activity-5-calculate-descriptives-for-the-variables-overall}}

It is also useful to calculate descriptives statistics for the sample overall so that you can check that the sample scores are what you were expecting (e.g., are they comparable to previous studies and samples?). This is also useful for the write-up.

\begin{itemize}
\tightlist
\item
  Run the below code. Read each line and ensure you understand what is being calculated.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{descriptives <-}\StringTok{ }\NormalTok{joined }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_anx =} \KeywordTok{mean}\NormalTok{(mean_anxiety, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min_anx =} \KeywordTok{mean}\NormalTok{(mean_anxiety,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(mean_anxiety,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }
            \DataTypeTok{max_anx =} \KeywordTok{mean}\NormalTok{(mean_anxiety,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(mean_anxiety,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd_anx =} \KeywordTok{sd}\NormalTok{(mean_anxiety, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{mean_weeks =} \KeywordTok{mean}\NormalTok{(n_weeks, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min_weeks =} \KeywordTok{mean}\NormalTok{(n_weeks) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(n_weeks,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }
            \DataTypeTok{max_weeks =} \KeywordTok{mean}\NormalTok{(n_weeks) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(n_weeks,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd_weeks =} \KeywordTok{sd}\NormalTok{(n_weeks, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-6-visualisations}{%
\section{Activity 6: Visualisations}\label{activity-6-visualisations}}

\begin{itemize}
\tightlist
\item
  Now that youwe have all of the variables in one place, write the code to reproduce the exact scatterplot below (using ggplot2).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{15-week-12_files/figure-latex/scatter-1} 

}

\caption{Scatteplot of mean anxiety and attendance}\label{fig:scatter}
\end{figure}

\begin{itemize}
\tightlist
\item
  According to the scatterplot, there is no apparent relationship as anxiety increases, engagement decreases as anxiety increases, engagement increases
\end{itemize}

\hypertarget{activity-7-run-the-regression}{%
\section{Activity 7: Run the regression}\label{activity-7-run-the-regression}}

The \texttt{lm()} function from Base R is the main function to estimate a \emph{L}inear \emph{M}odel (hence the function name \texttt{lm}). \texttt{lm()} uses formula syntax that you have seen before, i.e., \texttt{DV\ \textasciitilde{}\ predictor}.

\begin{itemize}
\tightlist
\item
  Use the \texttt{lm()} function to predict \texttt{n\_weeks} (DV) from \texttt{mean\_anxiety} (predictor). Store the result of the call to \texttt{lm()} in the variable \texttt{mod}. To see the results, use \texttt{summary(mod)}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(n_weeks }\OperatorTok{~}\StringTok{ }\NormalTok{mean_anxiety, joined)}
\NormalTok{mod_summary <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

Answer the following questions about the model. You may wish to refer to the lecture notes to help you answer these questions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The estimate of the y-intercept for the model, rounded to three decimal places, is 
\item
  To three decimal places, if the GLM for this model is \(Y_i = \beta_0 + \beta_1 X_i + e_i\), then \(\beta_1\) is 
\item
  To three decimal places, for each unit increase in anxiety, \texttt{n\_weeks} decreases by 
\item
  To two decimal places, what is the overall F-ratio of the model? 
\item
  Is the overall model significant? Yes No
\item
  What proportion of the variance does the model explain? 
\end{enumerate}

Explain these answers

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the summary table, this is the estimate of the intercept.
\item
  In the summary table, this is the estimate of mean\_anxiety, i.e., the slope.
\item
  In the summary table, this is also the estimate of mean\_anxiety, the slope is how much it decreases so you just remove the - sign.
\item
  In the summary table, the F-ratio is noted as he F-statistic.
\item
  The overall model p.value is .001428 which is less than .05, therefore significant.
\item
  The variance explained is determined by R-squared, you simply multiple it by 100 to get the percent. You should always use the adjusted R-squared value.
\end{enumerate}

\hypertarget{activity-8-assumption-checking}{%
\section{Activity 8: Assumption checking}\label{activity-8-assumption-checking}}

It's now time to check the assumptions, which for regression are a little bit more involved than they were for ANOVA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The outcome/DV is a interval/ratio level data
\item
  The predictor variable is interval/ratio or categorical (with two levels)
\item
  All values of the outcome variable are independent (i.e., each score should come from a different participant)
\item
  The predictors have non-zero variance
\item
  The relationship between outcome and predictor is linear
\item
  The residuals should be normally distributed
\item
  There should be homoscedasticity (homogeneity of variance, but for the residuals)
\end{enumerate}

Assumptions 1-3 are nice and easy. We know this from the data we have and the design of the study. Assumption 4 simply means that there is some spread in the data - for example, there's no point running a regression with age as a variable if all your participants are 20 years old. We can check this using the scatterplot we created in Activity 4 and we can see that this assumption is met, we do indeed have a spread of scores.

Assumption 5 could also be checked with the scatterplot but there are some nice regression-specific functions from \texttt{car} that we can use.

\begin{itemize}
\tightlist
\item
  Run the below code. It will produce the scatterplot with a linear line and the line that best fits the data. If these two lines are quite similar (they will never be perfect) then you can assume linearity.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{crPlots}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

Assumption 6 can be tested as we have done before with a qqplot and a Shapiro-Wilk test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqPlot}\NormalTok{(mod}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{15-week-12_files/figure-latex/normality-1} 

}

\caption{qqplot for residuals}\label{fig:normality}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(mod}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13 11
## 
##  Shapiro-Wilk normality test
## 
## data:  mod$residuals
## W = 0.91652, p-value = 0.008764
\end{verbatim}

The qqplot doesn't look too bad, on the basis of this alone we would probably assume normality. However, the Shapiro-Wilk test is significant which suggests something isn't quite right. The Shaprio-Wilk has been criticised for rejecting the null hypothesis (i.e., concluding that the data are not normal) too often and it's argued that if the qqplot and the Shapiro test disagree, it's better to rely on the qqplot. There is a good discussion \href{https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless}{about it here} if you'd like to know more. For now, we will proceed cautiously and check the final assumption before we make a decision about what to do.

The final assumption of homoscedasticity means that the error in the model is fairly constant at all points (i.e., if you looked at the scatterplot, the data points wouldn't be bunched together at one end and spread out at the other). We can test this using the non-constant error variance test from \texttt{car}.

\begin{itemize}
\tightlist
\item
  Run the below code. If the test is significant, the assumption has been violated. Based upon the results, can we assume homoscedasticity? Yes No
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ncvTest}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

Returning to the issue of normality, all of the other assumptions have been met and the qqplot suggests that any deviation from normality is very mild. Based upon this evidence, it seems justified to proceed with the original regression.

\hypertarget{activity-9-power-and-effect-size-1}{%
\section{Activity 9: Power and effect size}\label{activity-9-power-and-effect-size-1}}

First we can calculate the minimum effect size we were able to detect given the sample size and design of the study using \texttt{pwr.f2.test()}. As usual, we fill in all the information we have and set the effect size argument, in this case \texttt{f2}, to \texttt{NULL}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.f2.test}\NormalTok{(}\DataTypeTok{u =} \DecValTok{1}\NormalTok{, }\DataTypeTok{v =} \DecValTok{35}\NormalTok{, }\DataTypeTok{f2 =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }\DataTypeTok{power =} \FloatTok{.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Based on the power analysis, what is the minimum effect size we were able to detect rounded to 2 decimal places? \\
\item
  According to Cohen's guidelines, this would be a Small Medium Large effect.
\end{itemize}

There is no formula to calculate our observed f\textsuperscript{2}, we must do it manually using the formula from the lecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f2 <-}\StringTok{ }\NormalTok{mod_summary}\OperatorTok{$}\NormalTok{adj.r.squared}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{mod_summary}\OperatorTok{$}\NormalTok{adj.r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Is the observed effect size larger than the minimum effect size we could detect? Yes, our study is sufficiently powered No, our study is underpowered
\end{itemize}

\hypertarget{activity-10-write-up-1}{%
\section{Activity 10: Write-up}\label{activity-10-write-up-1}}

We need to manually calculate the p-value for the inline coding as you can't extract it from the \texttt{lm()} model. Run the below code to do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-mod_summary}\OperatorTok{$}\NormalTok{fstatistic}
\NormalTok{mod_p <-}\StringTok{ }\KeywordTok{pf}\NormalTok{(f[}\DecValTok{1}\NormalTok{], f[}\DecValTok{2}\NormalTok{], f[}\DecValTok{3}\NormalTok{], }\DataTypeTok{lower=}\OtherTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Now, copy and paste the below code into \textbf{white-space} and knit the document.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A simple linear regression was performed with }\KeywordTok{engagment}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r descriptives$mean_weeks %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r descriptives$sd_anx %>% round(2)}\StringTok{`}\NormalTok{) as the outcome variable and statistics }\KeywordTok{anxiety}\NormalTok{ (}\DataTypeTok{M =} \StringTok{`}\DataTypeTok{r descriptives$mean_anx %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{SD =} \StringTok{`}\DataTypeTok{r descriptives$sd_anx %>% round(2)}\StringTok{`}\NormalTok{) as the predictor variable. The results of the regression indicated that the model significantly predicted course }\KeywordTok{engagement}\NormalTok{ (}\KeywordTok{F}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_summary$fstatistic[2]}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r mod_summary$fstatistic[3]}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_summary$fstatistic[1] %>% round(2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ }\FloatTok{.001}\NormalTok{, Adjusted }\DataTypeTok{R2 =} \StringTok{`}\DataTypeTok{r mod_summary$adj.r.squared %>% round(2)}\StringTok{`}\NormalTok{, }\DataTypeTok{f2 =} \FloatTok{.63}\NormalTok{), accounting }\ControlFlowTok{for} \StringTok{`}\DataTypeTok{r (mod_summary$adj.r.squared %>% round(2))*100}\StringTok{`}\OperatorTok{% of the variance. Anxiety was a significant positive predictor (β = `r mod$coefficients[2] %}\ErrorTok{>}\OperatorTok{% round(2)`, p < `mod_p %}\ErrorTok{>}\NormalTok{% }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)}\StringTok{`}\DataTypeTok{.}
\DataTypeTok{)}
\end{Highlighting}
\end{Shaded}

A simple linear regression was performed with engagement (M = 4.54, SD = 0.56) as the outcome variable and statistics anxiety (M = 2.08, SD = 0.56) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(1, 35) = 11.99, p \textless{} .001, Adjusted R2 = 0.23, f\textsuperscript{2} = .63), accounting for 23\% of the variance. Anxiety was a significant positive predictor (β = -2.17, p \textless{} 0.001.
)

\hypertarget{activity-solutions-9}{%
\section{Activity solutions}\label{activity-solutions-9}}

\hypertarget{activity-1-7}{%
\subsection{Activity 1}\label{activity-1-7}}

Activity 1

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pwr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rcompanion"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"afex"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-2-5}{%
\subsection{Activity 2}\label{activity-2-5}}

Activity 2

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pwr"}\NormalTok{)}
\NormalTok{stars2 <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(stars, }\StringTok{"Question"}\NormalTok{, }\StringTok{"Score"}\NormalTok{, Q01}\OperatorTok{:}\NormalTok{Q51) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(ID)}
\KeywordTok{head}\NormalTok{(stars2) }\OperatorTok{%>%}\StringTok{ }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{align =} \StringTok{'c'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-3-5}{%
\subsection{Activity 3}\label{activity-3-5}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stars_means <-}\StringTok{ }\NormalTok{stars2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(ID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_anxiety =} \KeywordTok{mean}\NormalTok{(Score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{min =} \KeywordTok{mean}\NormalTok{(Score) }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(Score)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()), }
            \DataTypeTok{max =} \KeywordTok{mean}\NormalTok{(Score) }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(Score)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{n}\NormalTok{()),}
            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(Score))}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-4-4}{%
\subsection{Activity 4}\label{activity-4-4}}

Activity 4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{joined <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(stars_means, engage, }\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{activity-5-6}{%
\subsection{Activity 5}\label{activity-5-6}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(joined, }\KeywordTok{aes}\NormalTok{(mean_anxiety, n_weeks)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

** Click tab to see solution **

\hypertarget{multiple-regression}{%
\chapter{Multiple regression}\label{multiple-regression}}

There is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. We'll be looking at data from this recent study of English adolescents:

\begin{quote}
Przybylski, A. \& Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. \emph{Psychological Science}, \emph{28}, 204--215.
\end{quote}

This was a large-scale study that found support for the ``Goldilocks'' hypothesis among adolescents: that there is a ``just right'' amount of screen time, such that any amount more or less than this amount is associated with lower well-being. This was a huge survey study: the data contain responses from over 120,000 participants!

Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. In this exercise, we will look at whether the relationship between screen time and well-being is modulated by participants' (self-reported) gender.

The dependent measure used in the study was the \href{https://warwick.ac.uk/fac/med/research/platform/wemwbs/}{Warwick-Edinburgh Mental Well-Being Scale (WEMWBS)}. This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70.

At \href{https://osf.io/82ybd/}{Przybylski \& Weinstein's page for this study on the Open Science Framework}, you can find the \href{https://osf.io/82ybd/}{participant survey} which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen time). Within the same page you can also find the \href{https://osf.io/82ybd/}{raw data}; however, for the purpose of this exercise, you will be using local pre-processed copies of the data which can be downloaded from Moodle.

Przybylski and Weinstein looked at multiple measures of screen time, but we will be focusing on smartphone use. They found that decrements in well-being started to appear when respondents reported more than one hour of weekly smartphone use. Our question: Does the negative association between hours of use and well-being (beyond the one-hour point) differ for boys and girls?

Note that in this analysis, we have:

\begin{itemize}
\item
  a continuous\(^*\) DV, well-being;
\item
  a continuous\(^*\) predictor, screen time;
\item
  a categorical predictor, gender.
\end{itemize}

\(^*\)these variables are only quasi-continuous, inasmuch as only discrete values are possible. However, there are a sufficient number of discrete categories that we can treat them as effectively continuous.

What we want to do is to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. So this problem seems simultaneously like a situation where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups).

But the expressive power of regression allows us to do this all within a single model. As the \href{http://deevybee.blogspot.com/2017/11/anova-t-tests-and-regression-different.html}{Bishop blog showed}, \emph{an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical.} So although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. Regression allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors.

\hypertarget{activity-1-set-up-12}{%
\section{Activity 1: Set-up}\label{activity-1-set-up-12}}

\begin{itemize}
\tightlist
\item
  Open R Studio and set the working directory to your Week 13 folder. Ensure the environment is clear.\\
\item
  Open a new R Markdown document and save it in your working directory. Call the file ``Week 13''.\\
\item
  Download wellbeing.csv, participant\_info.csv and screen\_time.csv and save them in your Week 13 folder. Make sure that you do not change the file names at all.\\
\item
  Delete the default R Markdown welcome text and insert a new code chunk that loads \texttt{pwr}, \texttt{car}, \texttt{broom}, and \texttt{tidyverse} using the \texttt{library()} function.
\item
  Load the CSV datasets into variables called \texttt{pinfo}, \texttt{wellbeing} and \texttt{screen} using \texttt{read\_csv()}.
\end{itemize}

\hypertarget{activity-2-look-at-the-data-3}{%
\section{Activity 2: Look at the data}\label{activity-2-look-at-the-data-3}}

Take a look at the resulting tibbles \texttt{pinfo}, \texttt{wellbeing}, and \texttt{screen}. The \texttt{wellbeing} tibble has information from the WEMWBS questionnaire; \texttt{screen} has information about screen time use on weekends (variables ending with \texttt{we}) and weekdays (variables ending with \texttt{wk}) for four types of activities: using a computer (variables starting with \texttt{Comph}; Q10 on the survey), playing video games (variables starting with \texttt{Comp}; Q9 on the survey), using a smartphone (variables starting with \texttt{Smart}; Q11 on the survey) and watching TV (variables starting with \texttt{Watch}; Q8 on the survey). If you want more information about these variables, look at the items 8-11 on pages 4-5 of the the \href{https://osf.io/82ybd/}{PDF version of the survey on the OSF website}.

\begin{itemize}
\item
  The variable corresponding to \emph{gender} is located in the table named pinfo wellbeing screen and this variable is called .
\item
  The WEMWBS data is in long wide format, and contains observations from participants on items.
\item
  Individual participants in this dataset are identified by the variable named {[}be sure to type the name \emph{exactly}, including capitalization{]}. This variable will allow us to link information across the three tables.
\item
  Run \texttt{summary()} on the three data-sets. Are there any missing data points? Yes No
\end{itemize}

\hypertarget{activity-3-compute-the-well-being-score-for-each-respondent}{%
\section{Activity 3: Compute the well-being score for each respondent}\label{activity-3-compute-the-well-being-score-for-each-respondent}}

The WEMWBS well-being score is simply the \emph{sum} of all the items.

\begin{itemize}
\tightlist
\item
  Write the code to create a new table called \texttt{wemwbs}, with two variables: \texttt{Serial}, and \texttt{tot\_wellbeing}, the total WEMWBS score.
\end{itemize}

Hint

\begin{itemize}
\tightlist
\item
  ``gather'' the table from wide to long
\end{itemize}

Another Hint

\begin{itemize}
\tightlist
\item
  \texttt{group\_by()}; \texttt{summarise(tot\_wellbeing\ =\ ...)}
\end{itemize}

\textbf{Sanity check:} Verify for yourself that the scores all fall in the 14-70 range. Przybylski and Weinstein reported a mean of 47.52 with a standard deviation of 9.55. Can you reproduce these values?

Hint

\begin{itemize}
\tightlist
\item
  \texttt{summarise()}, \texttt{min()}, \texttt{max()}
\end{itemize}

* Now visualise the distribution of \texttt{tot\_wellbeing} in a histogram using ggplot2.

Hint

\begin{itemize}
\tightlist
\item
  \texttt{geom\_histogram()}
\end{itemize}

Solution

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(wemwbs, }\KeywordTok{aes}\NormalTok{(tot_wellbeing)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=1\linewidth]{16-week-13_files/figure-latex/wemwbs_histogram-1}

\}

\caption{**CAPTION THIS FIGURE!!**}

(\#fig:wemwbs\_histogram)
\textbackslash{}end\{figure\}

The distribution of well-being scores is symmetric negatively skewed positively skewed.

\hypertarget{activity-4-visualise-the-relationship}{%
\section{Activity 4: Visualise the relationship}\label{activity-4-visualise-the-relationship}}

Let's take a quick look at the relationship between screen time (for the four different technologies) and measures of well-being. Here is code to do this.

\begin{itemize}
\tightlist
\item
  Run the below code and try and explain in words what each line of code is doing (remember, pronounce \texttt{\%\textgreater{}\%} as ``and then''). You may find it easier to look at each of the tables that are produced.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{screen_long <-}\StringTok{ }\NormalTok{screen }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"var"}\NormalTok{, }\StringTok{"hours"}\NormalTok{, }\OperatorTok{-}\NormalTok{Serial) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(var, }\KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{, }\StringTok{"day"}\NormalTok{), }\StringTok{"_"}\NormalTok{)}

\NormalTok{screen2 <-}\StringTok{ }\NormalTok{screen_long }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable =}\NormalTok{ dplyr}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(variable,}
               \StringTok{"Watch"}\NormalTok{ =}\StringTok{ "Watching TV"}\NormalTok{,}
               \StringTok{"Comp"}\NormalTok{ =}\StringTok{ "Playing Video Games"}\NormalTok{,}
               \StringTok{"Comph"}\NormalTok{ =}\StringTok{ "Using Computers"}\NormalTok{,}
               \StringTok{"Smart"}\NormalTok{ =}\StringTok{ "Using Smartphone"}\NormalTok{),}
     \DataTypeTok{day =}\NormalTok{ dplyr}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(day,}
              \StringTok{"wk"}\NormalTok{ =}\StringTok{ "Weekday"}\NormalTok{,}
              \StringTok{"we"}\NormalTok{ =}\StringTok{ "Weekend"}\NormalTok{))}

\NormalTok{dat_means <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(wemwbs, screen2, }\StringTok{"Serial"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(variable, day, hours) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_wellbeing =} \KeywordTok{mean}\NormalTok{(tot_wellbeing))}

\KeywordTok{ggplot}\NormalTok{(dat_means, }\KeywordTok{aes}\NormalTok{(hours, mean_wellbeing, }\DataTypeTok{linetype =}\NormalTok{ day)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{variable, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{16-week-13_files/figure-latex/combined-1} 

}

\caption{Relationship between wellbeing and screentime usage by technology and weekday}\label{fig:combined}
\end{figure}

The graph makes it evident that smartphone use of more than 1 hour per day is associated with increasingly negative well-being. Note that we have combined the tables using an \texttt{inner\_join()}, such that we only include data for which we have observations across the \texttt{wemwbs} and \texttt{screen2} tables.

In the next step, we are going to focus in on the smartphone/well-being relationship.

\hypertarget{activity-5-smartphone-and-well-being-for-boys-and-girls}{%
\section{Activity 5: Smartphone and well-being for boys and girls}\label{activity-5-smartphone-and-well-being-for-boys-and-girls}}

For this analysis, we are going to collapse weekday and weekend use for smartphones.

\begin{itemize}
\tightlist
\item
  Create a new table, \texttt{smarttot}, that has the that has mean number of hours per day of smartphone use for each participant, averaged over weekends/weekdays.
\item
  You will need to filter the dataset to only include smartphone use and not other technologies.
\item
  You will also need to group the results by the participant ID (i.e., \texttt{serial}).
\item
  The final data-set should have two variables: \texttt{Serial} (the participant) and \texttt{hours\_per\_day}.
\item
  You will need to use the data-set \texttt{screen2} to do this.
\end{itemize}

\begin{itemize}
\tightlist
\item
  Next, create a new tibble called \texttt{smart\_wb} that only includes (filters) participants from \texttt{smarttot} who used a smartphone for more than one hour per day each week, and then combine (join) this table with the information in \texttt{wemwbs} and \texttt{pinfo}.**
\end{itemize}

\hypertarget{activity-6-mean-centering-variables}{%
\section{Activity 6: Mean-centering variables}\label{activity-6-mean-centering-variables}}

As discussed in the lecture, When you have continuous variables in a regression, it is often sensible to transform them by \emph{mean centering}. You mean center a predictor \texttt{X} simply by subtracting the mean (\texttt{X\_centered\ =\ X\ -\ mean(X)}). This has two useful consequences:

\begin{itemize}
\item
  the model intercept reflects the prediction for \(Y\) at the mean value of the predictor variable, rather than at the zero value of the unscaled variable;
\item
  if there are interactions in the model, any lower-order effects can be given the same interpretation as they receive in ANOVA (main effects, rather than simple effects).
\end{itemize}

For categorical predictors with two levels, these become coded as -.5 and .5 (because the mean of these two values is 0).

\begin{itemize}
\tightlist
\item
  Use \texttt{mutate} to add two new variables to \texttt{smart\_wb}: \texttt{tothours\_c}, calculated as a mean-centered version of the \texttt{tothours} predictor; and \texttt{male\_c}, recoded as -.5 for female and .5 for male.
\item
  To create \texttt{male\_c} you will need to use \texttt{if\_else(male\ ==\ 1,\ .5,\ -.5)} You can read this code as ``if the variable \texttt{male} equals 1, recode it as .5, if not, recode it as -.5''.
\item
  Finally, recode \texttt{male} and \texttt{male\_c} as factors, so that R knows not to treat them as a real numbers.
\end{itemize}

\hypertarget{activity-7-visualise-the-relationship}{%
\section{Activity 7: Visualise the relationship}\label{activity-7-visualise-the-relationship}}

\begin{itemize}
\tightlist
\item
  Reverse-engineer the below plot. Calculate mean well-being scores for each combination of \texttt{male} and \texttt{tothours}, and then create a scatterplot plot that includes separate regression lines for each gender.
\item
  You may find it useful to refer to Chapter \ref{vis}.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{16-week-13_files/figure-latex/plots-1} 

}

\caption{Relationship between mean wellbeing and smartphone use by gender}\label{fig:plots}
\end{figure}

Write an interpretation of the above plot in plain English.

Possible solution

Girls show lower overall well-being compared to boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls.

\hypertarget{activity-8-running-the-regression}{%
\section{Activity 8: Running the regression}\label{activity-8-running-the-regression}}

Now we're going to see if there is statistical support for our above interpretation of the graph.

For the data in \texttt{smart\_wb}, use the \texttt{lm()} function to calculate the multiple regression model:

\(Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + e_i\)

where

\begin{itemize}
\tightlist
\item
  \(Y_i\) is the well-being score for participant \(i\);
\item
  \(X_{1i}\) is the mean-centered smartphone use variable for participant \(i\);
\item
  \(X_{2i}\) is gender (-.5 = female, .5 = male);
\item
  \(X_{3i}\) is the interaction between smartphone use and gender (\(= X_{1i} \times X_{2i}\))
\end{itemize}

Then use \texttt{summary()} to view the results and store this in an object called \texttt{mod\_summary()}.

Hint

\begin{itemize}
\tightlist
\item
  R formulas look like this: \texttt{y\ \textasciitilde{}\ a\ +\ b\ +\ a:b} where \texttt{a:b} means interaction
\end{itemize}

\begin{itemize}
\item
  The interaction between smartphone use and gender is shown by the variable thours\_c male\_c thours\_c:male\_c, and this interaction was significant nonsignificant at the \(\alpha = .05\) level.
\item
  To 2 decimal places, what proportion of the variance in well-being scores does the overall model explain? 
\item
  The p-value for the overall model fit is \texttt{\textless{}\ 2.2e-16}. Is this significant? Yes No
\item
  What is the most reasonable interpretation of these results? smartphone use harms girls more than boys smartphone use harms boys more than girls there is no evidence for gender differences in the relationship between smartphone use and well-being smartphone use was more negatively associated with wellbeing for girls than for boys
\end{itemize}

\hypertarget{activity-9-assumption-checking}{%
\section{Activity 9: Assumption checking}\label{activity-9-assumption-checking}}

Now it's time to test those pesky assumptions. The assumptions for multiple regression are the same as simple regression but there is one additional assumption, that of multicollinearity, the idea that predictor variables should not be too highly correlated.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The outcome/DV is a interval/ratio level data
\item
  The predictor variable is interval/ratio or categorical (with two levels)
\item
  All values of the outcome variable are independent (i.e., each score should come from a different participant)
\item
  The predictors have non-zero variance
\item
  The relationship between outcome and predictor is linear
\item
  The residuals should be normally distributed
\item
  There should be homoscedasticity (homogeneity of variance, but for the residuals)
\item
  Multicollinearity: predictor variables should not be too highly correlated
\end{enumerate}

From the work we've done so far we know that assumptions 1 - 4 are met. Unlike when we did simple regression we can't use \texttt{crPlots()} to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.

Normally we would test for normality with a qqplot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because as we have mentioned before, with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the qqplots.

\begin{itemize}
\tightlist
\item
  Create a qqplot of the model residuals. Can we assume normality? Yes No
\end{itemize}

We also have this problem for testing homoscedasticity with \texttt{ncvTest()} so we need to rely on plots again. To check for homoscedasticity we can use \texttt{plot()} from Base R that will produce a bunch of helpful plots (\href{https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/}{more information here}). The residuals vs leverage plot shows a flat red line so, whilst it isn't perfect, we can assume that with such a large sample size regression is still an appropriate analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{# 4 charts in 1 panel}
\KeywordTok{plot}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{16-week-13_files/figure-latex/plots2-1} 

}

\caption{Regression assumption plots}\label{fig:plots2}
\end{figure}

Finally, to check for multicollinearity we can use \texttt{vif()} to calculate the variance inflation factor. Essentially, this function estimates how much the variance of a coefficient is ``inflated'' because of linear dependence with other predictors, i.e., that a predictor isn't actually adding any unique variance to the model, it's just really strongly related to other predictors. \href{https://statisticalhorizons.com/multicollinearity}{You can read more about this here}. Thankfully, VIF is not affected by large samples like the other tests.

There are various rules of thumb, but most converge on a VIF of above 2 - 2.5 for any one predictor being problematic.

\begin{itemize}
\tightlist
\item
  Run the below code. Do any of the predictors show evidence of multicollinearity? thours\_c male\_c thours\_c:male\_c There is no evidence of multicollinearity
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vif}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-10-power-and-effect-size}{%
\section{Activity 10: Power and effect size}\label{activity-10-power-and-effect-size}}

Finally, we'll calculate power and effect size as usual.

\begin{itemize}
\item
  Using the code from Week 12 calculate the minimum effect size we could reliably observe given our sample size and design but for 99\% power. Report this to 2 decimal places 
\item
  What is the observed effect size for the study to 2 decimal places? \\
\item
  Is the study sufficiently powered? Yes No
\end{itemize}

\hypertarget{activity-11-write-up-1}{%
\section{Activity 11: Write-up}\label{activity-11-write-up-1}}

Now, copy and paste the below code into \textbf{white-space} and then knit the document. Note that the p-values are entered manually because of the APA \texttt{p\ \textless{}\ .001} formatting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{All continuous predictors were mean}\OperatorTok{-}\NormalTok{centered and deviation coding was used }\ControlFlowTok{for}\NormalTok{ categorical predictors. The results of the regression indicated that the model significantly predicted course }\KeywordTok{engagement}\NormalTok{ (}\KeywordTok{F}\NormalTok{(}\StringTok{`}\DataTypeTok{r mod_summary$fstatistic[2]}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{r mod_summary$fstatistic[3] %>% round(2)}\StringTok{`}\NormalTok{) =}\StringTok{ `}\DataTypeTok{r mod_summary$fstatistic[1] %>% round(2)}\StringTok{`}\NormalTok{, p }\OperatorTok{<}\StringTok{ }\FloatTok{.001}\NormalTok{, Adjusted }\DataTypeTok{R2 =} \StringTok{`}\DataTypeTok{r mod_summary$adj.r.squared %>% round(2)}\StringTok{`}\NormalTok{, f}\OperatorTok{^}\DecValTok{2}\OperatorTok{^}\StringTok{ }\ErrorTok{=}\StringTok{ }\FloatTok{.63}\NormalTok{), accounting }\ControlFlowTok{for} \StringTok{`}\DataTypeTok{r (mod_summary$adj.r.squared %>% round(2))*100}\StringTok{`}\OperatorTok{% of the variance. Total screen time was a significant negative predictor of wellbeing scores (β = `r mod$coefficients[2] %}\ErrorTok{>}\OperatorTok{% round(2)`, p < .001, as was gender (β = `r mod$coefficients[3] %}\ErrorTok{>}\OperatorTok{% round(2)`, p < .001, with girls having lower wellbeing scores than boys. Importantly, there was a significant interaction between screentime and gender (β = `r mod$coefficients[4] %}\ErrorTok{>}\NormalTok{% }\KeywordTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)}\StringTok{`}\DataTypeTok{, p < .001), smartphone use was more negatively associated with wellbeing for girls than for boys. }
\end{Highlighting}
\end{Shaded}

\begin{quote}
All continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(3, \ensuremath{7.1029\times 10^{4}}) = 2450.89, p \textless{} .001, Adjusted R2 = 0.09, f2 = .63), accounting for 9\% of the variance. Total screen time was a significant negative predictor of well-being scores (β = -0.77, p \textless{} .001, as was gender (β = 5.14, p \textless{} .001, with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and gender (β = 0.45, p \textless{} .001), smartphone use was more negatively associated with well-being for girls than for boys.
\end{quote}

\hypertarget{finished-10}{%
\section{Finished!}\label{finished-10}}

And you're done! Not just with this week but with the R component of RM2! Well, aside from the final portfolio worksheet. The progress that you have made is truly astonishing. Even if you struggled with R and haven't quite understood every single line of code we've shown, what you're capable of with data wrangling and visualisation alone makes you some of the most highly competitive psychology graduates in the world.

Regardless of whether you continue with quantitative methods and using R, remember the more important critical skills that you have learned as part of this process. The next time you see a dataset or you see data being talked about in the news, think about all work that was put into getting the data into the final format. More importantly, think about all the decisions that the researcher needed to make along the way and how that might have affected the outcome.

\includegraphics{https://media.giphy.com/media/ujGfBmVppmgEg/giphy.gif}

\hypertarget{activity-solutions-10}{%
\section{Activity solutions}\label{activity-solutions-10}}

\hypertarget{activity-3-6}{%
\subsection{Activity 3}\label{activity-3-6}}

Activity 3

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wemwbs <-}\StringTok{ }\NormalTok{wellbeing }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"var"}\NormalTok{, }\StringTok{"score"}\NormalTok{, }\OperatorTok{-}\NormalTok{Serial) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Serial) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{tot_wellbeing =} \KeywordTok{sum}\NormalTok{(score))}

\CommentTok{# sanity check values}

\NormalTok{wemwbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(tot_wellbeing),}
                     \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(tot_wellbeing),}
                     \DataTypeTok{min =} \KeywordTok{min}\NormalTok{(tot_wellbeing), }
                     \DataTypeTok{max =} \KeywordTok{max}\NormalTok{(tot_wellbeing))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-5-7}{%
\subsection{Activity 5}\label{activity-5-7}}

Activity 5

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smarttot <-}\StringTok{ }\NormalTok{screen2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(variable }\OperatorTok{==}\StringTok{ "Using Smartphone"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Serial) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{tothours =} \KeywordTok{mean}\NormalTok{(hours))}

\NormalTok{smart_wb <-}\StringTok{ }\NormalTok{smarttot }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(tothours }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(wemwbs, }\StringTok{"Serial"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(pinfo, }\StringTok{"Serial"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-6-4}{%
\subsection{Activity 6}\label{activity-6-4}}

Activity 6

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smart_wb <-}\StringTok{ }\NormalTok{smarttot }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(tothours }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(wemwbs, }\StringTok{"Serial"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(pinfo, }\StringTok{"Serial"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{thours_c =}\NormalTok{ tothours }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(tothours),}
         \DataTypeTok{male_c =} \KeywordTok{ifelse}\NormalTok{(male }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{-.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-7-4}{%
\subsection{Activity 7}\label{activity-7-4}}

Activity 7

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smart_wb_gen <-}\StringTok{ }\NormalTok{smart_wb }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(tothours, male) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_wellbeing =} \KeywordTok{mean}\NormalTok{(tot_wellbeing))}

\KeywordTok{ggplot}\NormalTok{(smart_wb_gen, }\KeywordTok{aes}\NormalTok{(tothours, mean_wellbeing, }\DataTypeTok{color =}\NormalTok{ male)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Gender"}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Total hours smartphone use"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mean well-being score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-8-4}{%
\subsection{Activity 8}\label{activity-8-4}}

Activity 8

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(tot_wellbeing }\OperatorTok{~}\StringTok{ }\NormalTok{thours_c }\OperatorTok{*}\StringTok{ }\NormalTok{male_c, smart_wb)}
\CommentTok{# alternatively: }
\CommentTok{# mod <- lm(tot_wellbeing ~ thours_c + male_c + thours_c:male_c, smart_wb)}

\NormalTok{mod_summary <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-9-3}{%
\subsection{Activity 9}\label{activity-9-3}}

Activity 9

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqPlot}\NormalTok{(mod}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\hypertarget{activity-10-1}{%
\subsection{Activity 10}\label{activity-10-1}}

Activity 9

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.f2.test}\NormalTok{(}\DataTypeTok{u =} \DecValTok{3}\NormalTok{, }\DataTypeTok{v =} \DecValTok{71029}\NormalTok{, }\DataTypeTok{f2 =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{.05}\NormalTok{, }\DataTypeTok{power =} \FloatTok{.99}\NormalTok{)}
\NormalTok{f2 <-}\StringTok{ }\NormalTok{mod_summary}\OperatorTok{$}\NormalTok{adj.r.squared}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{mod_summary}\OperatorTok{$}\NormalTok{adj.r.squared)}
\end{Highlighting}
\end{Shaded}

\hypertarget{portfolio-instructions}{%
\chapter{Portfolio instructions}\label{portfolio-instructions}}

Just like you've done throughout this book so far, we're going to use R Markdown for the portfolio worksheets.

\textbf{There are just a couple of important rules we need you to follow to make sure this all runs smoothly.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  These worksheets will ask you to fill in your answers and not change any other information. For example, if we ask you to replace NULL with your answer, only write in the code you are giving as your answer and nothing else. To illustrate -
\end{enumerate}

\textbf{Task 1 read in your data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

The task above is to read in the data file we are using for this task - the correct answer is \texttt{data\ \textless{}-\ read\_csv(data.csv)}. You would replace the NULL with:

\textbf{Solution to Task 1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This means that we can look for your code and if it is in the format we expect to see it in, we can give you the marks! If you decide to get all creative on us then we can't give you the marks as `my\_work\_Nov\_2018.csv' isn't the filename we have given to you to use. So don't change the file, variable or data frame names as we need these to be consistent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We will look for your answers within the boxes which start and end with ``` and have \{r task name\} in them e.g.
\end{enumerate}

```\{r tidyverse, messages=FALSE\}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

```

These are called code chunks and are the part of the worksheet that we can read and pick out your answers. If you change these in any way we can't read your answer and therefore we can't give you marks. You can see in the example above that the code chunk (the grey zone), starts and ends with these back ticks (usually found on top left corner of the keyboard). This code chunk has the ticks and text which makes it the part of the worksheet that will contain code. The \{r tidyverse\} part tells us which task it is (e.g., loading in tidyverse) and therefore what we should be looking for and what we can give marks for - loading in the package called tidyverse in the example above. If this changes then it won't be read properly, so will impact on your grade.

The easiest way to use our worksheets is to think of them as fill-in-the-blanks and keep the file names and names used in the worksheet the same. If you are unsure about anything then use the forums on Moodle and Teams to ask any questions.

\hypertarget{dissertation-analysis-guide}{%
\chapter{Dissertation analysis guide}\label{dissertation-analysis-guide}}

In this final chapter we're going to provide a guide for how to get started working with your dissertation data. It's important to note that this guide won't work for every project and every kind of data - and nothing in this chapter should supersede advice your supervisor has given you. Rather, this chapter is here to help guide you through the initial steps of working with quantitative data and to show you how what you have learned in RM2 maps on to your dissertation project.

\hypertarget{before-r}{%
\section{Before R}\label{before-r}}

Before you touch R, you need to make sure you understand your design, data, and analysis plan. This will make your work in R considerably easier. Before you start coding, make sure you have answers to the following questions:

\hypertarget{study-design}{%
\subsection{Study design}\label{study-design}}

\begin{itemize}
\tightlist
\item
  What is the design of your study?
\item
  What are your variables?

  \begin{itemize}
  \tightlist
  \item
    Are they IVs/predictors/correlation variables?
  \item
    How many levels do they have?

    \begin{itemize}
    \tightlist
    \item
      Are they between or within-subject?
    \end{itemize}
  \end{itemize}
\item
  What are your DVs?

  \begin{itemize}
  \tightlist
  \item
    How will you calculate them? For example, do you need to take a mean of several of questionnaire items or reaction times from multiple trials?
  \end{itemize}
\item
  What are your hypotheses?
\item
  Do you have any demographic data included in the data files?
\item
  Thinking back to the concept of tidy data, how many variables will you have?
\end{itemize}

\hypertarget{data-wrangling-considerations}{%
\subsection{Data wrangling considerations}\label{data-wrangling-considerations}}

\begin{itemize}
\tightlist
\item
  What does each variable in your data set represent?

  \begin{itemize}
  \tightlist
  \item
    Do you want to rename any variables to make them easier to work with?

    \begin{itemize}
    \tightlist
    \item
      Do you have a convention for naming variables? For example, I use all lower-case variables and words are always separated by underscores, e.g., \texttt{group\_data}.\\
    \end{itemize}
  \item
    Are there any variables that are not necessary for the analysis? For example, consent forms.
  \item
    Are there minimum and maximum values for any of your variables?
  \end{itemize}
\item
  Do you have multiple data sets you need to join together?

  \begin{itemize}
  \tightlist
  \item
    Do they have a common variable? For example, does each file have the participant ID or a question number?
  \end{itemize}
\item
  Do you need to create any new variables? For example, the mean or sum of a number of questionnaire items to give a total scale score?\\
\item
  What type of data should each variable be?

  \begin{itemize}
  \tightlist
  \item
    Are there any variables that you need to convert to factors?\\
  \end{itemize}
\item
  Do you need to recode any variables? For example, 1 = male, 2 = female, or perhaps reverse coding questionnaire responses.
\item
  Do you have a rule for outlier exclusion/replacement?\\
\item
  Do you need to exclude any participants? For example, if they score below a certain threshold, if they are non-native speakers etc.
\item
  Is there any missing data for each variable?

  \begin{itemize}
  \tightlist
  \item
    What will you do about missing data?
  \end{itemize}
\item
  Do you need to tidy your data?
\end{itemize}

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

\begin{itemize}
\tightlist
\item
  What descriptive statistics do you need to calculate for each variable?

  \begin{itemize}
  \tightlist
  \item
    Do you need to calculate descriptive by groups?
  \item
    Are the descriptive statistics similar to values reported in other studies that have used the same measures. Are they similar? If not, what is the explanation?
  \end{itemize}
\item
  How will you visualize your data?
\item
  Do you need to transform your data?
\item
  Do you need to conduct any kind of reliability analysis?
\item
  What type of inferential tests are you going to conduct?

  \begin{itemize}
  \tightlist
  \item
    Where appropriate, do you need to perform one or two-tailed hypothesis testing?\\
  \item
    Do you need to do any dummy coding for regression models?\\
  \end{itemize}
\item
  What assumptions do you need to test in order to perform your analyses?

  \begin{itemize}
  \tightlist
  \item
    What will you do if your data do not meet these assumptions?
  \end{itemize}
\item
  Do you need to apply a correction for multiple comparison testing? If so, which one?
\item
  Do you need to calculate measures of effect size? If so, which ones?
\end{itemize}

If this looks like a lot of work - it is. It's important to remember that a lot of the problems that students face with R are really nothing to do with R. In order to wrangle and analyse your data you first need to understand the data that you have. If you don't know what your independent and dependent variables are or what analysis you're supposed to be running, it doesn't matter what statistical software you are using, you won't be able to complete your task. Don't rush or skip any part of the preparation, it will make coding much harder. If you know the answers to all of the above questions then it means you're ready to get started in R.

\hypertarget{exploring-and-cleaning-your-data}{%
\section{Exploring and cleaning your data}\label{exploring-and-cleaning-your-data}}

The following sections will not provide comprehensive instructions on how to use the example code, nor will they cover every function you may need to use. You should refer to the RM2 materials, help documentation, and online resources, however, these examples may give you an idea of where to start.

As a first step you should explore your dataset to understand its properties and then perform some basic cleaning operations that will facilitate further analysis.

\hypertarget{summary}{%
\subsection{\texorpdfstring{\texttt{summary()}}{summary()}}\label{summary}}

A useful first step is to run \texttt{summary()}. Check the output for:

\begin{itemize}
\tightlist
\item
  Missing data\\
\item
  What type of data each variable is\\
\item
  If the variable names are easy to work with, for example \texttt{Participant\ Age} is difficult to work with because it has two capital letters and a space. Renaming this as \texttt{age} will make your coding easier.\\
\item
  Any suspicious values, for example, if your likert scale is 1-7 you shouldn't have a maximum score of 10. If you have standardised IQ scores, you may want to check that a score of 200 or 20 isn't a typo.\\
\item
  If you make any changes, run \texttt{summary()} again
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualisations}{%
\subsection{Visualisations}\label{visualisations}}

To get an overview of the data and spot any potential issues such as outliers you should plot histograms and boxplots to eyeball the distributions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ variable)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}

\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ condition, }\DataTypeTok{y =}\NormalTok{ score)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{renaming-variables}{%
\subsection{Renaming variables}\label{renaming-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{rename}\NormalTok{(data, }\DataTypeTok{new_name =}\NormalTok{ old_name)}
\end{Highlighting}
\end{Shaded}

\hypertarget{converting-to-factors}{%
\subsection{Converting to factors}\label{converting-to-factors}}

This is incredibly important. Don't skip this step otherwise things might go very wrong much further down the line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{variable =} \KeywordTok{as_factor}\NormalTok{(variable))}
\end{Highlighting}
\end{Shaded}

\hypertarget{dropping-irrelevant-variables}{%
\subsection{Dropping irrelevant variables}\label{dropping-irrelevant-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{select}\NormalTok{(data, }\OperatorTok{-}\NormalTok{consent1)}
\end{Highlighting}
\end{Shaded}

\hypertarget{recoding-variables}{%
\subsection{Recoding variables}\label{recoding-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{variable =} \KeywordTok{recode}\NormalTok{(variable, }\StringTok{"old_code"}\NormalTok{ =}\StringTok{ "new_code"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{exclude-participantsdata}{%
\subsection{Exclude participants/data}\label{exclude-participantsdata}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, variable }\OperatorTok{!=}\StringTok{ "value"}\NormalTok{) }\CommentTok{# exclude anyone with value}
\NormalTok{data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, variable }\OperatorTok{==}\StringTok{ "value"}\NormalTok{) }\CommentTok{# include only those with value}
\NormalTok{data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, variable }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"value1"}\NormalTok{, }\StringTok{"value2"}\NormalTok{)) }\CommentTok{# include all specified values}
\NormalTok{data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, variable }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{) }\CommentTok{# keep data if value of variable is more than 10}
\NormalTok{data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, variable }\OperatorTok{<=}\StringTok{ }\DecValTok{10}\NormalTok{) }\CommentTok{# keep data if value of variable is less than or equal to 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{reliability}{%
\subsection{Reliability}\label{reliability}}

If you are using a scale, for example, as part of a questionnaire study you may need to calculate reliability and you should do this before you calculate the aggregated scale scores. There are several options about how you do this and you should consult your supervisor but one option is to use \texttt{alpha()} from the \texttt{psych} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Q1}\OperatorTok{:}\NormalTok{Q5) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{transforming-data}{%
\section{Transforming data}\label{transforming-data}}

The above steps should leave you with a good understanding of your data and all the variables you need for your analysis. The next step is to correct any problems with the data by replacing or transforming individual values. You may also need to create new variables, for example the total score for a questionnaire or mean reaction times or accuracy.

\hypertarget{replace-missing-values}{%
\subsection{Replace missing values}\label{replace-missing-values}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable =} \KeywordTok{replace_na}\NormalTok{(data}\OperatorTok{$}\NormalTok{variable, }\DecValTok{0}\NormalTok{)) }\CommentTok{# replace NAs in variable with 0}

\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{replace_na}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{gender =} \StringTok{"unknown"}\NormalTok{, }\CommentTok{# replace NAs in `gender` with "unknown"}
                              \DataTypeTok{score =} \DecValTok{0}\NormalTok{,          }\CommentTok{# replace NAs in `score` with 0  }
                              \DataTypeTok{rt =} \KeywordTok{mean}\NormalTok{(data}\OperatorTok{$}\NormalTok{rt, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)))  }\CommentTok{# replace NAs in `rt` with mean of `rt`}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-implausible-values}{%
\subsection{Convert implausible values}\label{convert-implausible-values}}

In the case of implausible values (such as a score or 10 on a 7-point likert scale), you may wish to recode these as missing, or as the mean (or some other value).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{q1=} \KeywordTok{ifelse}\NormalTok{(q1 }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{, }\OtherTok{NA}\NormalTok{, q1)) }\CommentTok{# if the value in q1 is more than 10 replace it with NA, if it's not, keep the value as it is}

\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rt =} \KeywordTok{ifelse}\NormalTok{(rt }\OperatorTok{>}\StringTok{ }\DecValTok{1000}\NormalTok{, }\KeywordTok{mean}\NormalTok{(rt, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{), rt)) }\CommentTok{# if the value in rt is more than 1000, replace it with the mean rt, if it's below 1000, keep the value as it is}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculate-z-scores}{%
\subsection{Calculate z-scores}\label{calculate-z-scores}}

You may want to calculate z-scores in order to remove outliers. You could then use \texttt{filter()} on the new z-score variable you have created.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{z_scores =} \KeywordTok{scale}\NormalTok{(scores))}
\end{Highlighting}
\end{Shaded}

There are a few other types of transformations we can do to correct for problems with normality. \href{https://rcompanion.org/handbook/I_12.html}{This page} gives a good overview of all the options.

\hypertarget{log-transformation}{%
\subsection{Log transformation}\label{log-transformation}}

A popular method of transformation is to calculate the log of a variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable_log =} \KeywordTok{log}\NormalTok{(variable))}
\end{Highlighting}
\end{Shaded}

\hypertarget{square-root}{%
\subsection{Square root}\label{square-root}}

Another popular method is to perform a square root transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable_sqrt =} \KeywordTok{sqrt}\NormalTok{(variable))}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-new-variables}{%
\subsection{Calculating new variables}\label{calculating-new-variables}}

You may wish to calculate the sum or the mean of a number of variables. For example, if you have 9 questions and you want the sum of questions 1-5 and the mean of questions 6 - 9 and your data is is wide-form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sum_scoreq1q5 =} \KeywordTok{rowSums}\NormalTok{(}\KeywordTok{select}\NormalTok{(., Q1}\OperatorTok{:}\NormalTok{Q5)),}
         \DataTypeTok{mean_scoreq6q10 =} \KeywordTok{rowMeans}\NormalTok{(}\KeywordTok{select}\NormalTok{(., Q6}\OperatorTok{:}\NormalTok{Q9)))}
\end{Highlighting}
\end{Shaded}

If your data is in long-form you may want to use functions such as \texttt{gather()}, \texttt{spread()} and \texttt{summarise()}. See RM2 Lab 3 for more info on this.

\hypertarget{tidy-data-1}{%
\subsection{Tidy data}\label{tidy-data-1}}

It is at this point that you should tidy the dataset using functions such as \texttt{gather()}. Refer back to the RM1 Lab 2 and RM2 Lab 3. You may also wish to have a wide-form version of your data depending upon the analyses you are conducting.

\hypertarget{summarising-and-visualising-data}{%
\section{Summarising and visualising data}\label{summarising-and-visualising-data}}

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive statistics}\label{descriptive-statistics}}

At this point you may want to calculate descriptive statistics for variables of interest. Refer back to RM1 Lab 2 and Lab 3 for more information on these functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\CommentTok{# produce descriptives for the total data set}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_score =} \KeywordTok{mean}\NormalTok{(score, }\DataTypeTok{na.rm =}\NormalTok{ true),}
            \DataTypeTok{sd_score =} \KeywordTok{sd}\NormalTok{(score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{median_score =} \KeywordTok{median}\NormalTok{(score))}

\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\CommentTok{# produce descriptives for each of the grouping variables}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender, condition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_score =} \KeywordTok{mean}\NormalTok{(score, }\DataTypeTok{na.rm =}\NormalTok{ true),}
            \DataTypeTok{sd_score =} \KeywordTok{sd}\NormalTok{(score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{median_score =} \KeywordTok{median}\NormalTok{(score))}
\end{Highlighting}
\end{Shaded}

You may also find the function \texttt{describe()} from the \texttt{psych} package useful (you may need to install this package but as always, \textbf{do not install packages on university computers}). \texttt{describe()} produces a full range of descriptive statistics including skew, kurtosis and standard error.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(psych)}
\KeywordTok{describe}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\texttt{describeBy()} produces descriptives by a grouping variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describeBy}\NormalTok{(data, }\DataTypeTok{group =} \StringTok{"gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-visualisation}{%
\subsection{Data visualisation}\label{data-visualisation}}

At this point you should plot your data using a method that reflects the analysis you wish to conduct (e.g., a scatterplot for a correlation, a violn-boxplot for a t-test). For the plots and code specific to each type of analysis, please refer to the relevant chapters. You should ensure that your plots are as informative as possible and display the spread of the data using functions such as \texttt{geom\_violin()}, and \texttt{geom\_point()}. Avoid purely aggregated plots like bar charts representing means.

\hypertarget{inferential-statistics}{%
\section{Inferential statistics}\label{inferential-statistics}}

You are now ready to conduct your inferential analyses. For details on how to perform different tests, please refer to relevant chapters. Ensure that you understand how each test relates to each of your hypotheses.

\hypertarget{assumption-tests}{%
\section{Assumption tests}\label{assumption-tests}}

Depending upon the analysis, you may be able to conduct assumption checks before the inferential tests are conducted, however, for methods such as ANOVA and regression, you need to check the model residuals and therefore this can't be done until afterwards.

Foe details on what assumption checks to conduct for each statistical test, please refer to the relevant chapters.

\hypertarget{refsup}{%
\chapter{Supplementary Analyses}\label{refsup}}

\textbf{WARNING} This chapter is still under construction.

\includegraphics{http://www.reactiongifs.com/wp-content/uploads/2013/07/see.gif}

This chapter will contain a number of supplementary analyses. These are beyond the core content of the MSc Conversion course. Some of them are taken from the undergraduate course where there is more time for statistics, and some of them are just for fun. You may find some of the function useful for your dissertation or if you want to stretch yourself with the mini-project so it's worth having a browse.WW

\hypertarget{data-transformation}{%
\section{Data transformation}\label{data-transformation}}

This section has been adapted from materials made available by \href{http://abacus.bates.edu/~ganderso/biology/bio270/homework_files/Data_Transformation.pdf}{Greg Anderson at Bates College}.

The following brief overview of Data Transformation is compiled from Howell (pp.~318-324,
2007) and Tabachnick and Fidell (pp.~86-89, 2007). See the references at the end of this handout for a more complete discussion of data transformation. Most people find it difficult to accept the idea of transforming data. Tukey (1977) probably had the right idea when he called data transformation calculations ``reexpressions'' rather than ``transformations.'' A researcher is merely reexpressing what the data have to say in other terms. However, it is important to recognize that conclusions that you draw on transformed data do not always transfer neatly to the original measurements. Grissom (2000) reports that the means of transformed variables can occasionally reverse the difference of means of the original variables. While this is disturbing, and it is important to think about the meaning of what you are doing, but it is not, in itself, a reason to rule out the use of transformations as a viable option.

If you are willing to accept that is it permissible to transform one set of measures into another, then many possibilities become available for modifying the data to fit more closely the underlying assumptions of statistical tests. An added benefit about most of the transformations is that when we transform the data to meet one assumption, we often come closer to meeting other assumptions as well. For example, a square-root transformation may help equate group variances, and because it compresses the upper end of a distribution more than it compresses the lower end, it may also have the effect of making positively skewed distributions more nearly normal in shape. If you decide to transform, it is important to check that the variable is normally or nearly normally distributed after transformation. That is, make sure it worked.

When it comes to reporting our data\ldots{} although it is legitimate and proper to run a statistical test, such as the one-way analysis of variance, on the transformed values, we often report means in the unit of the untransformed scale. This is especially true when the original units are intrinsically meaningful. Howell (2007) urges researchers to look at both the converted (transformed) and unconverted (original) means and make sure that they are telling the same basic story. Do not convert standard deviations -- you will do serious injustice if you try that. And be sure to indicate to your readers what you have done. It is not uncommon to see both the converted and unconverted values reported. Tabachnick and Fidell (2007) point out that, although data transformations are recommended as a remedy for outliers and for failures of normality, linearity, and homoscedasticity, they are not universally recommended. The reason is that an analysis is interpreted from the variables that are in it, and transformed variables are sometimes harder to interpret.

You should not get the impression that data transformations should be applied routinely to all your data. As a rule of thumb, ``If it's not broken, don't fix it.'' If your data are reasonably distributed (i.e., are more or less symmetrical and have few, if any, outliers) and if your variances are reasonably homogeneous, there is probably nothing to be gained by applying a transformation. If you have markedly skewed data or heterogeneous variances, however, some form of data transformation may be useful. Furthermore, it is perfectly legitimate to shop around for a transformation that makes the necessary changes to the variance and shape. The only thing you should not do it to try out every transformation, looking for one that gives you a significant result. You are trying to optimize the data, not the resulting F.

As suggested by Tabachnick and Fidell (2007) and Howell (2007), the following guidelines
should be used when transforming data.

\begin{tabular}{l|l|l}
\hline
Problem & Transformation & R function\\
\hline
Moderately positive skewness & Square-root & `sqrt(var)`\\
\hline
Substantially positive skewness & Logarithmic (Log 10) & `log10(var)`\\
\hline
Substantially positive skewness (with zero values) & Logarithmic (Log 10) & `log10(var + C)`\\
\hline
Moderately negative skewness & Square-Root & `sqrt(var + K)`\\
\hline
Substantially negative skewness & Logarithmic (Log 10) & `log10(K - var)`\\
\hline
\end{tabular}

\texttt{C} = a constant added to each score so that the smallest score is 1.\\
\texttt{K} = a constant from which each score is subtracted so that the smallest score is 1; usually equal to the largest score + 1.

\textbf{References}

Howell, D. C. (2007). Statistical methods for psychology (6th ed.). Belmont, CA: Thomson
Wadsworth.\\
Grissom, R. J. (2000). Heterogeneity of variance in clinical data. \emph{Journal of Consulting and Clinical Psychology, 68}, 155-165.\\
Tabachnick, B. G., \& Fidell, L. S. (2007). \emph{Using multivariate statistics (5th ed.)}. Boston: Allyn and Bacon.\\
Tukey, J. W. (1977). \emph{Exploratory data analysis}. Reading, MA: Addison-Wesley.

Also see:

Hoaglin, D. C., Mosteller, F., \& Tukey, J. W. (1983). \emph{Understanding robust and exploratory data analysis}. New York: Wiley.

\hypertarget{permutation-tests}{%
\section{Permutation tests}\label{permutation-tests}}

This section has been adapated from the Level 2 class on permutation tests written by Dr.~Phil McAleer. The original \href{https://psyteachr.github.io/ug2-practical/permutation-tests-a-skill-set.html}{can be viewed here}.

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

In this week's lab you will perform your first hypothesis test using a procedure known as a \textbf{permutation test}. We will help you learn how to do this through building and running data simulation procedures. In order to complete this lab you will require the following skills which we will teach you today:

\begin{itemize}
\tightlist
\item
  Skill 1: Generating random numbers with \texttt{base::rnorm()}
\item
  Skill 2: Permuting values with \texttt{base::sample()}\\
\item
  Skill 3: Creating a ``tibble'' (a type of data table) using \texttt{tibble::tibble()}
\item
  Skill 4: Computing and extracting a difference in group means using \texttt{dplyr::pull()} and \texttt{purrr::pluck()}
\item
  Skill 5: Creating your own custom functions using \texttt{base::function()}
\item
  Skill 6: Repeating operations using \texttt{base::replicate()}
\end{itemize}

To many, a lot of statistics must seem a bit like blind faith as it deals with estimating quantities we haven't observed (or can't observe), e.g.~the mean of a whole population. As such we have to know if we can trust our procedures for making estimations and inferences because we rarely get a chance to compare the estimated values to the true values to see if they match up. One way to test a procedure, and in turn learn about statistics, is through data simulation. In simulations \textbf{we create} a population and then draw samples and run tests on the data, i.e.~on this \textbf{known} population. By running lots of simulations we can test our procedures and make sure they are acting as we expect them to. This approach is known as a \textbf{Monte Carlo simulation}, named after the city famous for the many games of chance that are played there.

\begin{info}
You can go read up on the Monte Carlo approach if you like. It can
however get quite indepth, as having a brief glance at the wikipedia
entry on it highlights. The main thing to keep in mind is that the
method involves creating a population and continually taking samples
from that population in order to make an inference. This is what we will
show you in the lab. Data simulation and ``creating'' your own datasets,
to see how tests work, is a great way to understand statistics. When
doing this lab, keep in mind how easy it really is to find a significant
result if even randomly created data can give a significant result. This
may help dispell any notion that there is something inherently important
about a significant result, in itself.
\end{info}

We will now take each skill in turn. Be sure to try them all out. It looks a lot of reading but it is mainly just showing you the output of the functions so you can see you are doing it correctly. The key thing is to try them yourselves and don't be scared to change things to see what might happen if you do it slightly differently. We will also ask a couple of questions along the way to make sure you understand the skills.

\hypertarget{skill-1-generating-random-numbers}{%
\subsection{Skill 1: Generating Random Numbers}\label{skill-1-generating-random-numbers}}

The \texttt{base::rnorm()} function generates values from a normal distribution and takes the following arguments:

\begin{itemize}
\tightlist
\item
  \texttt{n}: the number of observations to generate
\item
  \texttt{mean}: the mean of the distribution (default 0)
\item
  \texttt{sd} : the standard deviation of the distribution (default 1)
\end{itemize}

To generate 10 or even 50 random numbers from a standard normal distribution (M = 0, SD = 1), you would use \texttt{rnorm(10)} or \texttt{rnorm(50)} respectively.

\begin{itemize}
\tightlist
\item
  Type \texttt{rnorm(50)} into your console and see what happens. Use the below example for \texttt{rnorm(10)} to help you.\\
\item
  Try increasing \texttt{n} to 1000.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] -0.76970764  0.37855190 -1.36985218  1.04914053  0.63403807
##  [6] -0.02941347 -0.51477745 -0.16351786  0.63594230  1.03281539
\end{verbatim}

{Quickfire Questions}

If you enter \texttt{rnorm(50)} again you will get different numbers. Why? I have made a mistake The numbers are random R has made a mistake Phil has made a mistake

If you want to change the mean or sd, you would need to pass additional arguments to the function as shown below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Try changing the mean and sd values a couple of times and see what happens. You get different numbers again that will be around the mean you set! Set a mean of 10, then a mean of 100, to test this.
\end{itemize}

Finally, for this Skill, you can concatenate (i.e.~link) numbers together into a single vector using the \texttt{c()} function from base R. For instance, say you wanted to create a vector with two sets of 50 random numbers from two separate samples: one set of 50 with a mean of 75 and the other with a mean of 90, you would use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random_numbers <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{75}\NormalTok{),}
                    \KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{90}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

{Quickfire Questions}

In the above example code, what is the standard deviation of the two samples you have created? 50 75 90 1

Explain This - I don't get this answer!

\begin{info}
What is the \textbf{default} sd of the function?

Both populations would have an sd of 1, because that is the default,
although you could easily change that. Try it out!
\end{info}

It is always good to check that your new vector has the right number of data points in it - i.e.~the total of the two samples; a sanity check if you will. The new vector \texttt{random\_numbers} should have 100 elements. You could verify this using the \texttt{length()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(random_numbers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

\hypertarget{skill-2-permuting-values}{%
\subsection{Skill 2: Permuting Values}\label{skill-2-permuting-values}}

Another thing that is useful to be able to do is to generate \textbf{permutations} of values.

Portfolio Point - What are Permutations?

\begin{info}
A \textbf{permutation} is just a different ordering of the same values.
For example, the numbers 1, 2, 3 can be permuted into the following 6
sequences:

\begin{itemize}
\tightlist
\item
  1, 2, 3
\item
  1, 3, 2
\item
  2, 1, 3
\item
  2, 3, 1
\item
  3, 1, 2
\item
  3, 2, 1
\end{itemize}

The more values you have, the more permutations of the order you have.
The number of permutations can be calculated by, for example,
\texttt{3*2*1}, where 3 is the number of values you have. Or through
code: \texttt{factorial(3)\ =\ 6}. This assumes that each value is used
once in the sequence and that each value never changes, i.e.~1234 cannot
suddenly become 1235.
\end{info}

We can create random permutations of a vector using the \texttt{sample()} function. Let's use one of R's built in vectors: \texttt{letters}.

\begin{itemize}
\tightlist
\item
  Type \texttt{letters} into the console, as below, and press RETURN/ENTER. You will see it contains all the lowercase letters of the English alphabet. Now, I bet you are wondering what \texttt{LETTERS} does, right?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
## [18] "r" "s" "t" "u" "v" "w" "x" "y" "z"
\end{verbatim}

We can combine \texttt{base::sample()} with \texttt{letters} to put the letters into a random order:

\begin{itemize}
\tightlist
\item
  Run the below line. Run it again. And again. What do you notice? And why is our output different from yours? (The answer is below)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(letters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "j" "b" "f" "h" "m" "k" "p" "z" "d" "v" "c" "s" "g" "n" "y" "x" "o"
## [18] "l" "u" "a" "t" "e" "w" "q" "i" "r"
\end{verbatim}

{Quickfire Questions}

If \texttt{month.name} contains the names of the twelve months of the year, how many possible permutations are there of \texttt{sample(month.name)}? 1 12 144 479001600

Portfolio Point - Different samples with sample()

\begin{info}
Each time you run \texttt{sample(letters)} it will give you another
random permutation of the sequence. That is what \texttt{sample()} does
- creates a random permutation of the values you give it. Try repeating
this command many times in the console. Because there are so many
possible sequences, it is very unlikely that you will ever see the same
sequence twice!

An interesting thing about \texttt{sample()} is that
\texttt{sample(c(1,2,3,4))} is the same as \texttt{sample(4)}. And to
recap, there would be 24 different permutations based on
\texttt{factorial(4)}, meaning that each time you type
\texttt{sample(4)} you are getting one of those 24 different orders. So
what would factorial(12) be?

Top Tip: Remember that you can scroll up through your command history in
the console using the up arrow on your keyboard; this way, you don't
ever have to retype a command you've already entered.
\end{info}

\hypertarget{skill-3-creating-tibbles}{%
\subsection{Skill 3: Creating Tibbles}\label{skill-3-creating-tibbles}}

Tables are important because most of the data we want to analyze comes in a table, i.e.~tabular form. There are different ways to get tabular data into R for analysis. One common way is to load existing data in from a data file (for example, using \texttt{readr::read\_csv()} which you have seen before). But other times you might want to just type in data directly. You can do this using the \texttt{tibble::tibble()} function. Being able to create a tibble is a useful data analysis skill because sometimes you will want to create some data on the fly just to try certain codes or functions.

\hypertarget{entering-data-into-a-tibble}{%
\subsubsection{Entering Data into a Tibble}\label{entering-data-into-a-tibble}}

The \texttt{tibble()} function takes named arguments - this means that the name you give each argument within the tibble function, e.g. \texttt{Y\ =\ rnorm(10)} will be the name of the column that appears in the table, i.e. \texttt{Y}. It's best to see how it works through an example.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{Y =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 1
##          Y
##      <dbl>
##  1  0.163 
##  2  1.10  
##  3 -0.258 
##  4 -0.276 
##  5  1.32  
##  6  0.0668
##  7 -0.0211
##  8 -0.928 
##  9  1.39  
## 10  0.382
\end{verbatim}

The above command creates a new table with one column named \texttt{Y}, and the values in that column are the result of a call to \texttt{rnorm(10)}: 10 randomly sampled values from a standard normal distribution (mean = 0, sd = 1) - See Skill 1.

If however we wanted to sample from two different populations for \texttt{Y}, we could combine two calls to \texttt{rnorm()} within the \texttt{c()} function. Again this was in Skill 1, here we are now just storing it in a tibble. See below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{-10}\NormalTok{), }
             \KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =}  \DecValTok{20}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 1
##         Y
##     <dbl>
##  1 -10.5 
##  2  -8.38
##  3  -8.82
##  4 -11.5 
##  5 -10.9 
##  6  20.2 
##  7  18.8 
##  8  18.5 
##  9  20.2 
## 10  19.9
\end{verbatim}

Now we have sampled a total of 10 observations - the first 5 come from a group with a mean of -10, and the second 5 come from a group with a mean of 20. Try changing the values in the above example to get an idea of how this works. Maybe even add a third group!

But, of course, it would be good to know which population each data point refers to and so we should add some group names. We can do this with some additional trickery using the \texttt{rep()} function.

\hypertarget{repeating-values-to-save-typing}{%
\subsubsection{Repeating Values to Save Typing}\label{repeating-values-to-save-typing}}

Before finalising our table let's learn a little about the base R function, \texttt{rep()}. This is most useful for automatically repeating values in order to save typing. For instance, if we wanted 20 letter ``A''s in a row, we would type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A" "A"
## [18] "A" "A" "A"
\end{verbatim}

The first argument to \texttt{rep()} is the vector containing the information you want repeated, \textbf{A}, and the second argument, \texttt{times}, is the number of times to repeat it; in this case \textbf{20}.

If you wanted to add more information, e.g.~if the first argument has more than one element, say ``A'' and ``B'', it will repeat the entire vector that number of times; A B, A B, A B, \ldots{} . Note that we enclose ``A'' and ``B'' in the \texttt{c()} function so that it is seen as a single argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A"
## [18] "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B" "A" "B"
## [35] "A" "B" "A" "B" "A" "B"
\end{verbatim}

But sometimes we want a specific number of As followed by a specific number of Bs; A A A B B B. If the \texttt{times} argument has the same number of elements as the vector given for the first argument, it will repeat each element of the first vector as many times as given by the corresponding element in the \texttt{times} vector. In other words, for example, \texttt{times\ =\ c(2,\ 4)} for vector \texttt{c("A",\ "B")} will give you 2 As followed by 4 Bs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A" "A" "B" "B" "B" "B"
\end{verbatim}

The best way to learn about this function is to play around with it in the console and see what happens. From the dropdown menus, the correct output of the following function would be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{rep(c("A",\ "B",\ "C"),(2,\ 3,\ 1))} - A A A B B C A A B B B C A A B B C C A B C A B C
\item
  \texttt{rep(1:5,\ 5:1)} - 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 5 5 5 5 5 4 4 4 4 3 3 3 2 2 1 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5
\end{enumerate}

\hypertarget{bringing-it-together-in-a-tibble}{%
\subsubsection{Bringing it Together in a Tibble}\label{bringing-it-together-in-a-tibble}}

Now we know \texttt{rep()}, we can complete our table of simulated data by combining what we've learned about generating random numbers and repeating values. We want our table to look like this:

\begin{verbatim}
## # A tibble: 10 x 2
##    group     Y
##    <chr> <dbl>
##  1 A     -8.39
##  2 A     -9.17
##  3 A     -9.63
##  4 A     -8.48
##  5 A     -9.83
##  6 B     21.7 
##  7 B     18.9 
##  8 B     19.2 
##  9 B     20.0 
## 10 B     21.3
\end{verbatim}

You now know how to create this table. Have a look at the code below and make sure you understand it. We have one column called \texttt{group} where we create \textbf{A}s and \textbf{B}s through \texttt{rep()}, and one column called \textbf{Y}, our data, all in our \texttt{tibble()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)),}
       \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{-10}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =}  \DecValTok{20}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Be sure to play around with the code chunk to get used to it. Try adding a third group or even a third column? Perhaps you want to give every participant a random age with a mean of 18, and a sd of 1; or even a participant number.

Helpful Hint

\begin{info}
Try \texttt{row\_number()} to create participant numbers.
\end{info}

Don't forget, if you wanted to store your tibble, you would just assign it to a name, such as \texttt{my\_data}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{row_number}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{), }
                    \DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)),}
                    \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{-10}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =}  \DecValTok{20}\NormalTok{)),}
                    \DataTypeTok{Age =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

{Skill 3 out of 6 Complete!}

\hypertarget{skill-4-computing-differences-in-group-means}{%
\subsection{Skill 4: Computing Differences in Group Means}\label{skill-4-computing-differences-in-group-means}}

You have already learned how to calculate group means using \texttt{group\_by()} and \texttt{summarise()}. For example, you might want to calculate sample means for a randomly generated dataset like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{)),}
                  \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,  }\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{-20}\NormalTok{, }\DecValTok{5}\NormalTok{)))}

\NormalTok{my_data_means <-}\StringTok{ }\NormalTok{my_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(group) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{m =} \KeywordTok{mean}\NormalTok{(Y))}

\NormalTok{my_data_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   group     m
##   <chr> <dbl>
## 1 A      18.9
## 2 B     -20.6
\end{verbatim}

Sometimes what we want though is to calculate \textbf{the differences between means} rather than just the means; so we'd like to subtract the second group mean -20.6 from the first group mean of 18.9, to get a single value, the difference: 39.5.

We can do this using the \texttt{dplyr::pull()} and \texttt{purrr::pluck()} functions. \texttt{pull()} will extract a single column from a dataframe and turn it into a vector. \texttt{pluck()} then allows you to pull out an element (i.e.~a value or values) from within that vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec <-}\StringTok{ }\NormalTok{my_data_means }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pull}\NormalTok{(m)}

\NormalTok{vec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  18.90517 -20.55524
\end{verbatim}

We have now created \texttt{vec} which is a vector containing only the group means; the rest of the information in the table has been discarded. Now that we have \texttt{vec}, we can calculate the mean difference as below, where \texttt{vec} is our vector of the two means and \texttt{{[}1{]}} and \texttt{{[}2{]}} refer to the two means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{vec[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 39.46041
\end{verbatim}

But \texttt{pluck()} is also useful, and can be written as so:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pluck}\NormalTok{(vec, }\DecValTok{1}\NormalTok{) }\OperatorTok{-}\StringTok{ }\KeywordTok{pluck}\NormalTok{(vec, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 39.46041
\end{verbatim}

It can also be incorporated into a pipeline as below where we still \texttt{pull()} the means column, \texttt{m}, and then \texttt{pluck()} each value in turn and subtract them from each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## whole thing in a pipeline}
\NormalTok{my_data_means }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(m) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{-}
\StringTok{  }\NormalTok{my_data_means }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(m) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pluck}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 39.46041
\end{verbatim}

However, there is an alternative way to extract the difference between means which may make more intuitive sense. You already know how to calculate a difference between values in the same row of a table using \texttt{dplyr::mutate()}, e.g. \texttt{mutate(new\_column\ =\ column1\ minus\ column2)}. So if you can get the observations in \texttt{my\_data\_means} into the same row, different columns, you could then use \texttt{mutate()} to calculate the difference. Previously you learned \texttt{gather()} to bring columns together. Well the opposite of gather is the \texttt{tidyr::spread()} function to split columns apart - as below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data_means }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(group, m)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##       A     B
##   <dbl> <dbl>
## 1  18.9 -20.6
\end{verbatim}

The spread function (\texttt{?spread}) splits the data in column \texttt{m} by the information, i.e.~labels, in column \texttt{group} and puts the data into separate columns. A call to \texttt{spread()} followed by a \texttt{mutate()} can be used to calculate the difference in means - see below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data_means }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(group, m) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{diff =}\NormalTok{ A }\OperatorTok{-}\StringTok{ }\NormalTok{B) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##       A     B  diff
##   <dbl> <dbl> <dbl>
## 1  18.9 -20.6  39.5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  What is the name of the column containing the differences between the means of A and B? means group m diff
\end{itemize}

Finally, if you then wanted to just get \texttt{diff} and throw away everything else in the table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data_means }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(group, m) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{diff =}\NormalTok{ A }\OperatorTok{-}\StringTok{ }\NormalTok{B) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pull}\NormalTok{(diff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 39.46041
\end{verbatim}

Portfolio Point - Reading pipes and verbalising tasks

\begin{info}
Keep in mind that a very useful technique for establishing what you want
to do to a dataframe is to verbalise what you need, or to write it down
in words, or to say it out loud. Take this last code chunk. What we
wanted to do was to \texttt{spread()} the data in \texttt{m} into the
groups A and B. Then we wanted to \texttt{mutate()} a new column that is
the difference, \texttt{diff}, of A minus B. And finally we wanted to
\texttt{pull()} out the value in \texttt{diff}.

Often step 1 of writing code or understanding code is knowing what it is
you want to do in the first place. After that you just need the correct
functions. Fortunately for us a lot of the \texttt{tidyverse} names its
functions based on what they specifically do!
\end{info}

\hypertarget{skill-5-creating-your-own-functions}{%
\subsection{Skill 5: Creating Your Own Functions}\label{skill-5-creating-your-own-functions}}

In Skills 1 to 4, we have looked at creating and sampling data, storing it in a tibble, and extracting information from that tibble. Now say we wanted to do this over and over again. For instance, we might want to generate 100 random datasets just like the one in Skill 4. It would be a pain to have to type out the \texttt{tibble()} function 100 times or even to copy and paste it 100 times. We'd likely make an error somewhere and it would be hard to read. To help us, we can create a custom function that performs the action you want; in our case, creating a tibble of random data.

Remember, a function is just a procedure that takes an input and gives you the same output each time - like a toaster! A function has the following format:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{name_of_function <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(arg1, arg2, arg3) \{}
  \CommentTok{## body of function goes between these curly brackets; i.e. what the function does for you.}
  \CommentTok{## Note that the last value calculated will be returned if you call the function.}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

First you define your own function name (e.g. \texttt{name\_of\_function}) and define the names of the arguments it will take (\texttt{arg1}, \texttt{arg2}, \ldots{}) - an argument is the information that you feed into your function, e.g.~data. Finally, you state the calculations or actions of the function in the body of the function (the portion that appears between the curly braces).

One of the most basic possible functions is one that takes no arguments and just prints a message. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hello <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
  \KeywordTok{print}\NormalTok{(}\StringTok{"Hello World!"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

So this function is called \texttt{hello}. It can be run by typing \texttt{hello()} in your console and it will give the output of \texttt{Hello\ World!} every single time you run it; it has no other actions or information. Test this in the console now by typing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hello}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello World!"
\end{verbatim}

Awesome right? Ok, so not very exciting. Let's make it better by adding an argument, \texttt{name}, and have it say Hello to \texttt{name}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hello <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{name =} \StringTok{"World!"}\NormalTok{) \{}
  \KeywordTok{paste}\NormalTok{(}\StringTok{"Hello"}\NormalTok{, name)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This new function is again called \texttt{hello()} and replaces the one you previously created. This time however you are supplying what is called a default argument, `name = ``World!'', but it still has the same action as the previous function of putting ``Hello'' and ``World!'' together. So if you run it you get ``Hello World!''. Try it yourself!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hello}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello World!"
\end{verbatim}

The difference this time however is that because you have added an argument to the input, you can change the information you give the argument and therefore change the output of the function. More flexible. More exciting.

{Quickfire Questions}

Test your understanding by answering these questions:

\begin{itemize}
\item
  Typing \texttt{hello("Phil")} in the console with this new function will give: Hello Heather Hello Phil Hello Niamh Hello Kevin
\item
  Typing the argument as \texttt{"is\ it\ me\ you\ are\ looking\ for"} will give: Hello is it me you are looking for I just called to say Hello You had me at Hello Hello seems to be the hardest word
\item
  What argument would you type to get ``Hello Dolly!'' as the output: Dolly Molly Holly Dolly!
\end{itemize}

Most of the time however we want to create a function that computes a value or constructs a table. For instance, let's create a function that returns randomly generated data from two samples, as we learned in the previous skills - see below. All we are doing is taking the tibble we created in Skill 4 and putting it in the body (between the curly brackets) of the function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen_data <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
  \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{)),}
                  \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,  }\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{-20}\NormalTok{, }\DecValTok{5}\NormalTok{)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function is called \texttt{gen\_data()} and when we run it we get a randomly generated table of two groups, each with 20 people, one with M = 20, SD = 5, the other with M = -20, sd = 5. Try running this \texttt{gen\_data()} function in the console a few times; remember that as the data is random, the numbers will be different each time you run it.

But say we want to modify the function to allow us to get a table with smaller or larger numbers of observations per group. We can add an argument \texttt{n} and modify the code as follows. Create this function and run it a couple of times through \texttt{gen\_data()}. The way to think about this is that every place that \texttt{n} appears in the body of the function (between the curly brackets) it will have the value of whatever you gave it in the arguments, i.e.~in this case, 20.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen_data <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{) \{}
  \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(n, n)),}
                  \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n,  }\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(n, }\DecValTok{-20}\NormalTok{, }\DecValTok{5}\NormalTok{)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  How many total participants would there be if you ran \texttt{gen\_data(2)}? 2 4 20 40
\item
  What would you type to get 100 participants per group? gen\_data(50) gen\_data(10) gen\_dota(100) gen\_data(100)
\end{itemize}

\textbf{Challenge Question:}

Keeping in mind that functions can take numerous arguments, and that each group in your function have separate means, can you modify the function \texttt{gen\_data} to allow the user to change the means for the two calls to \texttt{rnorm}? Have a try before revealing the solution below.

Solution To Challenge Question

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen_data <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{, }\DataTypeTok{m1 =} \DecValTok{20}\NormalTok{, }\DataTypeTok{m2 =} \DecValTok{-20}\NormalTok{) \{}
  \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\KeywordTok{c}\NormalTok{(n, n)),}
                  \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n,  m1, }\DecValTok{5}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(n, m2, }\DecValTok{5}\NormalTok{)))}
\NormalTok{\}}

\CommentTok{# m1 is the mean of group A, }
\CommentTok{# m2 is mean of group B and would look like:}

\CommentTok{# The function would be called by: gen_data(20, 20, -20)}
\CommentTok{# Giving 20 participants in each group, }
\CommentTok{# The first group having a mean of 20, }
\CommentTok{# The second group having a mean of -20. }

\CommentTok{# Likewise, a call of: gen_data(4, 10, 5)}
\CommentTok{# Would give two groups of 4, }
\CommentTok{# The first having a mean of 10, }
\CommentTok{# The second having a mean of 5.}
\end{Highlighting}
\end{Shaded}

Portfolio Point - Two important facts about functions

\begin{info}
Here are two important things to understand about functions.

\begin{enumerate}
\defenumi{enumi}.{enumi{enumi}.}
\item
  \textbf{Functions obey lexical scoping.} What does this mean? It's
  like what they say about Las Vegas: what happens in the function,
  stays in the function. Any variables created inside of a function will
  be discarded after the function executes and will not be accessible to
  the outside calling process. So if you have a line, say a variable
  \texttt{my\_var\ \textless{}-\ 17} inside of a function, and try to
  print \texttt{my\_var} from outside of the function, you will get an
  error:
  \texttt{object\ \textquotesingle{}my\_var\textquotesingle{}\ not\ found}.
  Although the function can `read' variables from the environment that
  are not passed to it through an argument, it cannot change them. So
  you can only write a function to return a value, not change a value.
\item
  \textbf{Functions return the last value that was computed.} You can
  compute many things inside of a function but only the last thing that
  was computed will be returned as part of the calling process. If you
  want to return \texttt{my\_var}, which you computed earlier but not as
  the final computation, you can do so explicitly using
  \texttt{return(my\_var)} at the end of the function (before the second
  curly bracket).
\end{enumerate}
\end{info}

\hypertarget{skill-6-replicating-operations}{%
\subsection{Skill 6: Replicating Operations}\label{skill-6-replicating-operations}}

The last skill you will need for the upcoming lab is knowing how to repeat an action (or expression) multiple times. You saw this in Lab 4 so we will only briefly recap here. Here, we use the base function \texttt{replicate()}. For instance, say you wanted to calculate the mean from \texttt{rnorm(100)} ten times, you could write it like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## bad way}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

But it's far easier and more readable to wrap the expression in \texttt{replicate()} function where the first argument is the number of times you want to repeat the expression stated as the second argument, i.e. \texttt{replicate(times,\ expression)}. Here below we replicate the mean of 100 randomly generated numbers from the normal distribution, and we do this 10 times:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{replicate}\NormalTok{(}\DecValTok{10}\NormalTok{, }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Also you'll probably want to store the results in a variable, for example, \texttt{ten\_samples}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ten_samples <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{10}\NormalTok{, }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mean}\NormalTok{())}
\NormalTok{ten_samples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] -0.044183836 -0.093613306 -0.065989976 -0.101770651  0.002358181
##  [6]  0.015655210 -0.182753390 -0.087999061  0.106490605  0.047600095
\end{verbatim}

Each element (value) of the vector within \texttt{ten\_samples} is the result of a single call to \texttt{rnorm(100)\ \%\textgreater{}\%\ mean()}.

\begin{itemize}
\tightlist
\item
  Assuming that your \texttt{hello()} function from Skill 5 still exists, and it takes the argument \texttt{name\ =\ Goodbye}, what would happen in the console if you wrote, \texttt{replicate(1000,\ hello("Goodbye"))}? Hello World would appear a thousand times hello Goodbye would appear a thousand times Hello Goodbye would appear a thousand times - Try it and see if it works!
\end{itemize}

Solution To Quickfire Question

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the function would be:}
\NormalTok{hello <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{name =} \StringTok{"World!"}\NormalTok{)\{}
  \KeywordTok{paste}\NormalTok{(}\StringTok{"Hello"}\NormalTok{, name)}
\NormalTok{\}}

\CommentTok{# and would be called by:}
\KeywordTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\KeywordTok{hello}\NormalTok{(}\StringTok{"Goodbye"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{finished-11}{%
\subsubsection{Finished!}\label{finished-11}}

To recap, we have shown you the following six skills:

\begin{itemize}
\tightlist
\item
  Skill 1: Generating random numbers with \texttt{base::rnorm()}
\item
  Skill 2: Permuting values with \texttt{base::sample()}\\
\item
  Skill 3: Creating a ``tibble'' (a type of data table) using \texttt{tibble::tibble()}
\item
  Skill 4: Computing and extracting a difference in group means using \texttt{dplyr::pull()} and \texttt{purrr::pluck()}
\item
  Skill 5: Creating your own custom functions using \texttt{base::function()}
\item
  Skill 6: Repeating operations using \texttt{base::replicate()}
\end{itemize}

You will need these skills in the coming lab to help you perform a real \textbf{permutation test}. Through these skills and the permutation test you will learn about null hypothesis significance testing.

\hypertarget{permutation-tests-of-hypotheses}{%
\subsection{Permutation Tests of Hypotheses}\label{permutation-tests-of-hypotheses}}

A common statistical question when comparing two groups might be, ``\textbf{Is there a real difference between the group means?}'' From this we can establish two contrasting hypotheses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{null hypothesis} which states that the group means \textbf{are equivalent} and is written as: \(H_0: \mu_1 = \mu_2\)

  \begin{itemize}
  \tightlist
  \item
    where \(\mu_1\) is the population mean of group 1
  \item
    and \(\mu_2\) is the population mean of group 2\\
  \end{itemize}
\item
  Or the \textbf{alternative hypothesis} which states the groups means \textbf{are not equivalent} and is written as: \(H_1: \mu_1 \ne \mu_2\).
\end{enumerate}

Using the techniques you read about earlier and in previous labs, today you will learn how to test the null hypothesis of no difference between two independent groups. We will first do this using a \textbf{permutation test} before looking at other tests in later labs.

A permutation test is a basic inferential procedure that involves a reshuffling of group labels or values to create new possible outcomes of the data you collected to see how your original mean difference compares to all possible outcomes. The test can in fact be applied in many situations, this is just one, and it provides a good starting place for understanding hypothesis testing. The steps for the exercise below, and really the logic of a permutation test for two independent groups, are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the real difference \(D_{orig}\) between the means of two groups (e.g.~Mean of A minus Mean of B).
\item
  Randomly shuffle the group labels (i.e.~which group each participant belonged to - A or B) and re-calculate the difference, \(D'\).
\item
  Repeat step 2 \(N_{r}\) times, where \(N_r\) is a large number (typically greater than 1000), storing each \(D_i'\) value to form a null hypothesis distribution.
\item
  Locate the difference you observed in step 1 (the real difference) on the null hypothesis distribution of all possible differences.
\item
  Decide whether the original difference is sufficiently extreme to reject the null hypothesis of no difference (\(H_0\)).
\end{enumerate}

This logic works because if the null hypothesis is true (there is no difference between the groups) then the labeling of the observations/participants into groups is arbitrary, and we can rearrange the labels in order to estimate the likelihood of our original difference under the \(H_0\). In other words if you know the original value of the difference between two groups (or the true difference) falls in the middle of your permuted distribution then there is no significant difference between the two groups. If however the original difference falls in the tail of the permuted distribution then there might be a significant difference depending on how far into the tail it falls.

Let's get started!

\hypertarget{Ch5InClassQueT1}{%
\subsection{Step 1: Load in Add-on Packages and Data}\label{Ch5InClassQueT1}}

1.1. Open a new script and call the \texttt{tidyverse} into your library.

1.2. Now type the statement \texttt{set.seed(1011)} at the top of your script after your library call and run it. (This `seeds' the random number generator so that you will get the same results as everyone else. The number 1011 is a bit random but if everyone uses it then we all get the same outcome. Different seeds give different outcomes)

1.3. Download the data file here. and read the data in \texttt{perm\_data.csv} into a variable called \texttt{dat}.

1.4. Let's give every participant a participant number as well by adding a new column to \texttt{dat}. Something like this would work: \texttt{mutate(subj\_id\ =\ row\_number())}

Helpful Hint

\begin{info}
\begin{enumerate}
\defenumi{enumi}.{enumi{enumi}.}
\item
  Something to do with \texttt{library()}
\item
  \texttt{set.seed(1011)}
\item
  Something to do with \texttt{read\_csv()}
\item
  pipe \texttt{\%\textgreater{}\%} on the mutate line shown
\end{enumerate}
\end{info}

Portfolio Point - Different uses of row\_number

\begin{info}
You will see that in the example here to put a row number for each of
the participants we do not have to state the number of participants we
have. In the Preclass however we did. What is the difference? Well, in
the Preclass we were making a tibble and trying to create a column in
that tibble using \texttt{row\_numbers}. If you want to do that you have
to state the number of rows, e.g. \texttt{1:20}. However, in this
example in the lab today the tibble already exists, we are just adding
to it. If that is the case then you can just mutate on a column of row
numbers without stating the number of participants. In summary:

\begin{itemize}
\tightlist
\item
  When creating the tibble, state the number of participants in
  \texttt{row\_numbers()}.
\item
  If tibble already exists, just mutate on \texttt{row\_numbers()}. No
  need for specific numbers.
\end{itemize}
\end{info}

Have a look at the resulting tibble, \texttt{dat}.

\begin{itemize}
\tightlist
\item
  The column \texttt{Y} is your dependent variable (DV)
\item
  The column \texttt{group} is your independent variable (IV).
\item
  The columns \texttt{subj\_id} is the participant number.
\end{itemize}

\hypertarget{Ch5InClassQueT2}{%
\subsection{\texorpdfstring{Step 2: Calculate the Original Mean Difference - \(D_{orig}\)}{Step 2: Calculate the Original Mean Difference - D\_\{orig\}}}\label{Ch5InClassQueT2}}

We now need to write a pipeline of five functions that calculates the mean difference between the groups in \texttt{dat}, Group A minus Group B. Broken down into steps this would be:

2.1.1. Use a pipe of two \texttt{dplyr} one-table verbs (e.g.~Lab 2) to create a tibble where each row contains the mean of one of the groups. Name the column storing the means as \texttt{m}.

2.1.2. Continue the pipe to \texttt{spread()} your data from long to wide format, based on the columns \texttt{group} and \texttt{m}.

2.1.3. Now add a pipe that \textbf{creates} a new column in this wide dataset called \texttt{diff} which is the value of group A's mean minus group B's mean.

2.1.4. Pull out the value in \texttt{diff} (the mean of group A minus the mean of group B) to finish the pipe.

Helpful Hint

\begin{info}
\texttt{dat\ \%\textgreater{}\%}

\texttt{group\_by(?)\ \%\textgreater{}\%}

\texttt{summarise(m\ =\ ?)\ \%\textgreater{}\%}

\texttt{spread(group,\ m)\ \%\textgreater{}\%}

\texttt{mutate(diff\ =\ ?\ -\ ?)\ \%\textgreater{}\%}

\texttt{pull(?)}
\end{info}

\begin{itemize}
\tightlist
\item
  Check that your value for \texttt{d\_orig} is correct, without using the solution, by typing your \texttt{d\_orig} value to two decimal places in the box. Include the sign, e.g. -1.23. The box will go green if you are correct. 
\end{itemize}

The above steps have created a pipeline of five functions to get one value. Nice! We now need to turn this into a function because we are going to be permuting the data set (specifically the grouping labels) and re-calculating the difference many, many times.

2.2. Wrap your pipeline in a function called \texttt{calc\_diff} but swap \texttt{dat} for \texttt{x}. This function will take a single argument named \texttt{x}, where \texttt{x} is the tibble that you want to calculate group means from. As in the previous step, the function will return a single value which is the difference between the group means. The start will look like this below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc_diff <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  x }\OperatorTok{%>%}\NormalTok{.....}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Helpful Hint

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc_diff <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  x }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(group) }\OperatorTok{%>%}
\StringTok{    }\NormalTok{the_rest_of_your_pipe...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

2.3. Now call your new function where \texttt{x} = \texttt{dat} as the argument and store the result in a new variable called \texttt{d\_orig}. Make sure that your function returns the same value as you got above and that your function returns a single value rather than a tibble. You can test this: \texttt{is.tibble(d\_orig)} should give you \texttt{FALSE} and \texttt{is.numeric(d\_orig)} should give you \texttt{TRUE}.

Helpful Hint

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d_orig <-}\StringTok{ }\KeywordTok{function_name}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ data_name) }
\CommentTok{# or}
\NormalTok{d_orig <-}\StringTok{ }\KeywordTok{function_name}\NormalTok{(data_name)}

\CommentTok{# Then type the following in the Console and look at answer:}

\KeywordTok{is.tibble}\NormalTok{(d_orig)}
\CommentTok{# True (is a tibble) or False (is not a tibble)}

\KeywordTok{is.numeric}\NormalTok{(d_orig)}
\CommentTok{# True (is numeric) or False (is not numeric; it is a character or integer instead.)}
\end{Highlighting}
\end{Shaded}

So we now have the original difference between the groups stored in \texttt{d\_orig}. Next we need to create a distribution of all possible differences to see where our original difference lies in this distribution. But first we need to shuffle the \texttt{group} letters (A or B) in our dataset and find the difference\ldots{}a few hundred times!

\hypertarget{Ch5InClassQueT3}{%
\subsection{Step 3: Permute the Group Labels}\label{Ch5InClassQueT3}}

3.1. Create a new function called \texttt{permute()} that takes as input a dataset \texttt{x} and returns the same dataset transformed such that the group labels (the values in the column \texttt{group}) are shuffled: started below for you. This will require using the \texttt{sample()} function within a \texttt{mutate()}. You have used \texttt{mutate()} twice already today and you saw how to \texttt{sample()} \textbf{letters} in the PreClass.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{permute <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  x }\OperatorTok{%>%}\NormalTok{.....}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Helpful Hint

\begin{info}
Might be easier to think of these steps in reverse.

\begin{enumerate}
\defenumi{enumi}.{enumi{enumi}.}
\item
  Start with a \texttt{mutate()} function that rewrites the column
  \texttt{group} every time you run it, e.g.
  \texttt{dat\ \%\textgreater{}\%\ mutate(variable\ =\ sample(variable))}
\item
  Now put that into your \texttt{permute()} function making the
  necessary adjustments to the code so it starts
  \texttt{x\ \%\textgreater{}\%...}. Again \texttt{x} should be in the
  function and not \texttt{dat}. ")
\end{enumerate}
\end{info}

3.2. Try out your new \texttt{permute()} function by calling it on \texttt{dat} (i.e. \texttt{x\ =\ dat}) a few times. You should see the group labels in the \texttt{group} column changing randomly. The most common mistake is that people mutate a new column by mispelling \texttt{group}. You want to overwrite/change the information in the \texttt{group} column not make a new one, so be careful with the spelling.

Now would be an excellent time to spend five minutes as a group recapping what you are doing.

\begin{itemize}
\tightlist
\item
  You have the original difference between groups.
\item
  You have a function that calculates and stores this difference.
\item
  You have a function that reshuffles the labels of the group.
\end{itemize}

Do you understand why? If not, go back to the principles of the permutation test at the start of the lab then read on\ldots{}

\hypertarget{Ch5InClassQueT4}{%
\subsection{Step 4: Create the Null-Hypothesis Distribution (NHD) for the Difference}\label{Ch5InClassQueT4}}

Now that we have the original difference and our two functions, one to shuffle group labels and one to calculate the difference between two groups, we need to actually create the distribution of possible differences and see where the original difference lies in it.

4.1.1. Write a \textbf{a single pipeline} that takes \texttt{dat} as the input, permutes the group labels with a call to your function \texttt{permute()}, and then calculates the difference in means between these new groups with a call to your function \texttt{calc\_diff()}.

4.1.2. Run this line manually a few times and watch the resulting value change as the labels get permuted.

Helpful Hint

\begin{info}
Think about verbalising your pipelines. In a single pipeline:

\begin{enumerate}
\defenumi{enumi}.{enumi{enumi}.}
\tightlist
\item
  I want to permute the data into two new groups.
\item
  Then I want to calculate the difference between these two new groups.
\end{enumerate}

The functions you have created do these steps. You just have to put them
in order and pipe the data through it.
\end{info}

4.2. Now take your pipeline of functions and repeat it 1000 times using the \texttt{replicate()} function. Store the output in a variable called \texttt{nhd}. \texttt{nhd} will contain 1000 values where each value is the mean difference of each of the 1000 random permutations of the data. (\textbf{Warning:} This will probably take a while to run, perhaps 10 seconds.)

Helpful Hint

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# replace expression with the pipeline you created in 4.1}
\NormalTok{nhd <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(times, expression)}
\end{Highlighting}
\end{Shaded}

You now have 1000 possible values of the difference between the permuted groups A and B - your permuted distribution.

4.3 Let's visualise this distribution through a frequency histogram of the values in \texttt{nhd}. This shows us the likelihood of various mean differences under \(H_0\). One thing to note however is that \texttt{nhd} is not a \texttt{tibble} and \texttt{ggplot} needs it to be a \texttt{tibble}. You need to convert it. You might start by do something like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{), }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

Helpful Hint

\begin{info}
Remember that \texttt{ggplot} works as:
\texttt{ggplot(data,\ aes(x))\ +\ geom...}. Here you need to convert
\texttt{nhd} into a tibble and put that in as your data. Look at the
example above and keep in mind that, in this case, the first NULL could
be replaced with the data in \texttt{nhd}.
\end{info}

\begin{itemize}
\tightlist
\item
  Looking at the histogram, visually locate where your original value would sit on this distribution. Would it be extreme, in the tail, or does it look rather common, in the middle? is in the middle so looks common is in the tail so looks extreme
\end{itemize}

Before moving on stop to think about what this means - that the difference between the two original groups is rather uncommon in this permuted distribution, i.e.~is in the tails! Again, if unsure, go back to the principles of NHST or discuss it with your tutor!

\hypertarget{Ch5InClassQueT5}{%
\subsection{Step 5: Compare the Observed Mean Difference to the NHD}\label{Ch5InClassQueT5}}

If the null hypothesis is false, and there is a real difference between the groups, then the difference in means we observed for the original data (\texttt{d\_orig}) should be somewhere in either tail of the null-hypothesis distribution we just estimated; it should be an ``extreme'' value. How can we test this beyond a visual inspection?

First we have to decide on a false positive (Type I error) rate which is the rate at which we will falsely reject \(H_0\) when it is true. This rate is referred to by the Greek letter \(\alpha\) (``alpha''). Let's just use the conventional level used in Psychology: \(\alpha = .05\).

So the question we must ask is, if the null hypothesis was true, what would be the probability of getting a difference in means as extreme as the one we observed in the original data? We will label this probability \texttt{p}.

Take a few moments to see if you can figure out how you might compute \texttt{p} from the data before we show you how. We will then show you the process in the next few, final, steps.

5.1. Replace the NULLS in the code below to create a logical vector which states TRUE for all values of \texttt{nhd} greater than or equal to \texttt{d\_orig} regardless of sign. \textbf{Note:} A logical vector is one that returns TRUE when the expression is true and FALSE when the expression is false.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lvec <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(}\OtherTok{NULL}\NormalTok{) }\OperatorTok{>=}\StringTok{ }\KeywordTok{abs}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Portfolio Point - abs and the case of one or two tails

\begin{info}
In the code above, the function \texttt{abs()} says to ignore the sign
and use the absolute value. For instance, if \texttt{d\_orig\ =\ -7},
then \texttt{abs(d\_orig)\ =\ 7}. Why do we do this here? Can you think
why you want to know how extreme your value is in this distribution
regardless of whether the value is positive or negative?

The answer relates to whether you are testing in one or two tails of
your distribution; the positive side, the negative side, or both. You
will have heard in your lectures of one or two-tailed tests. Most people
would say to run two-tailed tests. This means looking at the negative
and positive tails of the distribution to see if our original value is
extreme, and the simplest way to do this is to ignore the sign of the
values and treat both sides equally. If you wanted to only test
one-tail, say that your value is extreme to the negative side of the
tail, then you would not use the \texttt{abs()} and set the expression
to make sure you only find values less than your original value. To test
only on the positive side of the distribution, make sure you only get
values higher than the original. But for now we will mostly look at
two-tailed tests.
\end{info}

5.2. Replace the NULL in the code below to \texttt{sum()} the \texttt{lvec} vector to get the total number of values equal to or greater than our original difference, \texttt{d\_orig}. Fortunately R is fine with summing TRUEs and FALSEs so you do not have to convert the data at all.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_exceeding_orig <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

5.3. Replace the NULL in the code below to calculate the probability of finding a value of \texttt{d\_orig} in our \texttt{nhd} distribution by dividing \texttt{n\_exceeding\_orig}, the number of values greater than or equal to your original value, by the \texttt{length()} of your whole distribution \texttt{nhd}. \textbf{Note: the length of \texttt{nhd} is the same as the number of replications we ran. Using code reduces the chance of human error}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

5.4. Finally, complete the sentence below determining if the original value was extreme or not in regards to the distribution. Use inline coding, shown in Lab 1, to replace the \texttt{XXX}s. For example, when formatted without the space before the first r, \texttt{r\ length(nhd)} would present as 1000.

\textbf{" The difference between Group A and Group B (M = \texttt{XXX}) was found to be have a probability of p = \texttt{XXX}. This means that the original mean difference was \ldots{}\ldots{} and the null hypothesis is \ldots{}.." }

\hypertarget{finished-12}{%
\subsubsection{Finished!}\label{finished-12}}

Well done in completing this lab. Let's recap before finishing. We had two groups, A and B, that we had tested in an experiment. We calculated the mean difference between A and B and wanted to know if this was a significant difference. To test this we created a distribution of all possible differences between A and B using the premise of permutation tests and then found the probability of our original value in that permuted distribution. The more extreme the value in a distribution the more likely that the difference is significant. And that is exactly what we found; an \(\alpha < .05\). Next time we will look at using functions and inferential tests to perform this analysis but by understanding the above you now know how probability is determined.

\hypertarget{non-parametric-tests}{%
\section{Non-parametric tests}\label{non-parametric-tests}}

\hypertarget{simulation}{%
\section{Simulation}\label{simulation}}

\hypertarget{rtweet}{%
\section{rtweet}\label{rtweet}}

\hypertarget{packages-1}{%
\subsection{Packages}\label{packages-1}}

In order to run these analyses you will need \texttt{tidyverse} for data wrangling,\texttt{rtweet} for getting the twitter data, \texttt{tidytext} for working with text, \texttt{knitr} for tidy tables, and \texttt{igraph} and \texttt{ggraph} for making pretty network plots.

In order to get data from Twitter you will need to have a twitter account and gain access to Twitter's API. There are instructions \href{https://rtweet.info/}{for doing so here}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(tidytext)}
\KeywordTok{library}\NormalTok{(rtweet)}
\KeywordTok{library}\NormalTok{(igraph)}
\KeywordTok{library}\NormalTok{(ggraph)}
\end{Highlighting}
\end{Shaded}

\hypertarget{hashtag-search}{%
\subsection{Hashtag search}\label{hashtag-search}}

We can use rtweet to search for all tweets containing particular words or hashtags. It will return the last 18,000 tweets and only from the last 6-9 days. Be careful, there is a 15 minute time-out which means you can only retrieve 18,000 tweets every 15 minutes (there are various limits on what Twitter allows you to do with the data).

First, let's search for all tweets that contain \#GoT and \#ForTheThrone.

There are various arguments to the \texttt{search\_tweets} function, you can specify how many tweets you want to retrive with \texttt{n}, whether retweets should be included in the search results with \texttt{include\_rts} and you can also specify the language of the user's account with \texttt{lang}. Note that this doesn't tell you what language the tweets are written in, only that the account language is set to e.g., English, so it's not perfect. For additional options, see the help documentation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets <-}\StringTok{ }\KeywordTok{search_tweets}\NormalTok{(}\DataTypeTok{q =} \StringTok{"#GoT OR #ForTheThrone"}\NormalTok{, }\DataTypeTok{n =} \DecValTok{18000}\NormalTok{, }\DataTypeTok{include_rts =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{lang =} \StringTok{"en"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-wrangling}{%
\subsection{Data wrangling}\label{data-wrangling}}

The output of \texttt{search\_tweets()} is a list which makes it difficult to share. We need to do a little bit of tidying to turn it into a tibble, and additionally there's a lot of data and we don't need it all. I'm also going to add a tweet counter which will help when we move between wide and long-form. \texttt{rtweet} provides a huge amount of data, more than we're going to use in this example so have a look through to see what you have access to - there are some great examples of how this data can be used \href{https://mkearney.github.io/nicar_tworkshop/}{here} and \href{https://rud.is/books/21-recipes/index.html}{here} .

In the following code, I have added an identifier column \texttt{tweet\_number} and then selected \texttt{text} (the actual text of the tweets), \texttt{created\_at} (the timestamp of the tweet), \texttt{source} (iphone/android etc), \texttt{followers\_count} (how many followers the accounts the tweets came from have), and \texttt{country} (like \texttt{lang}, the is the country specified on the account, not necessarily the country the tweeter is in).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{tweet_number =} \KeywordTok{row_number}\NormalTok{())}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(tweet_number, text, created_at, source, followers_count, country)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You can download this file here so that you can reproduce my exact analyses (or you can use your own, but the results will look a bit different).

The first thing we need to do is tidy up the text by getting rid of punctuation, numbers, and links that aren't of any interest to us. We can also remove the hashtags because we don't want those to be included in any analysis. This code uses \textbf{regular expressions} which quite frankly make very little sense to me, I have copied and pasted this code from the Tidy Text book.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\NormalTok{dat }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{text =} \KeywordTok{str_replace_all}\NormalTok{(text, }\StringTok{"[^}\CharTok{\textbackslash{}x01}\StringTok{-}\CharTok{\textbackslash{}x7F}\StringTok{]"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \DataTypeTok{text =} \KeywordTok{str_replace_all}\NormalTok{(text, }\StringTok{"#GoT"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \DataTypeTok{text =} \KeywordTok{str_replace_all}\NormalTok{(text, }\StringTok{"#ForTheThrone"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \DataTypeTok{text =} \KeywordTok{str_replace_all}\NormalTok{(text, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.|[[:digit:]]+"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \DataTypeTok{text =} \KeywordTok{str_replace_all}\NormalTok{(text, }\StringTok{"https|amp|t.co"}\NormalTok{, }\StringTok{""}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{time-series}{%
\subsection{Time series}\label{time-series}}

We can plot when the tweets were sent. This is somewhat uninteresting because it's no longer airing, but it's worth highlighting this as a feature. If you were watching live, you could use this to see the spikes in tweets when people are watching each episode live (different timezones will muddle this a little, you could filter by \texttt{country} perhaps).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ts_plot}\NormalTok{(tweets, }\DataTypeTok{by =} \StringTok{"1 hours"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-22-1} 

}

\caption{Time series plot by hour}\label{fig:unnamed-chunk-22}
\end{figure}

You can change the time interval with the \texttt{by} argument and you can also change the time zone. \texttt{ts\_plot} creates a \texttt{ggplot} object so you can also add the usual ggplot layers to customise apperance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ts_plot}\NormalTok{(tweets, }\DataTypeTok{by =} \StringTok{"10 mins"}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"GMT"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Number of tweets"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-23-1} 

}

\caption{Time series plot by 10 minute intervals}\label{fig:unnamed-chunk-23}
\end{figure}

\hypertarget{tidy-text-and-word-frequencies}{%
\subsection{Tidy text and word frequencies}\label{tidy-text-and-word-frequencies}}

First, we can produce frequency plots for words used in all tweets to see which words are used most often in \#GoT and \#ForTheThrone tweets. To do this, we have to create a tidy dataset, just like we do when working with numerical data. We're going to use the \texttt{unnest\_tokens} function from \texttt{tidytext} which will separate each word on to a new line, something similar to like using \texttt{gather} (or \texttt{pivot\_longer} as it will soon be known). Helpfully, this function will also convert all of our words to lower case which makes them a bit easier to work with.

The second part of the code removes all the stop words. Stop words are words that are commonly used but are of not real interest, for example function words like ``the'', ``a'', ``it''. You can make your own list but \texttt{tidytext} helpfully comes with several databases. Look at the help documentation if you want to know more about these or change the defaults.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create tidy text}
\NormalTok{dat_token <-}\StringTok{ }\NormalTok{dat }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(word, text) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(stop_words, }\DataTypeTok{by =} \StringTok{"word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can plot the 20 most frequent words used in all the tweets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_token}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{na.omit}\NormalTok{()}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(word, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{reorder}\NormalTok{(word, n))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ word, }\DataTypeTok{y =}\NormalTok{ n))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-25-1} 

}

\caption{Most frequent words}\label{fig:unnamed-chunk-25}
\end{figure}

There's quite a few words here that aren't that helpful to us so it might be best to get rid of them (essentially we're building our own list of stop words).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{custom_stop <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"gameofthrones"}\NormalTok{, }\StringTok{"hbo"}\NormalTok{, }\StringTok{"season"}\NormalTok{, }\StringTok{"game"}\NormalTok{, }\StringTok{"thrones"}\NormalTok{, }\StringTok{"lol"}\NormalTok{, }\StringTok{"tco"}\NormalTok{, }\StringTok{"https"}\NormalTok{, }\StringTok{"watch"}\NormalTok{, }\StringTok{"watching"}\NormalTok{, }\StringTok{"im"}\NormalTok{, }\StringTok{"amp"}\NormalTok{)}

\NormalTok{dat_token <-}\StringTok{ }\NormalTok{dat_token }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\NormalTok{word }\OperatorTok{%in%}\StringTok{ }\NormalTok{custom_stop)}
\end{Highlighting}
\end{Shaded}

Now we can try plotting the words again and make them pretty.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_token}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{na.omit}\NormalTok{()}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(word, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{reorder}\NormalTok{(word, n))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ word, }\DataTypeTok{y =}\NormalTok{ n, }\DataTypeTok{fill =}\NormalTok{ word))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis}\NormalTok{(}\DataTypeTok{discrete =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-27-1} 

}

\caption{Most frequent words (edited)}\label{fig:unnamed-chunk-27}
\end{figure}

To be honest, this isn't that interesting because it's so general, it might be more interesting to see how often each of the main characters are being mentioned.

One problem is that people on the internet are terrible at spelling and we need to have the exact spellings which means that for Daenerys, Jon, and Jaime, the chances that people will have spelled their names wrong is quite high (as my level 2 students who watched me live code the first version of this will attest) so first we're going to correct those.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_token2 <-}\StringTok{ }\NormalTok{dat_token }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{recode}\NormalTok{(word, }\StringTok{"khaleesi"}\NormalTok{ =}\StringTok{ "daenerys"}\NormalTok{,}
                       \StringTok{"dany"}\NormalTok{ =}\StringTok{ "daenerys"}\NormalTok{,}
                       \StringTok{"jamie"}\NormalTok{ =}\StringTok{ "jaime"}\NormalTok{,}
                       \StringTok{"john"}\NormalTok{ =}\StringTok{ "jon"}\NormalTok{))}

\NormalTok{characters <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"jon"}\NormalTok{, }\StringTok{"daenerys"}\NormalTok{, }\StringTok{"bran"}\NormalTok{, }\StringTok{"arya"}\NormalTok{, }\StringTok{"sansa"}\NormalTok{, }\StringTok{"tyrion"}\NormalTok{, }\StringTok{"cersei"}\NormalTok{, }\StringTok{"jaime"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can plot a count of how many times each name has been mentioned.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_token2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(word }\OperatorTok{%in%}\StringTok{ }\NormalTok{characters)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(word, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{reorder}\NormalTok{(word, n))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ word, }\DataTypeTok{y =}\NormalTok{ n, }\DataTypeTok{fill =}\NormalTok{ word)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Number of mentions"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Character"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis}\NormalTok{(}\DataTypeTok{discrete =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-29-1} 

}

\caption{Frequecy of mentions for each character}\label{fig:unnamed-chunk-29}
\end{figure}

\hypertarget{bigram-analysis}{%
\subsection{Bigram analysis}\label{bigram-analysis}}

Rather than looking at individual words we can look at what words tend to co-occur. We want to use the data set where we've corrected the spelling so this is going to require us to transform from long to wide and then back to long because the night is dark and full of terror. DID YOU SEE WHAT I DID THERE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_bigram <-}\StringTok{ }\NormalTok{dat_token2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(tweet_number) }\OperatorTok{%>%}\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{text =} \KeywordTok{str_c}\NormalTok{(word, }\DataTypeTok{collapse =} \StringTok{" "}\NormalTok{))}\OperatorTok{%>%}\StringTok{ }\CommentTok{# this puts it back into wide-form}
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(bigram, text, }\DataTypeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\DataTypeTok{n =} \DecValTok{2}\NormalTok{, }\DataTypeTok{collapse =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{%>%}\StringTok{ }\CommentTok{# and then this turns it into bigrams in a tidy format}
\StringTok{  }\KeywordTok{na.omit}\NormalTok{()}

\NormalTok{dat_bigram }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(bigram, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{align =} \StringTok{"c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c}
\hline
bigram & n\\
\hline
freefolk gt & 165\\
\hline
togive freefolk & 164\\
\hline
se tvtime & 103\\
\hline
watched episode & 88\\
\hline
episode se & 86\\
\hline
ice fire & 85\\
\hline
song ice & 81\\
\hline
jonsnow daenerys & 73\\
\hline
bracelets fef & 71\\
\hline
connected matter & 71\\
\hline
\end{tabular}

Again there's a bit of nonsense here and it's a bit uninteresting but it's worth highlighting this is something you can do. Now that we've got our bigrams we can plot these to see the connections between the different words. First, we're going to use \texttt{separate} to put the two words into different columns, then we'll count them up and plot them. If you want more information about this see the \href{https://www.tidytextmining.com/ngrams.html}{tidytext book online} as I am entirely cribbing this from that book. The plot requires the packages \texttt{igraph} and \texttt{ggraph}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigrams_separated <-}\StringTok{ }\NormalTok{dat_bigram }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(bigram, }\KeywordTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{" "}\NormalTok{)}

\CommentTok{# new bigram counts:}
\NormalTok{bigram_counts <-}\StringTok{ }\NormalTok{bigrams_separated }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(word1, word2, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{bigram_counts}

\CommentTok{# filter for only relatively common combinations (more than 20 occurances)}
\NormalTok{bigram_graph <-}\StringTok{ }\NormalTok{bigram_counts }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{>}\StringTok{ }\DecValTok{20}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{graph_from_data_frame}\NormalTok{()}

\CommentTok{# make a pretty network plot}

\KeywordTok{ggraph}\NormalTok{(bigram_graph, }\DataTypeTok{layout =} \StringTok{"fr"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_edge_link}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_node_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_node_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ name), }\DataTypeTok{vjust =} \DecValTok{1}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-31-1} 

}

\caption{Network graph of bigrams}\label{fig:unnamed-chunk-31}
\end{figure}

\begin{verbatim}
## # A tibble: 30,134 x 3
##    word1     word2        n
##    <chr>     <chr>    <int>
##  1 freefolk  gt         165
##  2 togive    freefolk   164
##  3 se        tvtime     103
##  4 watched   episode     88
##  5 episode   se          86
##  6 ice       fire        85
##  7 song      ice         81
##  8 jonsnow   daenerys    73
##  9 bracelets fef         71
## 10 connected matter      71
## # ... with 30,124 more rows
\end{verbatim}

I am still figuring out how to customise the aesthetics of \texttt{ggraph}.

\hypertarget{sentiment-analysis}{%
\subsection{Sentiment analysis}\label{sentiment-analysis}}

Sentiment analyses look at whether the expressed opinion in a bit of text is positive, negative, or neutral, using information from databases about the valance of different words. We can perform a sentiment analysis on the tweets that contain each character's name to see whether e.g., Jon is mentioned in tweets that are largely positive or if Jaime is mentioned in tweets that are largely negative.

To do this, we first need to do a bit of wrangling. We're going to transform it back to wide-form, then we're going to add in in a column that says whether each character was mentioned in the tweet, then we're going to transform it back to long-form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_mentions <-}\StringTok{ }\NormalTok{dat_token2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(tweet_number) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{text =} \KeywordTok{str_c}\NormalTok{(word, }\DataTypeTok{collapse =} \StringTok{" "}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{jon =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".jon"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{daenerys =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".daenerys"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{bran =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".bran"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{arya =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".arya"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{sansa =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".sansa"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{tyrion =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".tyrion"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{cersei =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".cersei"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{jaime =} \KeywordTok{case_when}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(text, }\StringTok{".jaime"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\OtherTok{FALSE}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(word, text) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(stop_words, }\DataTypeTok{by =} \StringTok{"word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once we've done this, we can then run a sentiment analysis on the tweets for each character. I still haven't quite cracked iteration so this code is a bit repetitive, if you can give me the better way of doing this that's less prone to copy and paste errors, please do.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jon <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(jon)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"jon"}\NormalTok{)}

\NormalTok{daenerys <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(daenerys }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"daenerys"}\NormalTok{)}


\NormalTok{bran <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(bran }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"bran"}\NormalTok{)}

\NormalTok{arya <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(arya }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"arya"}\NormalTok{)}

\NormalTok{sansa <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(sansa }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"sansa"}\NormalTok{)}

\NormalTok{tyrion <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(tyrion }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"tyrion"}\NormalTok{)}

\NormalTok{cersei <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(cersei }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"cersei"}\NormalTok{)}

\NormalTok{jaime <-}\StringTok{ }\NormalTok{dat_mentions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(jaime }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{get_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{index =}\NormalTok{ tweet_number, sentiment) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(sentiment, n, }\DataTypeTok{fill =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sentiment =}\NormalTok{ positive }\OperatorTok{-}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{character =} \StringTok{"jaime"}\NormalTok{)}


\NormalTok{dat_sentiment <-}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(jon,daenerys,bran,arya,sansa,tyrion,cersei,jaime) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(character) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{positive =} \KeywordTok{sum}\NormalTok{(positive),}
            \DataTypeTok{negative =} \KeywordTok{sum}\NormalTok{(negative),}
            \DataTypeTok{overall =} \KeywordTok{sum}\NormalTok{(sentiment))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(positive}\OperatorTok{:}\NormalTok{overall, }\DataTypeTok{key =}\NormalTok{ type, }\DataTypeTok{value =}\NormalTok{ score)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{type =} \KeywordTok{factor}\NormalTok{(type, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"overall"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Now that we've done all that we can display a table of positive, negative and overall sentiment scores. Bear in mind that not all words have an associated sentiment score, particularly if they're a non-standard usage of English (as an aside, this makes RuPaul's Drag Race very difficult to analyse because tidytext will think a sickening death drop is a bad thing).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_sentiment }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(type, score)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(overall))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{align =} \StringTok{"c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c|c}
\hline
character & positive & negative & overall\\
\hline
jaime & 16 & 29 & -13\\
\hline
tyrion & 15 & 30 & -15\\
\hline
sansa & 26 & 44 & -18\\
\hline
cersei & 28 & 55 & -27\\
\hline
arya & 26 & 58 & -32\\
\hline
bran & 32 & 66 & -34\\
\hline
daenerys & 62 & 155 & -93\\
\hline
jon & 90 & 185 & -95\\
\hline
\end{tabular}

Because there's diferent numbers of tweets for each character, it might be more helpful to convert it to percentages to make it easier to compare.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_sentiment }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(type, score)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(character)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{total =}\NormalTok{ positive }\OperatorTok{+}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{positive_percent =}\NormalTok{ positive}\OperatorTok{/}\NormalTok{total}\OperatorTok{*}\DecValTok{100}\NormalTok{,}
         \DataTypeTok{negative_percent =}\NormalTok{ negative}\OperatorTok{/}\NormalTok{total}\OperatorTok{*}\DecValTok{100}\NormalTok{,}
         \DataTypeTok{sentiment =}\NormalTok{ positive_percent }\OperatorTok{-}\StringTok{ }\NormalTok{negative_percent)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(character, positive_percent, negative_percent, sentiment)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(sentiment))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{align =} \StringTok{"c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c|c|c|c}
\hline
character & positive\_percent & negative\_percent & sentiment\\
\hline
sansa & 37.14286 & 62.85714 & -25.71429\\
\hline
jaime & 35.55556 & 64.44444 & -28.88889\\
\hline
cersei & 33.73494 & 66.26506 & -32.53012\\
\hline
tyrion & 33.33333 & 66.66667 & -33.33333\\
\hline
jon & 32.72727 & 67.27273 & -34.54545\\
\hline
bran & 32.65306 & 67.34694 & -34.69388\\
\hline
arya & 30.95238 & 69.04762 & -38.09524\\
\hline
daenerys & 28.57143 & 71.42857 & -42.85714\\
\hline
\end{tabular}

They're all quite negative because there's no pleasing some people but there's some face validity to the analysis given the order of the rankings. If you'd been watching this live you could have repeated this each episode to see how the reactions to the characters changes.

Let's make that into a graph cause graphs are great.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_sentiment }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(type, score)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(character)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{total =}\NormalTok{ positive }\OperatorTok{+}\StringTok{ }\NormalTok{negative)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{positive_percent =}\NormalTok{ positive}\OperatorTok{/}\NormalTok{total}\OperatorTok{*}\DecValTok{100}\NormalTok{,}
         \DataTypeTok{negative_percent =}\NormalTok{ negative}\OperatorTok{/}\NormalTok{total}\OperatorTok{*}\DecValTok{100}\NormalTok{,}
         \DataTypeTok{sentiment =}\NormalTok{ positive_percent }\OperatorTok{-}\StringTok{ }\NormalTok{negative_percent)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(character, positive_percent, negative_percent, sentiment)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(sentiment))}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(character,sentiment), }\DataTypeTok{y =}\NormalTok{ sentiment)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Overall sentiment (percent)"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Character"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{19-supplementary_files/figure-latex/unnamed-chunk-36-1} 

}

\caption{Sentiment scores for each character}\label{fig:unnamed-chunk-36}
\end{figure}

\texttt{rtweet} is such a cool package and I've found that the limits of what you can do with it are much more about one's imagination. There's much more you could do with this package but when I first ran these analyses I found that tracking RuPaul's Drag Race was a fun way to learn a new package as it did give an insight into the fan reactions of one of my favourite shows. I also use this package to look at swearing on Twitter (replace the hashtags with swear words). The best way to learn what \texttt{rtweet} and \texttt{tidytext} can do for you is to find a topic you care about and explore the options it gives you. If you have any feedback on this tutorial you can find me on twitter: \citep{emilynordmann}(\url{https://twitter.com/emilynordmann}).

\hypertarget{appendix-appendices}{%
\appendix}


\hypertarget{rstudio-cloud}{%
\chapter{RStudio Cloud}\label{rstudio-cloud}}

Sometimes R might not work well on our own computers. However, there is an online version of R Studio (R Studio Cloud) which can be used in a pinch. Using R Studio Cloud is a little different to R Studio, so we have made a short guide to get you up and running.

\hypertarget{creating-an-account}{%
\section{Creating an Account}\label{creating-an-account}}

Head to \href{https://rstudio.cloud/}{RStudio Cloud} and click ``Sign Up'' at the top of the page

Enter details you wish to sign in with and select ``Sign up''

You'll receive an e-mail at the address you sign up with, make sure to click the link to activate your account fully.

\hypertarget{accessing-rstudio-cloud}{%
\section{Accessing RStudio Cloud}\label{accessing-rstudio-cloud}}

Head back to \href{https://rstudio.cloud/}{RStudio Cloud} and select ``log In'', where we previously chose ``Sign Up''

Once logged in, you'll be taken to the ``Your Workspace'' page, this is where all of your RStudio Cloud projects will be accessible from

Select ``New Project'' and then ``New Project'' again

You will see the message ``Deploying Project'' for a couple of minutes while it creates your Workspace

\hypertarget{getting-started-with-rstudio-cloud}{%
\section{Getting Started with RStudio Cloud}\label{getting-started-with-rstudio-cloud}}

Once loaded, you'll see a page that looks almost identical to the other screenshots in the learning material

\hypertarget{naming-the-workspace}{%
\subsection{Naming the Workspace}\label{naming-the-workspace}}

Let's give the project a better name!

Click on ``Untitled Project'' at the top of the page

This will allow you to rename to whatever you like, in this case we'll go for ``Network Training''

Press the return key on your keyboard, or click on a different area on the page to complete the task

\hypertarget{uploading-files}{%
\subsection{Uploading Files}\label{uploading-files}}

Since this is on the web, files on your computer won't be immediately accessible to RStudio Cloud, you will need to upload them yourself

Click the ``Upload'' button on the Files tab

An ``Upload Files'' element will load up where you can click ``Browse'' and select the file(s) you wish to make available to RStudio Cloud

Once you have selected a file and chosen ``OK'', you'll be taken back to the main application and you will now see the file you uploaded

You can now interact with this file as described in the rest of the learning material!

\bibliography{book.bib,packages.bib}


\end{document}
